{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db139b29-c059-4b4f-8ad5-bf77274a15cc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import torchmetrics\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, GenerationConfig\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "334efd5c-e16d-44ce-94de-ad6c7eccc031",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ee6c63a-ce6d-4610-93d9-16a57f847c21",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparamters \n",
    "top_k = 50\n",
    "top_p = 0.9\n",
    "temp = 0.8\n",
    "min_new_tokens = 10\n",
    "max_new_tokens = 50\n",
    "do_sample=True\n",
    "num_beams=1\n",
    "\n",
    "dataset_name=\"cnn\"\n",
    "model_name= \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "batch_size=8\n",
    "max_input_length=1024\n",
    "DEVICE = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "access_token = \"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\"\n",
    "cache_dir=\"/data/james/.cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc9a5a77-0973-48ac-8290-2e573d2bc654",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, metrics=None):\n",
    "        if not metrics:\n",
    "            metrics = [\"rouge\", \"sacre_bleu\", \"bertscore\", \"factkb\"]\n",
    "        self.metrics = metrics\n",
    "    \n",
    "    def evaluate(self, predictions, references, documents, metrics=[\"rouge\", \"bertscore\", \"factkb\", \"alignscore\"]):\n",
    "        result_dict = OrderedDict()\n",
    "        if \"rouge\" in metrics:\n",
    "            rouge_dict = self.calculate_rouge(predictions, references)\n",
    "            for k, v in rouge_dict.items():\n",
    "                result_dict[k] = v\n",
    "        if \"sacre_bleu\" in metrics:\n",
    "            sacre_bleu_dict = self.calculate_sacrebleu(predictions, references)\n",
    "            for k, v in sacre_bleu_dict.items():\n",
    "                result_dict[k] = v\n",
    "        if \"bertscore\" in metrics:\n",
    "            bertscore_dict = self.calculate_bertscore(predictions, references)\n",
    "            for k, v in bertscore_dict.items():\n",
    "                result_dict[k] = v\n",
    "        if \"factkb\" in metrics:\n",
    "            result_dict[\"factkb\"] = self.calculate_factkb(predictions, documents)\n",
    "            \n",
    "        if \"alignscore\" in metrics:\n",
    "            result_dict[\"alignscore\"] = self.calculate_alignscore(predictions, documents) \n",
    "\n",
    "        for k, v in result_dict.items():\n",
    "            print(f\"{k} -> {v*100:.2f}\")\n",
    "        return result_dict\n",
    "\n",
    "    def calculate_rouge(self, predictions, references):\n",
    "        from torchmetrics.functional.text.rouge import rouge_score\n",
    "        rouge_dict = rouge_score(preds=predictions, target=references)\n",
    "        return {k: v.item() for k, v in rouge_dict.items()}\n",
    "\n",
    "    def calculate_sacrebleu(self, predictions, references):\n",
    "        from torchmetrics.functional.text import sacre_bleu_score\n",
    "        score = sacre_bleu_score(preds=predictions, target=[[i] for i in references])\n",
    "        return {\"sacre_bleu\": score.item()}\n",
    "\n",
    "    def calculate_bertscore(self, predictions, references):\n",
    "        import evaluate\n",
    "        bertscore = evaluate.load(\"bertscore\")\n",
    "        bertscore_dict = bertscore.compute(predictions=predictions, references=references, model_type=\"roberta-large-mnli\")\n",
    "        res = {\"bertscore_precision\": np.mean(bertscore_dict[\"precision\"]), \"bertscore_recall\": np.mean(bertscore_dict[\"recall\"]), \"bertscore_f1\": np.mean(bertscore_dict[\"f1\"])}\n",
    "        return {k: v.item() for k, v in res.items()}\n",
    "    \n",
    "    def calculate_alignscore(self, predictions, documents):\n",
    "        from AlignScore.src.alignscore import AlignScore\n",
    "        ckpt_path = \"models/AlignScore-base.ckpt\"\n",
    "        align_scorer = AlignScore(model='roberta-base', batch_size=8, device=DEVICE, ckpt_path=ckpt_path, evaluation_mode='nli_sp')\n",
    "        alignscore_result = align_scorer.score(contexts=documents, claims=predictions)\n",
    "        #total_result['AlignScore'] = 100*np.mean(alignscore_result)\n",
    "        return np.mean(alignscore_result)\n",
    "\n",
    "    def calculate_factkb(self, predictions, documents):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\", padding=\"max_length\", truncation=True, cache_dir=cache_dir)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\"bunsenfeng/FactKB\", torch_dtype=torch.float16, cache_dir=cache_dir)\n",
    "        model = model.to(DEVICE)\n",
    "        res = []\n",
    "        for i in range(len(predictions)):\n",
    "            input_pretokenized = f\"{predictions[i]} {tokenizer.sep_token} {documents[i]}\"\n",
    "            tokenized_input = tokenizer(input_pretokenized, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "            with torch.no_grad():\n",
    "                output = model(input_ids=tokenized_input.input_ids.to(DEVICE))\n",
    "            logits = torch.softmax(output.logits, dim=1)  # (bz, 2)\n",
    "            res.append(logits.squeeze()[-1].item())\n",
    "        return np.mean(res)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e6f9c07-b5b5-461d-adb1-00580c2aa643",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "def xsum_pretokenize(dataset, tokenizer, max_input_length):\n",
    "    data = {\"context\": [], \"query\": [], \"summary\": []}\n",
    "    for i, row in tqdm(enumerate(dataset), desc=\"truncating documents...\"):\n",
    "        trunc_doc = tokenizer.batch_decode(tokenizer(row['document'], return_tensors=\"pt\", max_length=max_input_length,  truncation=True).input_ids, skip_special_tokens=True)[0]\n",
    "        data['context'].append(trunc_doc)\n",
    "        data['summary'].append(row['summary'])\n",
    "        data[\"query\"].append(\"Summarize the article in one sentence. Summary:\")\n",
    "    return Dataset.from_dict(data)\n",
    "\n",
    "def cnn_pretokenize(dataset, tokenizer, max_input_length):\n",
    "    data = {\"context\": [], \"query\": [], \"summary\": []}\n",
    "    for i, row in tqdm(enumerate(dataset), desc=\"truncating documents...\"):\n",
    "        trunc_doc = tokenizer.batch_decode(tokenizer(row['article'], return_tensors=\"pt\", max_length=max_input_length,  truncation=True).input_ids, skip_special_tokens=True)[0]\n",
    "        data['context'].append(trunc_doc)\n",
    "        data['summary'].append(row['highlights'])\n",
    "        data['query'].append(\"Summary of the above news article:\")\n",
    "    return Dataset.from_dict(data)\n",
    "\n",
    "def pubmedqa_pretokenize(dataset, tokenizer, max_input_length):\n",
    "    data = {\"context\": [], \"query\": [], \"summary\": []}\n",
    "    for i, row in tqdm(enumerate(dataset), desc=\"truncating documents...\"):\n",
    "        context= ''.join(c for c in row['context']['contexts'])\n",
    "        trunc_doc = tokenizer.batch_decode(tokenizer(context, return_tensors=\"pt\", max_length=max_input_length, truncation=True).input_ids, skip_special_tokens=True)[0]\n",
    "        data['context'].append(trunc_doc)\n",
    "        data['summary'].append(row['long_answer'])\n",
    "        data['query'].append(f\"Question: {row['question']}. Answer:\")\n",
    "    return Dataset.from_dict(data)\n",
    "\n",
    "def pretokenize(dataset_name, dataset, tokenizer, max_input_length):\n",
    "    if dataset_name == \"xsum\":\n",
    "        return xsum_pretokenize(dataset, tokenizer, max_input_length)\n",
    "    elif dataset_name == \"cnn\":\n",
    "        return cnn_pretokenize(dataset, tokenizer, max_input_length)\n",
    "    elif dataset_name == \"PubMedQA\":\n",
    "        return pubmedqa_pretokenize(dataset, tokenizer, max_input_length)\n",
    "    return None\n",
    "\n",
    "def template_input(row, dataset):\n",
    "    if dataset == \"xsum\" or dataset == \"cnn\":\n",
    "        return f\"Article: {row['context']}. {row['query']}\"\n",
    "    elif dataset == \"PubMedQA\":\n",
    "        return f\"Document: {row['context']}. {row['query']}\"\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def template_empty_input(row, dataset):\n",
    "    if dataset == \"xsum\" or dataset == \"cnn\":\n",
    "        return f\"Article: . {row['query']}\"\n",
    "    elif dataset == \"PubMedQA\":\n",
    "        return f\"Document: . {row['query']}\"\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c53db7de-5b48-46ee-b2a5-a421e0462638",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
    "                                          padding_side=\"left\",\n",
    "                                          use_fast=False,\n",
    "                                          token=access_token,\n",
    "                                          trust_remote_code=True,\n",
    "                                          cache_dir=cache_dir)\n",
    "if tokenizer.pad_token is None:\n",
    "    print(\"True\")\n",
    "    tokenizer.pad_token, tokenizer.pad_token_id = tokenizer.eos_token, tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d380e0c8-bdd3-4b8c-9bcf-29cd00f9b5c6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if dataset_name == \"PubMedQA\":\n",
    "    raw_test_set = load_dataset(\"qiaojin/PubMedQA\", \"pqa_labeled\", cache_dir=cache_dir)['train']\n",
    "elif dataset_name == 'xsum':\n",
    "    raw_test_set = load_dataset(dataset_name, split=\"test[:1000]\")\n",
    "elif dataset_name == 'cnn':\n",
    "    raw_test_set = load_dataset(\"abisee/cnn_dailymail\", \"3.0.0\", split=\"test[:1000]\", cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a68f6806-7061-4085-887c-09371a45f127",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "truncating documents...: 1000it [00:02, 387.71it/s]\n"
     ]
    }
   ],
   "source": [
    "test_set = pretokenize(dataset_name, raw_test_set, tokenizer, max_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02695206-4fe0-416f-b28c-24f5c57f1ac6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code for Pure DP decoding \n",
    "import torch.nn.functional as F\n",
    "from scipy.optimize import bisect\n",
    "\n",
    "def calculate_memorization(p, q, idx):\n",
    "    return abs(torch.log(p[idx]/q[idx])).numpy()#.cpu().numpy()\n",
    "\n",
    "def entropy(p):\n",
    "    return (-np.sum(p*np.log(p)))\n",
    "\n",
    "def calc_partition_loss(proj_logit, proj_output, pub_output, alpha, temperature):\n",
    "    max_loss = 0\n",
    "    for i in range(proj_logit.shape[0]):\n",
    "        proj_logit_i = torch.cat([proj_logit[:i, :], proj_logit[i+1:, :]])\n",
    "        proj_output_i = F.softmax(proj_logit_i / temperature, dim=-1).mean(dim=0)\n",
    "        ids = torch.nonzero(proj_output)\n",
    "        eps = renyi_priv_loss(proj_output[ids], proj_output_i[ids], alpha)\n",
    "        max_loss = max(max_loss, eps)\n",
    "    return max_loss\n",
    "\n",
    "def calc_group_memorization(output, ensemble_outputs, idx):\n",
    "    return [calculate_memorization(output, ensemble_outputs[i, :], idx) for i in range(0, ensemble_outputs.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cc4cab5-6d85-4bda-8639-b2079f259a9e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def post_calc_memorization(model,\n",
    "                   context_aware_input_ids,\n",
    "                   context_unaware_input_ids,\n",
    "                   response_input_ids,\n",
    "                   lambd,\n",
    "                   temperature,\n",
    "                   stop_token_ids,\n",
    "                   min_length,\n",
    "                   batch_size=None,\n",
    "                   ensemble_context_aware_input_ids=None,\n",
    "                  ):\n",
    "    mem_vals = []\n",
    "    for t in range(response_input_ids.shape[1]):\n",
    "        priv_context_aware_input_ids = torch.cat([context_aware_input_ids,\n",
    "                                      response_input_ids[:, :t]],\n",
    "                                     dim=1)\n",
    "        pub_logit = model(torch.cat([context_unaware_input_ids,\n",
    "                                     response_input_ids[:, :t]],\n",
    "                                    dim=1)\n",
    "                         ).logits.squeeze()[-1, :].type(torch.float64).cpu()\n",
    "        \n",
    "        priv_logit = model(priv_context_aware_input_ids).logits[-1, -1, :].type(torch.float64).cpu()\n",
    "\n",
    "        if batch_size != None:\n",
    "            N = ensemble_context_aware_input_ids.shape[0]\n",
    "            num_batch = N // batch_size + 1 if N % batch_size != 0 else N // batch_size\n",
    "            ensemble_priv_context_aware_input_ids = torch.cat([ensemble_context_aware_input_ids,\n",
    "                                      response_input_ids[:, :t].repeat(N, 1)],\n",
    "                                     dim=1)\n",
    "            ensemble_priv_logit = torch.cat([model(ensemble_priv_context_aware_input_ids[i*batch_size:(i+1)*batch_size]).logits[:, -1, :].type(torch.float64).cpu()\n",
    "                     for i in range(0, num_batch)], axis=0)\n",
    "            ensemble_proj_logit = lambd * ensemble_priv_logit + (1-lambd) * pub_logit.repeat(N, 1)\n",
    "        \n",
    "        proj_logit = lambd * priv_logit + (1-lambd) * pub_logit\n",
    "        \n",
    "        if t < min_length:\n",
    "            pub_logit[stop_token_ids[0]] = -float(\"Inf\")\n",
    "            proj_logit[stop_token_ids[0]] = -float(\"Inf\")\n",
    "            ensemble_proj_logit[:, stop_token_ids[0]] = -float(\"Inf\")\n",
    "            \n",
    "        if pub_logit.shape[0] > len(tokenizer):\n",
    "            pub_logit[len(tokenizer):pub_logit.shape[0]] = -float(\"Inf\")\n",
    "            proj_logit[len(tokenizer):pub_logit.shape[0]] = -float(\"Inf\")\n",
    "            ensemble_proj_logit[:, len(tokenizer):pub_logit.shape[0]] = -float(\"Inf\")\n",
    "        \n",
    "        pub_output = F.softmax(pub_logit / temperature, dim=-1)\n",
    "        priv_output = F.softmax(priv_logit / temperature, dim=-1)\n",
    "        proj_output = F.softmax(proj_logit / temperature, dim=-1)\n",
    "        \n",
    "        ids = torch.nonzero(pub_output)\n",
    "        if ensemble_context_aware_input_ids == None:\n",
    "            mem_val = calculate_memorization(proj_output[ids], pub_output[ids], response_input_ids[:, t].cpu())[0][0]\n",
    "        else:\n",
    "            ensemble_proj_output = F.softmax(ensemble_proj_logit / temperature, dim=-1)\n",
    "            mem_val = max(calc_group_memorization(proj_output[ids], ensemble_proj_output[:, ids].squeeze(-1), response_input_ids[:, t].cpu()))[0][0]\n",
    "        mem_vals.append(mem_val)\n",
    "        \n",
    "    return mem_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dfb0b43-28b3-4b55-89ef-e31a9c502360",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def partition(data, tokenizer, partition_length, dataset_name):\n",
    "    document_ids = tokenizer(data['context']).input_ids\n",
    "    ensemble = []\n",
    "    for i in range(0, len(document_ids), partition_length):\n",
    "        idx = (i+partition_length)\n",
    "        #ensemble = torch.cat([ensemble, input_ids[-1:, idx:i]], dim=1)\n",
    "        row = {'context': tokenizer.decode(document_ids[i:idx], skip_special_tokens=True), 'query': data['query']}\n",
    "        ensemble.append(template_input(row, dataset_name))\n",
    "    return ensemble\n",
    "\n",
    "def group_partition(data, tokenizer, partition_length, dataset_name):\n",
    "    document_ids = tokenizer(data['context']).input_ids\n",
    "    groups = [template_input(data, dataset_name)]\n",
    "    n_grams = []\n",
    "    for i in range(0, len(document_ids), partition_length):\n",
    "        idx = (i+partition_length)\n",
    "        group_i = document_ids[:i] + document_ids[idx:]\n",
    "        row = {'context': tokenizer.decode(group_i, skip_special_tokens=True), 'query': data['query']}\n",
    "        groups.append(template_input(row, dataset_name))\n",
    "    return groups\n",
    "\n",
    "\n",
    "def partition_n_gram(data, tokenizer, dataset_name, n):\n",
    "    document_ids = tokenizer(data['context']).input_ids\n",
    "    length = len(document_ids)\n",
    "    groups = []\n",
    "    n_grams = []\n",
    "    N = length - n + 1\n",
    "    if N < 0:\n",
    "        return [template_empty_input(data, dataset_name)]\n",
    "    for i in range(N):\n",
    "        removed_n_gram = document_ids[:i] + document_ids[i+n:]\n",
    "        n_grams.append(document_ids[i:i+n])\n",
    "        row = {'context': tokenizer.decode(removed_n_gram, skip_special_tokens=True), 'query': data['query']}\n",
    "        groups.append(template_input(row, dataset_name))\n",
    "    return groups, n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23fde59f-b6a9-4439-bdfa-0c0601e14a95",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cmad_generation(model,\n",
    "                  context_aware_input_ids,\n",
    "                  context_unaware_input_ids,\n",
    "                  lambd,\n",
    "                  temperature,\n",
    "                  max_length,\n",
    "                  min_length,\n",
    "                  stop_token_ids,\n",
    "                  device,\n",
    "                 ):\n",
    "    response_input_ids = torch.LongTensor([[]]).to(device)\n",
    "    for i in range(max_length):\n",
    "        priv_context_aware_input_ids = torch.cat([context_aware_input_ids,\n",
    "                                      response_input_ids.repeat(context_aware_input_ids.shape[0], 1)],\n",
    "                                     dim=1)\n",
    "        pub_logit = model(torch.cat([context_unaware_input_ids,\n",
    "                                     response_input_ids],\n",
    "                                    dim=1)\n",
    "                         ).logits.squeeze()[-1, :].type(torch.float64)\n",
    "\n",
    "        priv_logit = model(priv_context_aware_input_ids).logits[:, -1, :].type(torch.float64)\n",
    "        proj_logit = lambd * priv_logit + (1-lambd) * pub_logit.repeat(priv_logit.shape[0], 1)\n",
    "        \n",
    "        if i < min_length:\n",
    "            pub_logit[stop_token_ids[0]] = -float(\"Inf\")\n",
    "            proj_logit[:, stop_token_ids[0]] = -float(\"Inf\")\n",
    "            \n",
    "        if pub_logit.shape[0] > len(tokenizer):\n",
    "            pub_logit[len(tokenizer):pub_logit.shape[0]] = -float(\"Inf\")\n",
    "            proj_logit[:, len(tokenizer):pub_logit.shape[0]] = -float(\"Inf\")\n",
    "            \n",
    "        pub_output = F.softmax(pub_logit / temperature, dim=-1)\n",
    "        #priv_output = F.softmax(priv_logit, dim=-1)[-1]\n",
    "        proj_output = F.softmax(proj_logit / temperature, dim=-1)\n",
    "\n",
    "        pred_idx = proj_output[0].multinomial(1).view(1, -1).long()\n",
    "        if pred_idx.cpu()[0].item() in stop_token_ids:\n",
    "            break\n",
    "\n",
    "        response_input_ids = torch.cat([response_input_ids, pred_idx], dim=1)\n",
    "        del pred_idx\n",
    "    return response_input_ids.cpu()[0], 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9b8aa8a-0b65-4182-ba87-d7c7ba3d0441",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decode_experiment(test_set, model, tokenizer, lambd, temperature, dataset_name, min_length):\n",
    "    dp_predictions = []\n",
    "    stop_token_ids = [tokenizer.eos_token_id,\n",
    "                      tokenizer.pad_token_id,\n",
    "                     ]\n",
    "    doc_priv_loss = [] \n",
    "    for idx, data in tqdm(enumerate(test_set), total=len(test_set)):\n",
    "        context_unaware_tokenized_input = tokenizer(template_empty_input(data, dataset_name), return_tensors=\"pt\", padding=True)\n",
    "        context_aware_tokenized_input = tokenizer(template_input(data, dataset_name), return_tensors=\"pt\", padding=True)\n",
    "        with torch.no_grad():\n",
    "            dp_output, doc_eps = cmad_generation(model,\n",
    "                                    context_aware_tokenized_input.input_ids.to(DEVICE),\n",
    "                                    context_unaware_tokenized_input.input_ids.to(DEVICE),\n",
    "                                    lambd=lambd,\n",
    "                                    temperature=temperature,\n",
    "                                    max_length=max_new_tokens,\n",
    "                                    min_length=min_length,\n",
    "                                    stop_token_ids=stop_token_ids,\n",
    "                                    device=DEVICE,\n",
    "                                    )\n",
    "        decode_dp_output = tokenizer.decode(dp_output, skip_special_tokens=True)\n",
    "        dp_predictions.append(decode_dp_output)\n",
    "        doc_priv_loss.append(doc_eps)\n",
    "    return dp_predictions, doc_priv_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c430e6b-3217-4812-9184-e007c911433c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dir_name = \"results\"\n",
    "m_name = \"Meta-Llama-3-8B-Instruct\"\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295cf787-4c58-4b3c-84b1-778d8080ea15",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                  | 0/1000 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      " 52%|██████████████████████████████████████████████████████████████████████████████████████▌                                                                                 | 515/1000 [46:31<38:34,  4.77s/it]"
     ]
    }
   ],
   "source": [
    "os.makedirs(dir_name, exist_ok=True)\n",
    "lambds = [0.5, 1.0, 1.5]\n",
    "\n",
    "for lambd in lambds:\n",
    "    #file_name = f'{dataset_name}_{m_name}_{lambd}_context{max_input_length}.csv'\n",
    "    file_name = f'{dataset_name}_{m_name}_{lambd}.csv'\n",
    "    dp_predictions, dp_loss = decode_experiment(test_set, model, tokenizer, lambd=lambd, temperature=0.8, dataset_name=dataset_name, min_length=10)\n",
    "    df = pd.DataFrame({'generations': dp_predictions, 'privacy_loss': dp_loss})\n",
    "    df.to_csv(os.path.join(dir_name, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0dbd69e2-a5f1-4aa4-81c2-c95fdd69e550",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 30138.86it/s]\n"
     ]
    }
   ],
   "source": [
    "documents, references = [], []\n",
    "for idx, data in tqdm(enumerate(test_set), total=len(test_set)):\n",
    "    documents.append(data['context'])\n",
    "    references.append(data['summary'])\n",
    "evaluator = Evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b75a7ada-e105-4d13-9cb5-d14ee76356f5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "lambd=0.5\n",
    "file_name = f'{dataset_name}_{m_name}_{lambd}.csv'\n",
    "df = pd.read_csv(os.path.join(dir_name, file_name))\n",
    "doc_priv_loss = df['privacy_loss']\n",
    "predictions = df['generations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebc2e86d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ccf6a1b46b9410d89164b5523d02177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    token=access_token,\n",
    "    cache_dir=cache_dir,\n",
    "    #local_files_only=True,\n",
    "    #device_map=\"auto\",\n",
    "    #max_memory = {0: \"0GB\", 1: \"0GB\", 2: \"35GB\", 3: \"35GB\", 4: \"0GB\", 5: \"0GB\", 6: \"0GB\", 7: \"0GB\"}\n",
    "    ).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a3dd6c-11e3-4829-966c-4b43da6d9088",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "PubMedQA_Meta-Llama-3-8B-Instruct_1.0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "truncating documents...: 1000it [00:00, 1030.99it/s]\n",
      " 74%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                         | 745/1000 [4:42:09<1:38:24, 23.15s/it]"
     ]
    }
   ],
   "source": [
    "partition_len = max_input_length\n",
    "temperature=0.8\n",
    "stop_token_ids = [tokenizer.eos_token_id,\n",
    "                      tokenizer.pad_token_id,\n",
    "                     ]\n",
    "lambds = [1.0, 1.5, 0.5]\n",
    "mean_vals = []\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
    "                                      padding_side=\"left\",\n",
    "                                      use_fast=False,\n",
    "                                      token=access_token,\n",
    "                                      trust_remote_code=True,\n",
    "                                      cache_dir=cache_dir)\n",
    "if tokenizer.pad_token is None:\n",
    "    print(\"True\")\n",
    "    tokenizer.pad_token, tokenizer.pad_token_id = tokenizer.eos_token, tokenizer.eos_token_id\n",
    "\n",
    "batch_size = 32\n",
    "for n_gram_size in [2048]:\n",
    "    for lambd in lambds:\n",
    "        file_name = f'{dataset_name}_{m_name}_{lambd}.csv'\n",
    "        #file_name = f'{dataset_name}_{m_name}_{lambd}_context{context_len}.csv'\n",
    "        df = pd.read_csv(os.path.join(dir_name, file_name))\n",
    "        predictions = df['generations']\n",
    "        vals = []\n",
    "        print(file_name)\n",
    "        \n",
    "        test_set = pretokenize(dataset_name, raw_test_set, tokenizer, max_input_length)\n",
    "        query_set = test_set.select(range(1000))\n",
    "\n",
    "        for data, response in tqdm(zip(query_set, predictions), total=len(query_set)):\n",
    "            context_unaware_tokenized_input = tokenizer(template_empty_input(data, dataset_name), return_tensors=\"pt\", padding=True)\n",
    "            context_aware_tokenized_input = tokenizer(template_input(data, dataset_name), return_tensors=\"pt\", padding=True)\n",
    "            if n_gram_size == None:\n",
    "                ensemble_context_aware_tokenized_input_ids = None\n",
    "                batch_size = None\n",
    "            else:\n",
    "                ensemble = partition_n_gram(data, tokenizer, dataset_name, n_gram_size)\n",
    "                ensemble_context_aware_tokenized_input = tokenizer(ensemble, return_tensors=\"pt\", padding=True)\n",
    "                ensemble_context_aware_tokenized_input_ids = ensemble_context_aware_tokenized_input.input_ids.to(DEVICE)\n",
    "            response_tokenized_input = tokenizer(response, return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                cur_mem = post_calc_memorization(model,\n",
    "                                           context_aware_tokenized_input.input_ids.to(DEVICE),\n",
    "                                           context_unaware_tokenized_input.input_ids.to(DEVICE),\n",
    "                                           response_tokenized_input.input_ids[:, 1:].to(DEVICE),\n",
    "                                           lambd,\n",
    "                                           temperature,\n",
    "                                           stop_token_ids,\n",
    "                                           min_new_tokens,\n",
    "                                           batch_size,\n",
    "                                           ensemble_context_aware_tokenized_input_ids\n",
    "                                          )\n",
    "            vals.append(cur_mem)\n",
    "\n",
    "        mem_vals = np.zeros([len(vals),len(max(vals,key = lambda x: len(x)))])\n",
    "        for i,j in enumerate(vals):\n",
    "            mem_vals[i, 0:len(j)] = j\n",
    "        print(f\"N-gram size {n_gram_size}\\t Memorization: {np.mean(np.sum(mem_vals, axis=1))}\")\n",
    "        mean_vals.append(np.mean(np.sum(mem_vals, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf4ee72b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[66.38553924315379, 115.78231398467068, 17.25712741370103]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ded613",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mean_vals = np.mean(mem_vals, axis=0)\n",
    "plt.plot(np.arange(0, len(mean_vals)), mean_vals)\n",
    "plt.xlabel(\"Generation Length $|\\mathbf{y}|$\", fontsize=15)\n",
    "plt.ylabel(r'$\\mathbb{E}[f_{\\text{Mem}}(\\overline{p}_{\\theta})]$', fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.savefig(\"gen_len_analysis.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d1ad54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f426ec09-c9a8-4db5-8ffc-91f0972ca151",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/james/memorization_and_hallucination/venv/lib/python3.10/site-packages/lightning_fabric/utilities/cloud_io.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location=map_location)  # type: ignore[arg-type]\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.7.7 to v1.9.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file models/AlignScore-base.ckpt`\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/james/memorization_and_hallucination/venv/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:255: UserWarning: Found keys that are not in the model state dict but in the checkpoint: ['base_model.embeddings.position_ids']\n",
      "  rank_zero_warn(\n",
      "Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:51<00:00, 19.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1_fmeasure -> 29.12\n",
      "rouge1_precision -> 29.99\n",
      "rouge1_recall -> 31.10\n",
      "rouge2_fmeasure -> 8.82\n",
      "rouge2_precision -> 8.85\n",
      "rouge2_recall -> 9.80\n",
      "rougeL_fmeasure -> 20.17\n",
      "rougeL_precision -> 20.58\n",
      "rougeL_recall -> 21.81\n",
      "rougeLsum_fmeasure -> 24.08\n",
      "rougeLsum_precision -> 24.88\n",
      "rougeLsum_recall -> 25.69\n",
      "bertscore_precision -> 74.32\n",
      "bertscore_recall -> 74.76\n",
      "bertscore_f1 -> 74.51\n",
      "factkb -> 51.64\n",
      "alignscore -> 33.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result_dict = evaluator.evaluate(predictions, references, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0465a839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3851b65a21ef41788340ea1f98f48a54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    token=access_token,\n",
    "    cache_dir=cache_dir,\n",
    "    local_files_only=True,\n",
    "    #device_map=\"auto\",\n",
    "    #max_memory = {0: \"0GB\", 1: \"0GB\", 2: \"35GB\", 3: \"35GB\", 4: \"35GB\", 5: \"35GB\", 6: \"35GB\", 7: \"35GB\"}\n",
    "    ).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42608e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "calc_group_memorization() missing 1 required positional argument: 'idx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m proj_output \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(proj_logit \u001b[38;5;241m/\u001b[39m temperature, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     40\u001b[0m ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnonzero(pub_output)\n\u001b[0;32m---> 41\u001b[0m mem_val \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_group_memorization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproj_output\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_input_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: calc_group_memorization() missing 1 required positional argument: 'idx'"
     ]
    }
   ],
   "source": [
    "data, response = test_set[53], predictions[53]\n",
    "n = 2\n",
    "temperature=0.8\n",
    "\n",
    "context_unaware_tokenized_input = tokenizer(template_empty_input(data, dataset_name), return_tensors=\"pt\", padding=True)\n",
    "ensemble, n_grams = partition_n_gram(data, tokenizer, dataset_name, n)\n",
    "context_aware_tokenized_input = tokenizer(ensemble, return_tensors=\"pt\", max_length=max_input_length+25, padding=True, truncation=True)\n",
    "response_tokenized_input = tokenizer(response, return_tensors=\"pt\")\n",
    "\n",
    "context_aware_input_ids = context_aware_tokenized_input.input_ids.to(DEVICE)\n",
    "response_input_ids = response_tokenized_input.input_ids.to(DEVICE)\n",
    "context_unaware_input_ids = context_unaware_tokenized_input.input_ids.to(DEVICE)\n",
    "\n",
    "N = context_aware_input_ids.shape[0]\n",
    "priv_context_aware_input_ids = torch.cat([context_aware_input_ids,\n",
    "                              response_input_ids[:, :20].repeat(N, 1)],\n",
    "                             dim=1)\n",
    "batch_size = 1\n",
    "with torch.no_grad():\n",
    "    pub_logit = model(torch.cat([context_unaware_input_ids,\n",
    "                             response_input_ids[:, :20]],\n",
    "                            dim=1)\n",
    "                 ).logits.squeeze()[-1, :].type(torch.float64).cpu()\n",
    "    if batch_size == None:\n",
    "        priv_logit = model(priv_context_aware_input_ids).logits[:, -1, :].type(torch.float64).cpu()\n",
    "    else:\n",
    "        priv_logit = torch.stack([model(priv_context_aware_input_ids[i:(i+1)*batch_size]).logits[:, -1, :].type(torch.float64).cpu()\n",
    "                 for i in range(0, N, batch_size)]).squeeze(1)\n",
    "\n",
    "proj_logit = lambd * priv_logit + (1-lambd) * pub_logit.repeat(N, 1)\n",
    "\n",
    "if pub_logit.shape[0] > len(tokenizer):\n",
    "    pub_logit[len(tokenizer):pub_logit.shape[0]] = -float(\"Inf\")\n",
    "    proj_logit[:, len(tokenizer):pub_logit.shape[0]] = -float(\"Inf\")\n",
    "\n",
    "pub_output = F.softmax(pub_logit / temperature, dim=-1)\n",
    "priv_output = F.softmax(priv_logit / temperature, dim=-1)\n",
    "proj_output = F.softmax(proj_logit / temperature, dim=-1)\n",
    "\n",
    "ids = torch.nonzero(pub_output)\n",
    "mem_val = calc_group_memorization(proj_output[:, ids].squeeze(), response_input_ids[:, 20].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf16500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def value_to_color(value, min_value, max_value):\n",
    "    \"\"\" Map a float value to a color based on its position in the value range. \"\"\"\n",
    "    norm = mcolors.Normalize(vmin=min_value, vmax=max_value)\n",
    "    cmap = plt.get_cmap('coolwarm')  # You can choose different colormaps\n",
    "    return mcolors.to_hex(cmap(norm(value)))\n",
    "\n",
    "# Normalize and colorize\n",
    "min_value = min(mem_vals)\n",
    "max_value = max(mem_vals)\n",
    "\n",
    "colors = [value_to_color(val, min_value, max_value) for val in mem_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fadf9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "def colorize_text_html(vals, colors):\n",
    "    colored_text = ' '.join(\n",
    "        f'<span style=\"color: {color};\">{tokenizer.decode(input_ids)}</span>'\n",
    "        for input_ids, color in zip(vals, colors)\n",
    "    )\n",
    "    return f'<p>{colored_text}</p>'\n",
    "\n",
    "colored_text_html = colorize_text_html(data_tokenized_input_ids[47:141], colors[47:141])\n",
    "HTML(colored_text_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43932170",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.lib.colors import HexColor, black\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.lib.units import inch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import ScalarMappable\n",
    "import numpy as np\n",
    "\n",
    "def create_colored_pdf(vals, colors, filename):\n",
    "    c = canvas.Canvas(filename, pagesize=letter)\n",
    "    width, height = letter\n",
    "\n",
    "    # Set starting position\n",
    "    x, y = 50, height - 50\n",
    "    line_height = 14\n",
    "\n",
    "    # Draw text with colors\n",
    "    for input_ids, color in zip(vals, colors):\n",
    "        word = tokenizer.decode(input_ids)\n",
    "        color = HexColor(color)\n",
    "\n",
    "        # Set color and draw text\n",
    "        c.setFillColor(color)\n",
    "        c.drawString(x, y, word)\n",
    "        x += c.stringWidth(word + ' ', 'Helvetica', 12)\n",
    "\n",
    "        # Move to next line if needed\n",
    "        if x > width - 50:\n",
    "            x = 50\n",
    "            y -= line_height\n",
    "\n",
    "    # Draw the color scale image in PDF\n",
    "    #c.drawImage(color_scale_image, scale_x, scale_y, width=scale_width, height=scale_height)\n",
    "    \n",
    "    c.save()\n",
    "\n",
    "# Create the PDF\n",
    "create_colored_pdf(data_tokenized_input_ids[47:141], colors[47:141], \"colored_text.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a237aee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = canvas.Canvas(\"colored_text.pdf\", pagesize=letter)\n",
    "\n",
    "# Draw color scale\n",
    "scale_width = 5 * inch\n",
    "scale_height = 0.5 * inch\n",
    "scale_x = 50\n",
    "scale_y = 50\n",
    "c.setFillColor(black)\n",
    "c.rect(scale_x, scale_y, scale_width, scale_height, fill=0)\n",
    "\n",
    "# Create color scale using matplotlib\n",
    "fig, ax = plt.subplots(figsize=(5, 0.5), dpi=80)\n",
    "fig.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "cmap = plt.get_cmap('coolwarm')\n",
    "norm = mcolors.Normalize(vmin=min_value, vmax=max_value)\n",
    "sm = ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])\n",
    "cb = plt.colorbar(sm, cax=ax, orientation='horizontal')\n",
    "cb.ax.tick_params(labelsize=10)\n",
    "#cb.ax.set_title('Value Scale', fontsize=10)\n",
    "#plt.show()\n",
    "# Save color scale to an image\n",
    "color_scale_image = \"color_scale.pdf\"\n",
    "plt.savefig(color_scale_image, bbox_inches='tight', pad_inches=0)\n",
    "plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b596499e6b756ee3ee734d514920d364bec1c6a0ebf031bda292a137927d8dcb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
