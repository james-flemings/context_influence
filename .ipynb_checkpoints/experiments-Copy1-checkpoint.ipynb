{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db139b29-c059-4b4f-8ad5-bf77274a15cc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import torchmetrics\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, GenerationConfig\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "334efd5c-e16d-44ce-94de-ad6c7eccc031",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ee6c63a-ce6d-4610-93d9-16a57f847c21",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparamters \n",
    "top_k = 50\n",
    "top_p = 0.9\n",
    "temp = 0.8\n",
    "min_new_tokens = 10\n",
    "max_new_tokens = 50\n",
    "do_sample=True\n",
    "num_beams=1\n",
    "\n",
    "dataset_name=\"PubMedQA\"\n",
    "model_name= \"EleutherAI/pythia-1.4b\"\n",
    "batch_size=8\n",
    "max_input_length=2048\n",
    "DEVICE = \"cuda:2\" if torch.cuda.is_available() else \"cpu\"\n",
    "access_token = \"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\"\n",
    "cache_dir=\"/data/james/.cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc9a5a77-0973-48ac-8290-2e573d2bc654",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, metrics=None):\n",
    "        if not metrics:\n",
    "            metrics = [\"rouge\", \"sacre_bleu\", \"bertscore\", \"factkb\"]\n",
    "        self.metrics = metrics\n",
    "    \n",
    "    def evaluate(self, predictions, references, documents, metrics=[\"rouge\", \"bertscore\", \"factkb\", \"alignscore\"]):\n",
    "        result_dict = OrderedDict()\n",
    "        if \"rouge\" in metrics:\n",
    "            rouge_dict = self.calculate_rouge(predictions, references)\n",
    "            for k, v in rouge_dict.items():\n",
    "                result_dict[k] = v\n",
    "        if \"sacre_bleu\" in metrics:\n",
    "            sacre_bleu_dict = self.calculate_sacrebleu(predictions, references)\n",
    "            for k, v in sacre_bleu_dict.items():\n",
    "                result_dict[k] = v\n",
    "        if \"bertscore\" in metrics:\n",
    "            bertscore_dict = self.calculate_bertscore(predictions, references)\n",
    "            for k, v in bertscore_dict.items():\n",
    "                result_dict[k] = v\n",
    "        if \"factkb\" in metrics:\n",
    "            result_dict[\"factkb\"] = self.calculate_factkb(predictions, documents)\n",
    "            \n",
    "        if \"alignscore\" in metrics:\n",
    "            result_dict[\"alignscore\"] = self.calculate_alignscore(predictions, documents) \n",
    "\n",
    "        for k, v in result_dict.items():\n",
    "            print(f\"{k} -> {v*100:.2f}\")\n",
    "        return result_dict\n",
    "\n",
    "    def calculate_rouge(self, predictions, references):\n",
    "        from torchmetrics.functional.text.rouge import rouge_score\n",
    "        rouge_dict = rouge_score(preds=predictions, target=references)\n",
    "        return {k: v.item() for k, v in rouge_dict.items()}\n",
    "\n",
    "    def calculate_sacrebleu(self, predictions, references):\n",
    "        from torchmetrics.functional.text import sacre_bleu_score\n",
    "        score = sacre_bleu_score(preds=predictions, target=[[i] for i in references])\n",
    "        return {\"sacre_bleu\": score.item()}\n",
    "\n",
    "    def calculate_bertscore(self, predictions, references):\n",
    "        import evaluate\n",
    "        bertscore = evaluate.load(\"bertscore\")\n",
    "        bertscore_dict = bertscore.compute(predictions=predictions, references=references, model_type=\"roberta-large-mnli\")\n",
    "        res = {\"bertscore_precision\": np.mean(bertscore_dict[\"precision\"]), \"bertscore_recall\": np.mean(bertscore_dict[\"recall\"]), \"bertscore_f1\": np.mean(bertscore_dict[\"f1\"])}\n",
    "        return {k: v.item() for k, v in res.items()}\n",
    "    \n",
    "    def calculate_alignscore(self, predictions, documents):\n",
    "        from AlignScore.src.alignscore import AlignScore\n",
    "        ckpt_path = \"models/AlignScore-base.ckpt\"\n",
    "        align_scorer = AlignScore(model='roberta-base', batch_size=8, device=DEVICE, ckpt_path=ckpt_path, evaluation_mode='nli_sp')\n",
    "        alignscore_result = align_scorer.score(contexts=documents, claims=predictions)\n",
    "        #total_result['AlignScore'] = 100*np.mean(alignscore_result)\n",
    "        return np.mean(alignscore_result)\n",
    "\n",
    "    def calculate_factkb(self, predictions, documents):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\", padding=\"max_length\", truncation=True, cache_dir=cache_dir)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\"bunsenfeng/FactKB\", torch_dtype=torch.float16, cache_dir=cache_dir)\n",
    "        model = model.to(DEVICE)\n",
    "        res = []\n",
    "        for i in range(len(predictions)):\n",
    "            input_pretokenized = f\"{predictions[i]} {tokenizer.sep_token} {documents[i]}\"\n",
    "            tokenized_input = tokenizer(input_pretokenized, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "            with torch.no_grad():\n",
    "                output = model(input_ids=tokenized_input.input_ids.to(DEVICE))\n",
    "            logits = torch.softmax(output.logits, dim=1)  # (bz, 2)\n",
    "            res.append(logits.squeeze()[-1].item())\n",
    "        return np.mean(res)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e6f9c07-b5b5-461d-adb1-00580c2aa643",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "def xsum_pretokenize(dataset, tokenizer, max_input_length):\n",
    "    data = {\"context\": [], \"query\": [], \"summary\": []}\n",
    "    for i, row in tqdm(enumerate(dataset), desc=\"truncating documents...\"):\n",
    "        trunc_doc = tokenizer.batch_decode(tokenizer(row['document'], return_tensors=\"pt\", max_length=max_input_length,  truncation=True).input_ids, skip_special_tokens=True)[0]\n",
    "        data['context'].append(trunc_doc)\n",
    "        data['summary'].append(row['summary'])\n",
    "        data[\"query\"].append(\"Summarize the article in one sentence. Summary:\")\n",
    "    return Dataset.from_dict(data)\n",
    "\n",
    "def cnn_pretokenize(dataset, tokenizer, max_input_length):\n",
    "    data = {\"context\": [], \"query\": [], \"summary\": []}\n",
    "    for i, row in tqdm(enumerate(dataset), desc=\"truncating documents...\"):\n",
    "        trunc_doc = tokenizer.batch_decode(tokenizer(row['article'], return_tensors=\"pt\", max_length=max_input_length,  truncation=True).input_ids, skip_special_tokens=True)[0]\n",
    "        data['context'].append(trunc_doc)\n",
    "        data['summary'].append(row['highlights'])\n",
    "        data['query'].append(\"Summary of the above news article:\")\n",
    "    return Dataset.from_dict(data)\n",
    "\n",
    "def pubmedqa_pretokenize(dataset, tokenizer, max_input_length):\n",
    "    data = {\"context\": [], \"query\": [], \"summary\": []}\n",
    "    for i, row in tqdm(enumerate(dataset), desc=\"truncating documents...\"):\n",
    "        context= ''.join(c for c in row['context']['contexts'])\n",
    "        trunc_doc = tokenizer.batch_decode(tokenizer(context, return_tensors=\"pt\", max_length=max_input_length, truncation=True).input_ids, skip_special_tokens=True)[0]\n",
    "        data['context'].append(trunc_doc)\n",
    "        data['summary'].append(row['long_answer'])\n",
    "        data['query'].append(f\"Question: {row['question']}. Answer:\")\n",
    "    return Dataset.from_dict(data)\n",
    "\n",
    "def pretokenize(dataset_name, dataset, tokenizer, max_input_length):\n",
    "    if dataset_name == \"xsum\":\n",
    "        return xsum_pretokenize(dataset, tokenizer, max_input_length)\n",
    "    elif dataset_name == \"cnn\":\n",
    "        return cnn_pretokenize(dataset, tokenizer, max_input_length)\n",
    "    elif dataset_name == \"PubMedQA\":\n",
    "        return pubmedqa_pretokenize(dataset, tokenizer, max_input_length)\n",
    "    return None\n",
    "\n",
    "def template_input(row, dataset):\n",
    "    if dataset == \"xsum\" or dataset == \"cnn\":\n",
    "        return f\"Article: {row['context']}. {row['query']}\"\n",
    "    elif dataset == \"PubMedQA\":\n",
    "        return f\"Document: {row['context']}. {row['query']}\"\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def template_empty_input(row, dataset):\n",
    "    if dataset == \"xsum\" or dataset == \"cnn\":\n",
    "        return f\"Article: . {row['query']}\"\n",
    "    elif dataset == \"PubMedQA\":\n",
    "        return f\"Document: . {row['query']}\"\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c53db7de-5b48-46ee-b2a5-a421e0462638",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
    "                                          padding_side=\"left\",\n",
    "                                          use_fast=True,\n",
    "                                          token=access_token,\n",
    "                                          trust_remote_code=True,\n",
    "                                          cache_dir=cache_dir,\n",
    "                                          revision=\"step1000\",\n",
    "                                         )\n",
    "if tokenizer.pad_token is None:\n",
    "    print(\"True\")\n",
    "    tokenizer.pad_token, tokenizer.pad_token_id = tokenizer.eos_token, tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d380e0c8-bdd3-4b8c-9bcf-29cd00f9b5c6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if dataset_name == \"PubMedQA\":\n",
    "    raw_test_set = load_dataset(\"qiaojin/PubMedQA\", \"pqa_labeled\", cache_dir=cache_dir)['train']\n",
    "elif dataset_name == 'xsum':\n",
    "    raw_test_set = load_dataset(dataset_name, split=\"test[:1000]\")\n",
    "elif dataset_name == 'cnn':\n",
    "    raw_test_set = load_dataset(\"abisee/cnn_dailymail\", \"3.0.0\", split=\"test[:1000]\", cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a68f6806-7061-4085-887c-09371a45f127",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "truncating documents...: 1000it [00:01, 841.68it/s]\n"
     ]
    }
   ],
   "source": [
    "test_set = pretokenize(dataset_name, raw_test_set, tokenizer, max_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02695206-4fe0-416f-b28c-24f5c57f1ac6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code for Pure DP decoding \n",
    "import torch.nn.functional as F\n",
    "from scipy.optimize import bisect\n",
    "\n",
    "def calculate_memorization(p, q, idx):\n",
    "    return abs(torch.log(p[idx]/q[idx])).numpy()#.cpu().numpy()\n",
    "\n",
    "def entropy(p):\n",
    "    return (-np.sum(p*np.log(p)))\n",
    "\n",
    "def calc_partition_loss(proj_logit, proj_output, pub_output, alpha, temperature):\n",
    "    max_loss = 0\n",
    "    for i in range(proj_logit.shape[0]):\n",
    "        proj_logit_i = torch.cat([proj_logit[:i, :], proj_logit[i+1:, :]])\n",
    "        proj_output_i = F.softmax(proj_logit_i / temperature, dim=-1).mean(dim=0)\n",
    "        ids = torch.nonzero(proj_output)\n",
    "        eps = renyi_priv_loss(proj_output[ids], proj_output_i[ids], alpha)\n",
    "        max_loss = max(max_loss, eps)\n",
    "    return max_loss\n",
    "\n",
    "def calc_group_memorization(output, ensemble_outputs, idx):\n",
    "    return [calculate_memorization(output, ensemble_outputs[i, :], idx)[0][0] for i in range(0, ensemble_outputs.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3177cb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_distributions(model, \n",
    "                      context_aware_input_ids, \n",
    "                      context_unaware_input_ids,\n",
    "                      response_input_ids,\n",
    "                      lambd,\n",
    "                      temperature,\n",
    "                      stop_token_ids,\n",
    "                      min_length,\n",
    "                      t,\n",
    "                      batch_size=None,\n",
    "                      ensemble_context_aware_input_ids=None,\n",
    "                      ):\n",
    "    ensemble_proj_output = None\n",
    "    priv_context_aware_input_ids = torch.cat([context_aware_input_ids,\n",
    "                                      response_input_ids[:, :t]],\n",
    "                                     dim=1)\n",
    "    pub_logit = model(torch.cat([context_unaware_input_ids,\n",
    "                                 response_input_ids[:, :t]],\n",
    "                                dim=1)\n",
    "                     ).logits.squeeze()[-1, :].type(torch.float64).cpu()\n",
    "\n",
    "    priv_logit = model(priv_context_aware_input_ids).logits[-1, -1, :].type(torch.float64).cpu()\n",
    "    \n",
    "    if batch_size != None:\n",
    "        N = ensemble_context_aware_input_ids.shape[0]\n",
    "        num_batch = N // batch_size + 1 if N % batch_size != 0 else N // batch_size\n",
    "        ensemble_priv_context_aware_input_ids = torch.cat([ensemble_context_aware_input_ids,\n",
    "                                  response_input_ids[:, :t].repeat(N, 1)],\n",
    "                                 dim=1)\n",
    "        ensemble_priv_logit = torch.cat([model(ensemble_priv_context_aware_input_ids[i*batch_size:(i+1)*batch_size]).logits[:, -1, :].type(torch.float64).cpu()\n",
    "                 for i in range(0, num_batch)], axis=0)\n",
    "        ensemble_proj_logit = lambd * ensemble_priv_logit + (1-lambd) * pub_logit.repeat(N, 1)\n",
    "\n",
    "    proj_logit = lambd * priv_logit + (1-lambd) * pub_logit\n",
    "\n",
    "    if t < min_length:\n",
    "        pub_logit[stop_token_ids[0]] = -float(\"Inf\")\n",
    "        proj_logit[stop_token_ids[0]] = -float(\"Inf\")\n",
    "        if ensemble_context_aware_input_ids != None:\n",
    "            ensemble_proj_logit[:, stop_token_ids[0]] = -float(\"Inf\")\n",
    "\n",
    "    if pub_logit.shape[0] > len(tokenizer):\n",
    "        pub_logit[len(tokenizer):pub_logit.shape[0]] = -float(\"Inf\")\n",
    "        proj_logit[len(tokenizer):pub_logit.shape[0]] = -float(\"Inf\")\n",
    "        if ensemble_context_aware_input_ids != None:\n",
    "            ensemble_proj_logit[:, len(tokenizer):pub_logit.shape[0]] = -float(\"Inf\")\n",
    "\n",
    "    pub_output = F.softmax(pub_logit / temperature, dim=-1)\n",
    "    priv_output = F.softmax(priv_logit / temperature, dim=-1)\n",
    "    proj_output = F.softmax(proj_logit / temperature, dim=-1)\n",
    "    if ensemble_context_aware_input_ids != None:\n",
    "        ensemble_proj_output = F.softmax(ensemble_proj_logit / temperature, dim=-1)\n",
    "    return proj_output, pub_output, ensemble_proj_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cc4cab5-6d85-4bda-8639-b2079f259a9e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def post_calc_memorization(model,\n",
    "                   context_aware_input_ids,\n",
    "                   context_unaware_input_ids,\n",
    "                   response_input_ids,\n",
    "                   lambd,\n",
    "                   temperature,\n",
    "                   stop_token_ids,\n",
    "                   min_length,\n",
    "                   batch_size=None,\n",
    "                   ensemble_context_aware_input_ids=None,\n",
    "                  ):\n",
    "    mem_vals = []\n",
    "    for t in range(response_input_ids.shape[1]):\n",
    "        proj_output, pub_output, ensemble_proj_output = calc_distributions(model, \n",
    "                                                                          context_aware_input_ids,\n",
    "                                                                          context_unaware_input_ids,\n",
    "                                                                          response_input_ids,\n",
    "                                                                          lambd,\n",
    "                                                                          temperature,\n",
    "                                                                          stop_token_ids,\n",
    "                                                                          min_length,\n",
    "                                                                          t,\n",
    "                                                                          batch_size,\n",
    "                                                                          ensemble_context_aware_input_ids)\n",
    "        ids = torch.nonzero(pub_output)\n",
    "        if ensemble_context_aware_input_ids == None:\n",
    "            mem_val = calculate_memorization(proj_output[ids], pub_output[ids], response_input_ids[:, t].cpu())[0][0]\n",
    "        else:\n",
    "            mem_val = max(calc_group_memorization(proj_output[ids], ensemble_proj_output[:, ids].squeeze(-1), response_input_ids[:, t].cpu()))\n",
    "        mem_vals.append(mem_val)    \n",
    "    return mem_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dfb0b43-28b3-4b55-89ef-e31a9c502360",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def partition(data, tokenizer, partition_length, dataset_name):\n",
    "    document_ids = tokenizer(data['context']).input_ids\n",
    "    ensemble = []\n",
    "    for i in range(0, len(document_ids), partition_length):\n",
    "        idx = (i+partition_length)\n",
    "        #ensemble = torch.cat([ensemble, input_ids[-1:, idx:i]], dim=1)\n",
    "        row = {'context': tokenizer.decode(document_ids[i:idx], skip_special_tokens=True), 'query': data['query']}\n",
    "        ensemble.append(template_input(row, dataset_name))\n",
    "    return ensemble\n",
    "\n",
    "def group_partition(data, tokenizer, partition_length, dataset_name):\n",
    "    document_ids = tokenizer(data['context']).input_ids\n",
    "    groups = [template_input(data, dataset_name)]\n",
    "    for i in range(0, len(document_ids), partition_length):\n",
    "        idx = (i+partition_length)\n",
    "        group_i = document_ids[:i] + document_ids[idx:]\n",
    "        row = {'context': tokenizer.decode(group_i, skip_special_tokens=True), 'query': data['query']}\n",
    "        groups.append(template_input(row, dataset_name))\n",
    "    return groups\n",
    "\n",
    "def partition_n_gram(data, tokenizer, dataset_name, n):\n",
    "    document_ids = tokenizer(data['context']).input_ids\n",
    "    length = len(document_ids)\n",
    "    groups = []\n",
    "    n_grams = []\n",
    "    N = length - n + 1\n",
    "    if N < 0:\n",
    "        return [template_empty_input(data, dataset_name)]\n",
    "    for i in range(N):\n",
    "        removed_n_gram = document_ids[:i] + document_ids[i+n:]\n",
    "        n_grams.append(document_ids[i:i+n])\n",
    "        row = {'context': tokenizer.decode(removed_n_gram, skip_special_tokens=True), 'query': data['query']}\n",
    "        groups.append(template_input(row, dataset_name))\n",
    "    return groups, n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23fde59f-b6a9-4439-bdfa-0c0601e14a95",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cmad_generation(model,\n",
    "                  context_aware_input_ids,\n",
    "                  context_unaware_input_ids,\n",
    "                  lambd,\n",
    "                  temperature,\n",
    "                  max_length,\n",
    "                  min_length,\n",
    "                  stop_token_ids,\n",
    "                  device,\n",
    "                 ):\n",
    "    response_input_ids = torch.LongTensor([[]]).to(device)\n",
    "    for i in range(max_length):\n",
    "        proj_output, pub_output, ensemble_proj_output = calc_distributions(model, \n",
    "                                                                          context_aware_input_ids,\n",
    "                                                                          context_unaware_input_ids,\n",
    "                                                                          response_input_ids,\n",
    "                                                                          lambd,\n",
    "                                                                          temperature,\n",
    "                                                                          stop_token_ids,\n",
    "                                                                          min_length,\n",
    "                                                                          i,\n",
    "                                                                          )\n",
    "        pred_idx = proj_output.multinomial(1).view(1, -1).long().to(device)\n",
    "        if pred_idx.cpu()[0].item() in stop_token_ids:\n",
    "            break\n",
    "\n",
    "        response_input_ids = torch.cat([response_input_ids, pred_idx], dim=1)\n",
    "        del pred_idx\n",
    "    return response_input_ids.cpu()[0], 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9b8aa8a-0b65-4182-ba87-d7c7ba3d0441",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decode_experiment(test_set, model, tokenizer, lambd, temperature, dataset_name, min_length):\n",
    "    dp_predictions = []\n",
    "    stop_token_ids = [tokenizer.eos_token_id,\n",
    "                      tokenizer.pad_token_id,\n",
    "                     ]\n",
    "    doc_priv_loss = [] \n",
    "    for idx, data in tqdm(enumerate(test_set), total=len(test_set)):\n",
    "        context_unaware_tokenized_input = tokenizer(template_empty_input(data, dataset_name), return_tensors=\"pt\", padding=True)\n",
    "        context_aware_tokenized_input = tokenizer(template_input(data, dataset_name), return_tensors=\"pt\", padding=True)\n",
    "        with torch.no_grad():\n",
    "            dp_output, doc_eps = cmad_generation(model,\n",
    "                                    context_aware_tokenized_input.input_ids.to(DEVICE),\n",
    "                                    context_unaware_tokenized_input.input_ids.to(DEVICE),\n",
    "                                    lambd=lambd,\n",
    "                                    temperature=temperature,\n",
    "                                    max_length=max_new_tokens,\n",
    "                                    min_length=min_length,\n",
    "                                    stop_token_ids=stop_token_ids,\n",
    "                                    device=DEVICE,\n",
    "                                    )\n",
    "        decode_dp_output = tokenizer.decode(dp_output, skip_special_tokens=True)\n",
    "        dp_predictions.append(decode_dp_output)\n",
    "        doc_priv_loss.append(doc_eps)\n",
    "    return dp_predictions, doc_priv_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c430e6b-3217-4812-9184-e007c911433c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dir_name = \"results\"\n",
    "m_name = \"pythia-1.4b\"\n",
    "model_name = \"EleutherAI/pythia-1.4b\"\n",
    "revision=\"step1000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295cf787-4c58-4b3c-84b1-778d8080ea15",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "truncating documents...: 1000it [00:01, 895.67it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [3:06:39<00:00, 11.20s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "truncating documents...: 1000it [00:01, 900.11it/s]\n",
      " 23%|██████████████████████████████████████▌                                                                                                                               | 232/1000 [44:08<2:37:55, 12.34s/it]"
     ]
    }
   ],
   "source": [
    "os.makedirs(dir_name, exist_ok=True)\n",
    "lambds = [1.0]\n",
    "#model_names = [\"facebook/opt-6.7b\"]\n",
    "#m_names = [\"opt-6.7b\"]\n",
    "#for model_name, m_name in zip(model_names, m_names):\n",
    "for revision in [\"step23000\", \"step44000\", \"step65000\", \"step85000\", \"step105000\", \"step126000\"]:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
    "                                          padding_side=\"left\",\n",
    "                                          use_fast=True,\n",
    "                                          token=access_token,\n",
    "                                          trust_remote_code=True,\n",
    "                                          cache_dir=cache_dir,\n",
    "                                          revision=revision)\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        print(\"True\")\n",
    "        tokenizer.pad_token, tokenizer.pad_token_id = tokenizer.eos_token, tokenizer.eos_token_id\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        token=access_token,\n",
    "        cache_dir=cache_dir,\n",
    "        revision=revision\n",
    "        #device_map=\"auto\"\n",
    "        ).to(DEVICE)\n",
    "    \n",
    "    test_set = pretokenize(dataset_name, raw_test_set, tokenizer, max_input_length)\n",
    "    \n",
    "    for lambd in lambds:\n",
    "        #file_name = f'{dataset_name}_{m_name}_{lambd}_context{max_input_length}.csv'\n",
    "        file_name = f'{dataset_name}_{m_name}_{lambd}_{revision}.csv'\n",
    "        dp_predictions, dp_loss = decode_experiment(test_set, model, tokenizer, lambd=lambd, temperature=0.8, dataset_name=dataset_name, min_length=10)\n",
    "        df = pd.DataFrame({'generations': dp_predictions, 'privacy_loss': dp_loss})\n",
    "        df.to_csv(os.path.join(dir_name, file_name))\n",
    "    model.cpu()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbd69e2-a5f1-4aa4-81c2-c95fdd69e550",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents, references = [], []\n",
    "for idx, data in tqdm(enumerate(test_set), total=len(test_set)):\n",
    "    documents.append(data['context'])\n",
    "    references.append(data['summary'])\n",
    "evaluator = Evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b75a7ada-e105-4d13-9cb5-d14ee76356f5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "lambd=1.0\n",
    "file_name = f'{dataset_name}_{m_name}_{lambd}.csv'\n",
    "df = pd.read_csv(os.path.join(dir_name, file_name))\n",
    "doc_priv_loss = df['privacy_loss']\n",
    "predictions = df['generations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebc2e86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    token=access_token,\n",
    "    cache_dir=cache_dir,\n",
    "    local_files_only=True,\n",
    "    #device_map=\"auto\",\n",
    "    #max_memory = {0: \"0GB\", 1: \"0GB\", 2: \"35GB\", 3: \"35GB\", 4: \"0GB\", 5: \"0GB\", 6: \"0GB\", 7: \"0GB\"}\n",
    "    ).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38a3dd6c-11e3-4829-966c-4b43da6d9088",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "PubMedQA_pythia-1.4b_1.0_step23000.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "truncating documents...: 1000it [00:01, 903.70it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [1:59:18<00:00,  7.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-gram size None\t Memorization: 39.27287736949486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "PubMedQA_pythia-1.4b_1.0_step44000.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "truncating documents...: 1000it [00:01, 989.38it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [1:55:45<00:00,  6.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-gram size None\t Memorization: 40.337728452295124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "PubMedQA_pythia-1.4b_1.0_step65000.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "truncating documents...: 1000it [00:01, 989.52it/s]\n",
      " 53%|████████████████████████████████████████████████████████████████████████████████████████▌                                                                               | 527/1000 [59:04<53:01,  6.73s/it]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 50276 is out of bounds for dimension 0 with size 50276",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m     response_tokenized_input \u001b[38;5;241m=\u001b[39m tokenizer(response, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 59\u001b[0m         cur_mem \u001b[38;5;241m=\u001b[39m \u001b[43mpost_calc_memorization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mcontext_aware_tokenized_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mcontext_unaware_tokenized_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mresponse_tokenized_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mlambd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mstop_token_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mmin_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mensemble_context_aware_tokenized_input_ids\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     vals\u001b[38;5;241m.\u001b[39mappend(cur_mem)\n\u001b[1;32m     71\u001b[0m model\u001b[38;5;241m.\u001b[39mcpu()\n",
      "Cell \u001b[0;32mIn[11], line 27\u001b[0m, in \u001b[0;36mpost_calc_memorization\u001b[0;34m(model, context_aware_input_ids, context_unaware_input_ids, response_input_ids, lambd, temperature, stop_token_ids, min_length, batch_size, ensemble_context_aware_input_ids)\u001b[0m\n\u001b[1;32m     25\u001b[0m ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnonzero(pub_output)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensemble_context_aware_input_ids \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m     mem_val \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_memorization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproj_output\u001b[49m\u001b[43m[\u001b[49m\u001b[43mids\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpub_output\u001b[49m\u001b[43m[\u001b[49m\u001b[43mids\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_input_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m     mem_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(calc_group_memorization(proj_output[ids], ensemble_proj_output[:, ids]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), response_input_ids[:, t]\u001b[38;5;241m.\u001b[39mcpu()))\n",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m, in \u001b[0;36mcalculate_memorization\u001b[0;34m(p, q, idx)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_memorization\u001b[39m(p, q, idx):\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(torch\u001b[38;5;241m.\u001b[39mlog(\u001b[43mp\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m/\u001b[39mq[idx]))\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[0;31mIndexError\u001b[0m: index 50276 is out of bounds for dimension 0 with size 50276"
     ]
    }
   ],
   "source": [
    "partition_len = max_input_length\n",
    "temperature=0.8\n",
    "stop_token_ids = [tokenizer.eos_token_id,\n",
    "                      tokenizer.pad_token_id,\n",
    "                     ]\n",
    "lambds = [1.0]\n",
    "mean_vals = []\n",
    "\n",
    "batch_size = 32\n",
    "n_gram_size = None\n",
    "for revision in [\"step23000\", \"step44000\", \"step65000\", \"step85000\", \"step105000\", \"step126000\"]:\n",
    "    for lambd in lambds:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
    "                                      padding_side=\"left\",\n",
    "                                      use_fast=True,\n",
    "                                      token=access_token,\n",
    "                                      trust_remote_code=True,\n",
    "                                      cache_dir=cache_dir,\n",
    "                                      revision=revision,\n",
    "                                      )\n",
    "        if tokenizer.pad_token is None:\n",
    "            print(\"True\")\n",
    "            tokenizer.pad_token, tokenizer.pad_token_id = tokenizer.eos_token, tokenizer.eos_token_id\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    trust_remote_code=True,\n",
    "                    torch_dtype=torch.float16,\n",
    "                    token=access_token,\n",
    "                    cache_dir=cache_dir,\n",
    "                    #local_files_only=True,\n",
    "                    revision=revision,\n",
    "                    #device_map=\"auto\",\n",
    "                    #max_memory = {0: \"0GB\", 1: \"0GB\", 2: \"35GB\", 3: \"35GB\", 4: \"0GB\", 5: \"0GB\", 6: \"0GB\", 7: \"0GB\"}\n",
    "                    ).to(DEVICE)\n",
    "        \n",
    "        file_name = f'{dataset_name}_{m_name}_{lambd}_{revision}.csv'\n",
    "        #file_name = f'{dataset_name}_{m_name}_{lambd}_context{context_len}.csv'\n",
    "        df = pd.read_csv(os.path.join(dir_name, file_name))\n",
    "        predictions = df['generations']\n",
    "        vals = []\n",
    "        print(file_name)\n",
    "        \n",
    "        test_set = pretokenize(dataset_name, raw_test_set, tokenizer, max_input_length)\n",
    "        query_set = test_set.select(range(1000))\n",
    "\n",
    "        for data, response in tqdm(zip(query_set, predictions), total=len(query_set)):\n",
    "            context_unaware_tokenized_input = tokenizer(template_empty_input(data, dataset_name), return_tensors=\"pt\", padding=True)\n",
    "            context_aware_tokenized_input = tokenizer(template_input(data, dataset_name), return_tensors=\"pt\", padding=True)\n",
    "            if n_gram_size == None:\n",
    "                ensemble_context_aware_tokenized_input_ids = None\n",
    "                batch_size = None\n",
    "            else:\n",
    "                ensemble, _ = partition_n_gram(data, tokenizer, dataset_name, n_gram_size)\n",
    "                ensemble_context_aware_tokenized_input = tokenizer(ensemble, return_tensors=\"pt\", padding=True)\n",
    "                ensemble_context_aware_tokenized_input_ids = ensemble_context_aware_tokenized_input.input_ids.to(DEVICE)\n",
    "            response_tokenized_input = tokenizer(response, return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                cur_mem = post_calc_memorization(model,\n",
    "                                           context_aware_tokenized_input.input_ids.to(DEVICE),\n",
    "                                           context_unaware_tokenized_input.input_ids.to(DEVICE),\n",
    "                                           response_tokenized_input.input_ids[:, 1:].to(DEVICE),\n",
    "                                           lambd,\n",
    "                                           temperature,\n",
    "                                           stop_token_ids,\n",
    "                                           min_new_tokens,\n",
    "                                           batch_size,\n",
    "                                           ensemble_context_aware_tokenized_input_ids\n",
    "                                          )\n",
    "            vals.append(cur_mem)\n",
    "        model.cpu()\n",
    "        del model\n",
    "        mem_vals = np.zeros([len(vals),len(max(vals,key = lambda x: len(x)))])\n",
    "        mem_vals[:] = np.nan\n",
    "        for i,j in enumerate(vals):\n",
    "            mem_vals[i, 0:len(j)] = j\n",
    "        print(f\"N-gram size {n_gram_size}\\t Memorization: {np.nanmean(np.nansum(mem_vals, axis=1))}\")\n",
    "        mean_vals.append(np.nanmean(np.nansum(mem_vals, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0528c7de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  '"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([50276])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac75a528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' No (1\\xa0)\\n\\nInteraction between adjacent structures {#Sec13}\\n--------------------------------------\\n\\nTwo numerical evaluations were performed. The first evaluation was the point of intersection between adjacent structures *at risk* during drilling and anchor insertion. Five hundred fifty'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[526]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a8067f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [50276], 'attention_mask': [1]}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "947faee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[39.27287736949486, 40.337728452295124]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8fe8d2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-gram size None\t Memorization: 40.43289637687549\n"
     ]
    }
   ],
   "source": [
    "mem_vals = np.zeros([len(vals),len(max(vals,key = lambda x: len(x)))])\n",
    "mem_vals[:] = np.nan\n",
    "for i,j in enumerate(vals):\n",
    "    mem_vals[i, 0:len(j)] = j\n",
    "print(f\"N-gram size {n_gram_size}\\t Memorization: {np.nanmean(np.nansum(mem_vals, axis=1))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6ded613",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAAHCCAYAAADy9P3IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/TGe4hAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB5H0lEQVR4nO3deVxU1fsH8M/MAMO+I4tsiiBuoOCuiFtqmqmklmW5le3aarmVpaZ9S8syf5a5fPObZVpa7kmpuYKKgisqKpugrMM+LHN/f+CMEtsMzDAz8Hm/XvMq77lz7jOXch7OOfc5IkEQBBARERFRncT6DoCIiIjIGDBpIiIiIlIDkyYiIiIiNTBpIiIiIlIDkyYiIiIiNTBpIiIiIlIDkyYiIiIiNTBpIiIiIlKDib4DaE4UCgXu3LkDGxsbiEQifYdDREREahAEAfn5+fDw8IBYXPt4klElTWfPnsXBgwcRHR2N6OhopKamAqj8sA11+/ZtLF++HAcOHFAlPP7+/oiIiMC7776rUV937tyBl5dXg2MhIiIi/UlOToanp2et7SJj2kZl7Nix+P3336sdb+hH2LdvH8aPH4/i4mKEhITA398fWVlZuHDhAqysrHDjxg2N+pPJZLC3t0dycjJsbW0bFBMRERE1rby8PHh5eSE3Nxd2dna1nmdUI019+vRBUFAQevTogR49esDX1xdyubxBfV29ehURERGwsbHBwYMH0bdvX1WbQqFATEyMxn0qp+RsbW2ZNBERERmZ+pbWGNVI07+Zm5tDLpc3aKRp5MiR2LdvH/bs2YORI0dqJZ68vDzY2dlBJpMxaSIiIjIS6n5/t8in55KTk3HgwAG0bdtWawkTERERNW9GNT2nLYcPH4ZCoUDfvn1RXl6O3377DcePH0dFRQU6d+6MJ598Eg4ODvoOk4iIiAxIi0yaLl++DACwtrZGWFgYTp06VaV9/vz52L59OwYNGlRnP3K5vMqaqry8PO0HS0RERAahRU7P5eTkAAC+//57XL16FVu2bEF2djbi4+MxefJkZGdnY9y4caqSBrVZtmwZ7OzsVC+WGyAiImq+WmTSpFAoAADl5eX49ttvMWnSJDg4OCAgIACbN29Gjx49IJPJsGbNmjr7mTt3LmQymeqVnJzcFOETERGRHrTIpMna2lr1zwkTJlRrnzZtGgDgyJEjdfYjlUpV5QVYZoCIiKh5a5FJk4+PDwDA29u7xpoMvr6+AIB79+41ZVhERERkwFpk0tStWzcAD9Y2/Vt2djaAByNSRERERC0yaerbty+cnJyQnp6O+Pj4au3KaTllckVERETUrJOm1atXIzAwEHPnzq1y3MTEBG+99RYEQcCrr75apVRAZGQkNm3aBJFIhBdffLGpQyYiIiIDZVR1mvbs2YPFixer/lxaWgoA6N27t+rYwoULMWrUKABAZmYm4uPjkZaWVq2vd999F4cOHUJkZCQCAgLQu3dvZGZm4tSpU6ioqMDSpUvRs2dPHX8iIiIiMhZGlTRlZGQgKiqq2vGHj2VkZKjVl6mpKfbu3YsvvvgCP/zwAw4cOAAzMzOEh4fjzTffxGOPPaa1uI2JQiGgTKGA1ESi71CIiIgMilFv2GtojH3DXkEQMOab48gpKsXvr/aHo5WZvkMiIiLSOW7YSxrLLSpDXIoMydnFWHmw+gJ5IiKiloxJE6mk5har/n1LVBIu3ZHpMRoiIiLDwqSJVO48lDQpBOCjPy6Ds7dERESVmDSRijJp6uplD3NTMaJvZ2N3XPUnD4mIiFoiJk2kckdWAgAI9XHAy+HtAACf7L2CotJyfYZFRERkEJg0kYpyTVNrewu8GN4Wre0tkCYrwf8dTtBzZERERPrHpIlUlNNzHvYWMDeVYOFjHQAA3/5zE8nZRfoMjYiISO+YNJHKnYdGmgBgeCc39PVzQmm5Akv2XNZnaERERHrHpIkAAKXlCtzLlwMAPOzNAQAikQgfju4EiViEA5fu4tj1TH2GSEREpFdMmggAcDevBIIASE3EVSqBt3ezwbO9fQAAH+26hLIKhb5CJCIi0ismTQSg6iJwkUhUpe3NoQFwsDTF9XsF2HwyUR/hERER6R2TJgJQdRH4v9lZmuLd4YEAgC8iryGrQN6ksTVGWYUCs346h6fXnUI5R8mIiKgRmDQRACA1R5k0mdfY/mQPL3TysEV+STk+/9N49qVbuucK/oi9gxMJWbiRUaDvcIiIyIgxaSIAwB1Z7SNNACARi7Do8U4AgJ9PJ+NiquHvS7ftTDI2nbit+nNmfqn+giEiIqPHpIkAAKm5ldXAa0uaAKCHryPGdPWAIACzfj6HEzcM92m62ORczN95EQBgKqlco5VpRNOKRERkeJg0EYDqNZpqM/fRDnCyMsPNjEI8/X0Upm6MxtX0vKYIUW0Z+XK89L+zKC1XYGgHVwzv5AaASRMRETUOkyaCIAh1LgR/mJudOQ68OQBT+vjARCzC4fgMPLrqKN7ZFqvqQ59KyxV49ccYpMlK4OdihS+eDEYrm8p1WhlMmoiIqBGYNBFkxWUoKq0AALjb1bwQ/GHO1lJ8NKYzIt8Kx6gu7hAEYPvZFAz6/DCW77sKWXGZrkOu1ZI9lxF9Oxs2UhN891x32Jibwtmmsu4U1zQREVFjMGkiVY0mZ2szmJtK1H6fr7MVvnkmBDte6YuebRwhL1dg7ZEEhH92CN8fvQl5eYWuQq7RL6eT8cPJRIhEwJdPdYWfizWAyiQP4PQcERE1DpMmwp37i8DrW89Um27eDtg6szfWT+mOAFdr5BaVYcmeKxj+xT84ci2jwXHdzizEz9FJuJgqg0Ih1HnuuaQcLLi/8PvNoQEY0sFV1ebCpImIiLTARN8BkP6pu56pLiKRCEM6uGJg+1b49WwKPv8zHrezijBlQzRGdnHDwsc6wt1Ovf7TZSVY9dd1/HImGRX3kyVnaykGtnfBwPYuCGvnAjtLU9X59/JLKhd+VygwrKMrXhvUrkp/LjZMmoiIqPGYNJFWkiYliViEiT288GgXN3wZeR2bTtzG3gvpOByfgdlD/DG9fxuYSmoe4MwuLMX/Hb6BH04mQl5eWb27c2tb3MwoRGaBHNvPpmD72RRIxCKEeNtjYPtWCA9wwaI/LuFunhz+rayx8smuEIurbgOjnJ7LKiiFQiFUayciIlIHkyZSrWnSRtKkZGNuioWPdcT4UE8s3HkRZxJzsGzfVWw/m4LFYzujd1sn1bkF8nKsP3oL647eRIG8HADQ09cR745ojx6+jpCXV+DM7RwcunoPh69l4Ma9Apy+nYPTt3Pw2YH4+9erXPhtLa3+n7STdeVC8HKFAFlxGRwe2pCYiIhIXUya6KEaTfU/OaepDu62+OXFPvg1JgXL913F9XsFeOq7UxjXrTXeHhaA/RfTseZwArILK59s6+hui3dHtMfAABfVxsFSEwn6tXNGv3bOWAAgObsIh69l4Ej8PRy/kYVyhQJfPdUNbZytaozBVCKGvaUpcovKkFEgZ9JEREQNwqSJdDLS9DCxWIQJ3b0wrKMbPvvzKn6MSsKOc6nYcS5VdU5bZyu8NSwAIzu71zt95uVoiWd7++DZ3j4oKatAcWlFvYmQs7UUuUVlyMyXI8DVRiufi4iIWhYmTS1cabkC9/IrF0jrKmlSsrM0xZKxXTAh1AsLdl7EhVQZ3O3MMXuIP8aHesKklrVOdTE3lahVJsHZ2gw37rHAJRERNZxRJU1nz57FwYMHER0djejoaKSmVo5UCELdj6Or4/r16wgKCkJJSQmGDBmCyMjIRvdpDO7mlUAQADMTMZyaaNoq2MseO1/th0t3ZAhwtdGoNlRDPajVxAKXRETUMEaVNC1evBi///67TvqeOXMm5PKWNwqR+tCec8o1RE1BIhYhyNO+ya7HApdERNRYRlXcsk+fPli4cCH++OMPpKWlQSqVaqXf9evX4/Dhw3jhhRe00p8xeVBuQPuLwA2JqlZTPpMmIiJqGKMaaXrvvfe03ufdu3fx7rvv4pFHHsGkSZPw3Xffaf0ahkyVNKlZeNJYOd8vO8CRJiIiaiijGmnShdmzZ6O4uBhr1qzRdyh6kXp/CxVdLwLXN65pIiKixmrRSdPevXuxdetWzJs3D+3atav/Dc3QnYfWNDVn3EqFiIgay6im57SpsLAQr7zyCtq3b9/gaT+5XF5l8XheXp62wmsy2txCxZA9vJWKIAhNuuidiIiahxY70rRgwQIkJiZi7dq1MDNr2KP2y5Ytg52dnerl5eWl5Sh1SxCEFrMQXLmVSmmFAnnF5XqOhoiIjFGLTJrOnDmDr776Cs899xwGDhzY4H7mzp0LmUymeiUnJ2svyCaQV1yOwtIKAM1/pElqIoGteeXAakZBiZ6jISIiY9TipufKy8vxwgsvwN7eHp9//nmj+pJKpVore6APyhpNztZmTVJgUt+cbaTIKylHRn4p2rXSdzRERGRsWlzSlJKSgvPnz8PNzQ0TJkyo0pabmwugsvK4cgTq8OHDTRtgE2op65mUnK2luJlRyMXgRETUIC0uaVJKT09Henp6jW25ubk4cuRIE0fU9FJbSI0mJRdWBSciokZocWuafH19IQhCja9Dhw4BAIYMGaI61py1vJEmFrgkIqKGa9ZJ0+rVqxEYGIi5c+fqOxSDlNpCnpxTUhW4zGeBSyIi0pxRTc/t2bMHixcvVv25tLTyy693796qYwsXLsSoUaMAAJmZmYiPj0daWlrTBmokWkphSyVnFrgkIqJGMKqkKSMjA1FRUdWOP3wsIyOjKUMyandayBYqSlzTREREjSESmvvCnSaUl5cHOzs7yGQy2Nra6jucOpVVKBCwYB8EATg9f6hqm5Hm7HxyLsZ+cxyt7S1w/P3B+g6HiIgMhLrf3816TRPVLl1WAkEAzEzEcLJqWEV0Y6NcCJ5RIG/2i/yJiEj7mDS1UKon5+zMIRa3jH3YlAvBS8sVyCvhVipERKQZJk0t1B1Zyyo3AADmphLYSCuX8XFdExERaYpJUwvV0haBK6meoMtn0kRERJph0tRCpbawwpZKDwpcslYTERFphklTC6Vc0+TZ4pImlh0gIqKGYdLUQqXmtNSRJiZNRETUMEyaWiBBEB7ad65lbKGixKSJiIgaiklTC5RXXI7C0goALW+kSVnEM4P7zxERkYaYNLVAykXgTlZmMDeV6DmapvVgIThHmoiISDNMmlqgOy30yTmAm/YSEVHDMWlqgR4UtmxZ65mAB5v2ZuRzKxUiItIMk6YWqKXWaAIeLASXlytQIOdWKkREpD4mTS2Qshp46xaYNFmYSWBlVrmOiwUuiYhIE0yaWqCWvKYJ4LomIiJqGCZNLVCLT5qsuf8cERFpjklTC1NWocDdPOVmvS1vITjAsgNERNQwTJpamLt5JVAIgJlEDGcrqb7D0QvlSFMG1zQREZEGmDS1MMo959ztzSEWi/QcjX64cE0TERE1AJOmZmLvhTSM/78TuJKWV+d5yhpNLfHJOSWuaSIiooZg0tRMfPXXdZxJzMErP8bUWX9IWW6gpS4CB7hpLxERNQyTpmYgNbcYV9PzAQC3Mgvxwc6LdZ4LtOykycWmciF4BpMmIiLSAJOmZuDQ1XsAKqfcxCLgt3Op2H42pcZzleUGWrfQJ+eAh6fnuBCciIjUx6SpGVAmTU/38sabQwMAAAt3XkRCRkG1c1t6jSbgQdJUXFaBQm6lQkREamLSZORKyipwPCETADA4sBVeGdQOff2cUFxWgde2nENJWYXqXEEQVE/PteSkyUpqAgtT5VYqnKIjIiL1MGkycidvZqGkTAF3O3MEutlAIhbhiye7wsnKDFfS8vDJ3iuqc/NKylFYWplEedi13KQJAJxtWOCSiIg0Y1RJ09mzZ7F8+XJERETA09MTIpEIIpHmtYZyc3OxZcsWTJo0CW3atIGZmRlsbGzQq1cvrFq1CmVlZTqIXjeUU3ODAlup7oWrrTlWTAwGAPxwMhH7L6YDeDA152hlBov7m9a2VKoCl1zXREREajLRdwCaWLx4MX7//fdG9/P5559j6dKlEIlE6Nq1K3r16oWMjAwcP34c0dHR2L59Ow4cOABLS0stRK07giDg7/tJ0+D2raq0DWzfCi8OaItv/7mJOdtj0bm17UPrmVruInAllh0gIiJNGdVIU58+fbBw4UL88ccfSEtLg1TasG1ArKysMGfOHNy+fRsxMTH4+eef8ddff+HChQvw9vbGsWPHsGTJEi1Hr3037hUgJacYZiZi9G3nVK397WHtEexlj7yScsz66RwSs4oAcGoOYFVwIiLSnFGNNL333nta6Wfu3Lk1Hvf398fy5cvx9NNP46effsInn3yilevpyqH4ylGm3m2dYGlW/UdpZiLG6kndMHLVUcQk5eK2MmlqwYvAlTjSREREmjKqkaamEBxcuRbozp07eo6kfg+m5lxqPcfL0RLLnwgCAGQXVq7faclbqCi5WN9fCM41TUREpCYmTf9y8+ZNAICbm5ueI6lbXkkZztzOAQAMDnSt89xRQe54upe36s8caXpoIThHmoiISE1Mmv5l1apVAIAxY8boOZK6Hb2WiXKFAD8XK3g71b9g/YPHOqKThy1MxCIEedo1QYSGzZlrmoiISENGtaZJ19auXYvIyEjY29vj/fffr/d8uVwOufzBl25eXp4uw6tCNTUX2KqeMyuZm0rw2yt9ISsqQytbPj33YCsVJk1ERKQejjTdd/ToUcyePRsikQgbNmyAh4dHve9ZtmwZ7OzsVC8vL68miBRQKAQcufagPpO6pCYSJkz3Od9f01RYWoHi0op6ziYiImLSBAC4ePEixowZg9LSUqxatQrjxo1T631z586FTCZTvZKTk3UcaaW4VBkyC0phIzVBD1/HJrlmc2MtNYHUpPI/f07RERGROlr89NytW7cwbNgw5OTkYNGiRXj99dfVfq9UKm1wrajGUE7NhQU4w1TCvLchRCIRnK2lSM0tRkaBHF6Ohl3IlIiI9K9Ff+OmpaXhkUceQVpaGmbPno0PP/xQ3yGp5fD9+kyD2qs/NUfVqRaDc10TERGpocUmTTk5ORg+fDgSEhIwbdo0fPHFF/oOSS338ksQlyIDAITXUZ+J6ueiKnDJWk1ERFS/Zp00rV69GoGBgdUqgBcVFWHUqFG4cOECJk6ciHXr1jVo4199OByfAQAI8rRDKxsu6m4MF5v7BS65pomIiNRgVGua9uzZg8WLF6v+XFpaOULQu3dv1bGFCxdi1KhRAIDMzEzEx8cjLS2tSj/z58/HyZMnIZFIYGJighkzZtR4vU2bNmn5EzTeoaucmtMWbqVCRESaMKqkKSMjA1FRUdWOP3wsIyOj3n5ycioraVdUVGDLli21nmdoSVNpuQJHr2cCUL8+E9VOVRWca5qIiEgNIkEQBH0H0Vzk5eXBzs4OMpkMtra2Wu//xI1MPP19FJytpYieNwRisXFMKRqqPXFpeHVLDHr4OmDbS331HQ4REemJut/fzXpNU3OjLDUwsL0LEyYtUBa45EJwIiJSB5MmI/J3vGZbp1DdWHKAiIg0waTJSCRmFeJmRiFMxCL093fWdzjNgnJNU768HCVl3EqFiIjqxqTJSCifmuvh6whbc1M9R9M82JqbwEzCrVSIiEg9TJqMxN/36zMNCmRBS22p3Eql+a5r+uvKXRy8fFffYRARNRtGVXKgpSoqLcepm1kAuJ5J25xtpLgjK2l265qyC0sxc/NZVCgETOzuiY/HdIa5qUTfYRERGTWONBmB4zeyUFqugJejBfxcrPUdTrPi0kwLXMYm56JCUVlN5JczKYhYcwJJWUV6joqIyLgxaTICylIDg9u3MprtXoxFc60KHpuSC6Byux0nKzNcTsvDY18fRSSn64iIGoxJkxGY3s8X7w5vjzHdWus7lGbH+f7+c82tKrhyU+dx3Vpj96z+CPG2R15JOZ7/4Qw+O3BVNQpFRETqY9JkBPxdbfDqoHYI8XbQdyjNzoORpuazEFwQBMSpRprs4W5ngZ9n9sHUvr4AgG8OJeC5DVHNbnSNiEjXmDRRi6baf64ZJRB3ZCXILCiFiViETh6V2wGYmYix6PFO+GpSN1iaSXD8RhYe++oYzibm6DlaIiLjwafnqEVrjmuaLtwfZQpwtan2xNzjwR7o4GaDl/53FgkZhXjy25N4tIs7HCxNYWtuClsLE9iYV/67jbkJbC1MYW9hCh8nS66nI6IWj0kTtWgu99c0NaeSA7H31zMFe9nV2O7vaoPfX+uP936Nw564NOyKvVNvn9P6+eLD0Z20GicRkbFh0kQtmnKkKa+kHPLyCkhNjL+W0cPrmWpjLTXB6knd8ERIayTcK0ReSRnyisuQX1J+/98r/ykrLkOarATbzqRgzvBAWJgZ//0hImooJk3UotlZmMJUIkJZhYCsglJ42FvoO6RGUSgE1ZNzQZ41jzQpiUQiDA50xeDA2s8RBAFh/zmElJxiRF65i9HBHtoMl4jIqHAhOLVoIpEITlbNZ13T7axC5JeUQ2oiRoCrTaP7E4lEGHe/1MXOc6mN7o+IyJgxaaIWz8Wm+SRNylGmTh62MJVo53/vMV0rk6Yj1zKQ1QzuERFRQzFpohZPtWlvvu5rNRWXViApqwiCoJvikrFqrGfSVLtW1gjytEO5QsCeC2la65eIyNhwTRO1eLqs1XQntxhnE3NwNjEHMUk5uHwnD+UKAf95IggTe3hp/Xpx9Tw511BjurZGXIoMO86l4rk+vlrtm4jIWDBpohbP+f70nDa2UrmYKkPUrWzE3E+U0vNKajxvS3SS1pOm8goFLt1RLgK312rfo4PdsXTPZZxLykViViF8nKy02j8RkTFg0kQtnjYKXAqCgKV7ruD7Y7eqHJeIRejobotQHweE+DjA18kSY785jvPJuUjOLoKXo2WjYn/YtbsFKClTwEZqgjZaTmpa2Zijv78L/rmWgZ3n7mD2UH+t9k9EZAyYNFGLp1rT1MCkqUIhYO5vcfjlTAoAYFB7F3T3dUSojwOCPO1gaVb1f7NebZxw8mYW9l1Mw8wBfo0L/iHK+kxdPO0gFmu/eve4bh6VSdP5VMwa0o4VwomoxWHSRC2eSyM27ZWXV+DNreex90I6xCLgP+ODMT7Us873jApyx8mbWdgTp92kKTZFN1NzSsM6usHC9CJuZRYiNkWGrl66uQ4RkaHi03PU4inXNKXmFGu0gW1RaTle+OEs9l5Ih5lEjDXPhNSbMAHAiM5uEIsqk5ykrKIGx/1vypGm4HqKWjaUldQEwzu5AmDNJiJqmZg0UYvn5WAJJyszFJdV4In/O4HXfzqHlJy6kxlZcRmeWx+Nf65lwMJUgg1Te2BEZ3e1rudsLUUfPycA0Noj/CVlFYhPzwcABOlwBGjs/UKXu2LvoKxCobPrEBEZIiZN1OJZmEmw740wPNndCyJRZUIwZMURfH4gHoXy8mrnZxbIMem7UziTmANbcxP87/le6O/vrNE1Hwuq3I5kz4X6N8tVx+W0ylIGztZm8LAz10qfNenfzhnO1mbIKizFseuZOrsOEZEhYtJEhMqnwz4dH4Tdr/dH77aOkJcrsPrQDQz8/DB+OZMMhaKyGGVqbjEmrj2Jy2l5cLY2w88z+yDUx0Hj6w3v5AaJWISLqXm4nVnY6PjjknMBVK5n0uUCbROJWJXw7eAUHRG1MEaVNJ09exbLly9HREQEPD09IRKJGvUFkZOTg9mzZ8PHxwdSqRQ+Pj544403kJubq72gyah08rDDTy/0xrfPhsLHyRIZ+XLM2R6H0auP4ffzqZjwfydwM7MQre0tsO2lvujoYdug6zhamaGvFqfo1N2kVxuUe9H9eTkdBTWMxNVk0/FbGLP6GE4mZOkyNCIinRIJGuzn8MMPP2jtws8995zG7xk7dix+//33ascbsiVFZmYm+vTpgxs3bqBt27bo3r07Ll26hEuXLiEgIAAnT56Eo6OjRn3m5eXBzs4OMpkMtrYN+zIlwyEvr8APJxLx1V/Xkf9QctDWxQr/m9ELHvYWjep/6+kkvPfrBXRwt8W+2WGN6mvIisNIyCjExqk9MCiwVaP6qo8gCBiy4ghuZhZixYRgPFHP4vf/nriND/+4BAAwlYjw+YRg1X52RESGQN3vb42SJrFYrLWh/4qKCo3f8+mnn6KwsBA9evRAjx494OvrC7lc3qCkafLkyfjxxx8RERGBrVu3wsSksvrCrFmz8PXXX2PKlCnYtGmTRn0yaWqesgrk+CLyGrZEJaGThx02TuuhKojZGLlFpei+JBLlCgF/vR0OPxfrBvWTX1KGoI/+hCAAZxcMhZMWYqvPV39dx8qD1xDm74zNM3rVet6vZ1Pw9rZYAIB/K2tcv1cAAHh3eHu8MtCPtZ6IyCDoLGnq2rUrxowZ0+DAdu7cibi4uAYlTf9mbm7eoKQpLS0Nnp6eMDExQVJSElxdXVVtcrkcXl5eyM7Oxp07d9Cqlfq/tTNpat5kRWWwNjeBRIuFI6dujMbh+Ay8/UgAXh/SsCrbJxIy8fS6KLS2t8Dx9wdrLba6JGUVYcBnhyAWAafmDkEr2+qLz/dfTMcrP56FQgCm9vXFB491xPL9V/HdPzcBAE/38sbHj3eCicSoVgkQUTOk7ve3xsUtu3btig8//LDBgd2+fRtxcXENfr827N+/HwqFAmFhYVUSJgCQSqUYPXo0NmzYgL1792Lq1Kn6CZIMjp2lqdb7HNXFHYfjM7A7Lq3BSZOuNumti7eTJUJ9HHA2MQd/xN7B82Ftq7QfvZ6BWT+dg0IAxod64oPHOkIsFmHeyA5obW+BRbsuYUtUEtJlJfh6UjdYSVlnl4gMn0a/4tna2sLSsnF7ZVlYWOh9FCY2tnK6ICQkpMZ25XF9J3fU/A3r6AZTiQjxd/Nx/W5+g/pQFrXUVSXw2ihrNu08X/UpurOJ2Zj5w1mUVijwaGc3LI/oUmVblyl9fbF2ciikJmL8ffUenvruFO7l17yxMRGRIdEoacrNzcXq1asbdcE1a9YgJ0f9qsu6kJSUBADw9Kx5AavyeGJiYp39yOVy5OXlVXkRacLO0hQD/F0ANPwputjkpnty7mGjurjD5H7ZBGXCd+mODFM3nkZxWQUGBLjgy6e61jj9NryTG36a2RuOVma4kCpDxJoTuHF/vRMRkaFqkYsJCgoq/3KubdTMyqpyh/j8/Lp/81+2bBns7OxULy8vL+0GSi3CqKDKSuK749I0Xp+XVSBHam4xRCKgS+umTZocrcwwsH1lwrfzfCoSMgrw3Ppo5JeUo7uPA9ZODoHURFLr+0O8HfDby33h62SJlJxiPPF/J3D6dnZThU9EpLEWmTRpy9y5cyGTyVSv5ORkfYdERmhoR1eYScS4ca8A1+5qNtqiXM/U1tkKNubaX3NVH+UU3a9nU/Hs91HIKixFJw9bbJjWA5Zm9a9T8nW2wq8v90U3b3vIisvwzPdRuHD/MxERGRqtJE2lpaWIjY1FZGQktm/fjsjISMTGxqK0VPNd45uCtXXlo91FRTXvL1ZYWFmh2cbGps5+pFIpbG1tq7yINGVrbooBAfen6OI021YlVrVJr72Wo1LP0A6usJaaID2vBHdkJfBzscIP03vCVoMEzslaip9e6I3wABeUliswe+s5FJc2/ulaIiJta3DSlJGRgc8++wwDBw6Evb09QkJCMHz4cDz55JMYPnw4QkJCYG9vj4EDB+Lzzz9HRkaGNuNuFG9vbwBASkpKje3K4z4+Pk0WE7Vso4PvT9Fd0GyKrikrgdfE3FSCRzu7AQBa21vgf8/3alCdKHNTCb58sitcbaW4mVGIJXsuaztUIqJG0/g53xs3bmDhwoXYsWOHaiTJ2dkZoaGhcHR0hK2tLWQyGXJycnD16lX8888/+Oeff7BgwQJERETg448/Rrt27bT+QTQRHBwMAIiJiamxXXk8KCioyWKilm1IB1eYmYhxM6MQV9Ly1dqeRRCEB0/OednrNsA6vDuiPdztLTCxuyfc7RpeJd3BygwrJnTF5PVR+DEqCYPat8LQjq71v5GIqIloVNzytddew7p161BRUYFBgwbh6aefxsCBA9GmTZta33Pz5k0cOnQIW7ZswZEjRyCRSDBz5kx8/fXXjQ5eG8Utk5OTqxSwZHFL0pcXN5/BgUt38eogP7w7PLDe81Nzi9Fv+d+VT7B9NBzmprUvujYmS3ZfxvfHbsHJygz73xgAFxvdVzgnopZN3e9vjabnNmzYgJdffhlJSUk4ePAgpk2bVmfCBABt27bFjBkz8NdffyExMREvvfQSNmzYoMllG2z16tUIDAzE3Llzqxx3d3fHpEmTUFpaildeeQXl5Q/2FZszZw4yMjIwefJkjRImosYaFeQBANij5lN0ccm5AID2bjbNJmECKkeuAt1skFVYijnbYxu0TVJd0mUlmLHpNHaeS63/ZCKih2iUNN28eRNffvklPDw8GnSx1q1bY9WqVUhISGjQ+/fs2YPevXurXsrpwYeP7dmzR3V+ZmYm4uPjkZZWvf7Nl19+CT8/P/z6668IDAzEU089hS5duuCrr76Cv78/Vq5c2aAYiRpqSGArmJuKcTurCJfu1F/zK1a1nslex5E1LamJBKue6gYzEzEOxWfgf6fqrpemCUEQMG/HBfx19R4W776MsgqF1vomouZPo6TJzc1NKxdtaD8ZGRmIiopSvZS/gT58TN0F587OzoiOjsbrr7+O0tJS7NixAzKZDLNmzUJ0dDQcHR0bFCNRQ1lJTTA4sHJ0c3dc/YUu41RPzulnEbgutXezwfsjKqcol+y5ghv3GlYt/d/2XEjD31fvAQCyCktxJN5wHlAhIsOn0ZomqhvXNFFj7YlLw6tbYuDlaIF/3h0EkajmzYEVCgHBH/2JfHk59s4KU2vhuLFRKARM2RiNo9cz0cnDFjte6Qczk4ZXSZEVlWHIyiPILJCjlY0U9/LleLSzG/5vcqgWoyYiY6STNU31USgUOHnyJH7++Wf8+uuviIqKUtU8IqL6DQp0gYWpBMnZxXhnWxxOJmRBoaj+e82trELky8thbipGgKu1HiLVPbFYhBUTguFgaYpLd/Kw4mB8o/pbtu8KMgvk8HOxwrrnugMA/rpyD7lFhllPjogMj9aSpujoaLRt2xb9+vXD008/jQkTJqBv375wcHBA3759sWbNGpSUcFNOorpYmplgcu/KOmK/xqRg0rpT6P/p31i+7yqupj9Y56ScmuvkYVfj3m7NRStbcyx/orL0x3f/3MTJhKwG9XPqZhZ+Pl1ZsX/5E0EI9rJHB3dblFYosCtWs4KiRNRyae1v25kzZ6Jdu3a4evUqCgoKkJqaCkEQ8Prrr8PDwwNz5sxBQEAAjhw5oq1LEjVLcx/tgJ9e6I0nu3vBxtwEd2QlWHskASO+PIoRX/6DtUcSVGtx9FXUsikN7+SGp3p4QRCAt345D1lRmUbvLymrwLzfLgAAnu7ljR6+lesVnwip3AJmewyfoiMi9Wgtabp27RoWLFiAgIAAWFpaqh7Xf+aZZ7B9+3bcuXMHzzzzDEaOHIkTJ05o67JEzY5YLEIfPyd8Oj4Ip+cPxf89E4LhnSr3p7uano/l+65i5/nK0RF9bZ/S1BY+1hG+TpZIk5Vg7o44lGvw1NuaQzdwM7MQrWykeG/Eg/pXY7q2hkQsQmxyrtYWmhNR86a1pKlz5861VtgGAFtbWyxbtgxvvvkm3n33XW1dlqhZMzeV4NEu7vj22e44PX8olkV0Qc82jvfbxOjVtmU85WklNcEXT3aFRCzC3gvpePr7KNzNq3+6/9rdfPzfkcoSJx893gl2Fg/2xHOxkWLg/T3/tp/laBMR1U9rSdOSJUvw4YcfYv369XUWoxs6dCjOnTunrcsStRh2lqaY1NMbv7zYByfnDsZfbw9s1LYlxqabtwO+ebobrKUmiL6VjVFfHcXxG5m1nq9QCHj/1ziUVQgY2sEVIzpXL3UyPtQTALDjXAoqalhwT0T0MK0lTcOGDcOPP/6I999/H/7+/li4cCFEIhEKCgpU59y6dQvLli1D27ZttXVZohbJ3c4Cre1bTsKkNKKzO/54rR8C3WyQWVCKyeuj8NVf12t8wvDHqETEJOXCykyCxWM71Vi+YXCHVrCzMMXdPHmdCVhLIQgCCuTl9Z9I1EJplDRJJHVv1fD444/jxo0bmDFjBvbs2QNBEDBo0CCYmZlBKpWiXbt2iI2Nxdq1axsVNBG1XG1drLHz1X54snvl4vCVB69h6qbTyCqQq85Jl5Xg0/2VJQrmjAisdUROaiLB48GVOxz8GpOi++AN3Ppjt9Bl0QFsPZ2k71CIDJJGxS3FYjEUCvUXYGZnZyM2NhbJyckoLy+Hl5cXwsLCYG5u3qBgDR2LWxI1re1nU7Bg5wWUlCngZmuOb57phlAfR8z84Qz+vHwX3bztsf2lvpCIay4SCgDnk3Mx9pvjMDcV4/T8obAxN6313IcVyMtx7HomJGIRLEwlMDcVw9xUAnNTCSzMJDA3Ed//pwTiOq5vKBQKAQM+O4SUnGJYmUlw4M0B8HSw1HdYRE1C3e9vE006ra06cUlJCW7cuIF79+7ByckJ7du3h7m5ORwdHTFo0CDNIiciUtP4UE90bm2LV36Mwc2MQjz57Sk8HuyBPy/fhYlYhGURXepMmIDKbWj8XKyQkFGIvRfS8GQP73qvW1ahwPRNpxF9K7vec52szPDRmE54LKhhe3Y2lbNJOUjJKQYAFJZWYP6Oi9g0rUetf+8TtUSNWtMkCAI+/vhjuLq6Ijg4GI888ghCQkLg5OSEsWPHsrQAEelcoJst/nitP0YHe6BcIeC3c5VPwr0U7odAt/pHfEUiEZ64vyD8VzWfovt031VE38qGpZkEwV72CHSzgY+TJVxtpbA1N6my3UtWYSle23IOc3+LQ3FpRQM+YdPYcf++9W7rCDMTMY5cy8DO83yqkOhhGk3PSSQSVFQ8+J9+0aJF+PjjjwEAQUFBaNeuHYqKinD27FlkZGRAJBJh+vTp+Oabb2BmZqb96A0Mp+eI9EcQBPwvKgmLd12GXytr7HilL8xN616HqZQmK0bf5X9DEIB/3h0Eb6fap6V2x93Ba1sqnwBeOzm0xqfyAKBCIaC4rAJrDyfgm8M3IAhAgKs1Vj8dggBXG80/oA7JyyvQc+lfkBWX4cfne+F8ci4+OxAPe0tTRL4VDmdrqb5DJNKpJtl7buPGjRCLxdi+fTvOnz+P7du3Y+/evbh79y4OHDiAnj17Yv369Rg9erRGa6GIiDQlEonwbG8fnF04VKOECah8GrF/O2cAdS8Iv3EvH3O2xwGoHMmqLWECAIlYBGupCd4Z3h6bp/eCs7UU1+4W4PHVx/BzdFKdpVma2uH4DMiKy+BqK0Xvtk6YOaAtOrjbIreoDB/tuqzv8IgMRqOSpvT0dAwYMAARERHV2h555BGcOHECU6ZMQWRkJJ+YI6ImYWNuqlHCpPRESOUU3W/nUmosYZBfUoaZm8+iqLQCff2c8M6wALX77u/vjH2zwxDm74ySMgXe/+0CXv/pHPJKNNsSRld+vz8Np6ySbioR4z9PBEEsAnbF3kHk5bt6jpDIMDQqaWrVqhWcnZ1rbReJRFi7di1cXFzw/fffN+ZSREQ6NbyTG6ylJkjOLsbp21UXeAuCgDnb43AzoxButub4alI3jTdKdrGR4r/TeuL9RwNhIhZhd1waHvvqGGKTc7X4KTQnKy5D5JV7AIAxXR8sVu/iaYcXwipr6i3YedFgEjwifWpU0hQeHo4jR46gpKT27QykUinCwsJw9erVxlyKiEinLMwkGNmlcrrt31N0647exL6L6TCViLBmckiD1/iIxSK8FO6HX17qA08HCyRlF2H82hP474nbjQ2/wfZfTENpuQIBrtbo6F51LccbQwPg62SJ9LwSfLqPf4cTNSppWrBgAYqKivDyyy/XeZ5MJoOdXfPfjZ2IjJtyim7vhXTVk24nE7Kw/H7C8MHoTgjxdmj0dUK8HbBnVhge7eyGsgoBH/5xCVfS8hrdb0Mon5ob2611tfICFmYSLIsIAgD8GJWEUzezmjw+IkPSqKRpypQpCAgIwA8//IBBgwbh1KlT1c45cuQIDh8+XOO6JyIiQ9LD1xFejhYokJfjwKV0pMtK8PpPMVAIQERIa0zuVX8NJ3XZWZhizTMheKSjKwBg6+lkrfWtrju5xYi6X2tqTNfWNZ7Tx88Jk3pWfu65v11ASZnhlk0g0rVGJU2nT5/GuXPnIAgCjhw5gn79+sHHxwcRERGYOnUqBg4ciCFDhmD06NH4/PPPtRUzEZFOiMUiRHSrHG36+XQSXvnxLDILStHB3RZLx3bReqFHkUiEZ+4nYjvOpTZ5QvJH7B0IAtCrjWOdexnOHRkIV1spbmUW4svI600YIZFh0agi+L/du3cP586dU71iYmJw48YNJCdX/Y3p/PnzePbZZxESEoJu3bohJCQErq6ujQqciEgXngjxxKq/ruPUzcoRGBtzE6ydHAILM82fyFNHmL8LPOzMcUdWgj8v31XthdcUdj40NVcXW3NTLB7TGTM3n8W6ozfxWJA7OrfmkgtqeRpV3LImBQUFiI2NVSVR586dw+XLl1FWVvnkhfI3NTc3N6SmNq9qsyxuSdQ8TFx7EtH3n6BbP6U7hnTQ7S95Kw9ew1d/XUf/ds743/O9GtRHbHIufJwsYW+pXiHhK2l5eHTVUZhJKvfcs7Osf8+9V7fEYE9cGjq62+I3DWthERkynew9p05+ZW1tjX79+qFfv36qY2VlZbh48aIqiYqJicGFCxc0uTQRUZOZOaAtYpJy8OYjATpPmABgQqgnvv77Oo7dyERydhG8HDXbKPdQ/D1M23gaPk6W+P3VfmolTspRpsGBrdRKmABg0ehOOHY9E5fT8jBy1VEsi+iCXm2dNIqVyJhpNNKkTYIgNLuNIDnSRNR8VCiEejf71aZn10fh6PVMzBrcDm8Na6/2+wRBwOOrj+NCqgwA0L+dMzZN61FnHSmFQkDf5X8jPa+kzq1ganLiRiZmbz2PjHw5AOCZXt5479FA2Jqrl3gRGSKdbKNy6dKlRgem7Ke5JUxE1Lw0ZcIEABO7ewEAtp1NQUUNFclr89eVe7iQKoOFqQSWZhIcu5GJT/bWXVPp1K0spOeVwNbcBIMCXTSKs287Z0S+GY5JPSvj/TEqCY+sPII/L6Vr1A+RMdIoaQoKCsKkSZMQFxfXoIudO3cOEydORHBwcIPeT0TUXA3r5Ap7S1OkyUrwz/UMtd4jCAK+/OsaAGBKX1+smFD5d+uG47ew7UztJQyUU3OjgjwgNdF8XZKdpSmWRQRhywu94Otkibt5cszcfBav/HgW9/JrL3ZMZOw0Spo+/PBD7NmzB926dUPXrl3x6aef4tSpU5DL5TWeX1JSgpMnT2LZsmXo0qULunfvjv379+PDDz/USvBERM2F1ESCsfdrJf2iZs2myCv3cDE1D5ZmEswc0BaPdnHHrCH+AID5Oy4iJimn2ntKyiqw70LlqNDYro17Uq+vnzP2vzEALw/0g0Qswt4L6Ri64gh+OZ1sUBsSE2mLxmua7t27h6VLl+KHH36ATCaDSCSCiYkJvLy84ODgABsbG+Tn5yM7OxvJycmoqKiAIAiws7PDtGnTMHfuXLi4aDYc/G/FxcVYtmwZfv75ZyQlJcHR0REjRozA4sWL0bp13Y/O/tvBgwfx5ZdfIjo6Grm5ubC1tUVoaChefvlljBs3TqO+uKaJiBpD+USbqUSEk3OH1LldiyAIeOzrY7h0Jw8vD/TDeyMCAVSuV3r5x7M4cOkuXGyk2PVaf7jZmavetycuDa9uiUFrewscnTMIYi1NQ166I8P7v15Qra0KdLOBt6MlHK3M4GBlBkfL+/+0MoWDpRkcLM3Q2sECphru4UekC+p+fzd4IXhxcTF++eUX7N69G8eOHcPdu9V3wXZzc0NYWBhGjRqFiRMnwtzcvIaeNFNSUqKqPu7u7o6wsDDcvn0b0dHRcHFxwalTp9C2bVu1+vryyy/x5ptvQiQSoU+fPvDy8kJycjJOnjwJQRAwb948LF26VO3YmDQRUWONWX0MsSkyzB/ZAS8MqP3vsj8vpWPm5rOwMpPg6HuD4Wj14Im5Qnk5ItacQPzdfAR72mHri31U5QFe+OEMDl6+i1cG+mHO/URLW8orFNh4/DZWHIxHSZmi3vNb2Ugxb2QHjOnqwXWupFc6T5r+LSMjA/fu3VPtM9eqVatGjyjVZMGCBVi6dCn69OmDP//8E9bW1gCAlStX4u2330Z4eDgOHz6sVrxeXl5QKBQ4ePAgwsPDVW3//PMPhg0bhtLSUty4cUPtJIxJExE11o9RiZi/4yLatbLGwTcH1JhMCIKAUV8dw+W0vFqTn6SsIjz+zTHkFpVhXLfWWDkxGLlFZej5SSTKKgQcfHMA/F1tdPIZ0mUliEnKQXZhKXIKS5FdpPxnWeU/C0uRVShXJVY92zhi8ZjOaO+mm3iI6tPkSVNTKC0tRatWrSCTyRATE4Nu3bpVaQ8ODkZcXBzOnDmD0NDQOvvavXs3Ro8ejeHDh2P//v3V2seMGYM//vgDW7duxcSJE9WKj0kTETVWfkkZei79C8VlFfj15b4I9am+QfCBS+l48f4o07H3BsPBqua6TCduZOLZDdGoUAiYP7IDLMwkWLDzIjp52GLPrDBdf5Q6ycsr8P3RW/j67+soKVNAIhZhSh9fvPGIP8sXUJPTSckBfTt+/DhkMhn8/PyqJUwAMH78eADArl276u1LKq19rcDDnJxYuI2Imo6NuSlGdnEHAGw9nVStXaEQVPu/Te3nW2vCBFSWB1g4qgMAYNm+K1hz6AYAqBac65PURIJXB7VD5FvhGNHJDRUKARuO38KQFUew41wKF5KTQWqSpEmhUFR7NURsbCwAICQkpMZ25XF1SiL07NkT9vb2+Pvvv3HkyJEqbf/88w8OHDgAf39/hIXp97cxImp5nuxRWQNpd1waCuTlVdr+vHwXV9LyYC01wfP96186MKWvL57s7gWFANyRlUAkAh5v5FNz2uTpYIm1z4biv9N7oo2zFTLy5Xhzayye/PYUrqbn6Ts8oip0kjTl5eXhjTfegK+vL6RSKUxNTau9GiIpqfK3Lk9PzxrblccTExPr7cvOzg7r16+HWCzGoEGD0L9/fzz11FPo378/Bg4ciB49euDAgQMwM6v9tzi5XI68vLwqLyKixurh64C2zlYoKq3Anrg7quOVo0yVdZmm9q17lElJJBLh47GdVNN8/fyc4Wrb+IdytC08wAX73wjDu8Pbw8JUgujb2Rj11TF8/dd1fYdGpKLR3nPqmjJlCo4ePYrnn38efn5+dSYemigoKAAAWFrWvC+TlZUVACA/P1+t/iIiIrBv3z5MnDgRx48fVx23tbXFsGHD6i1fsGzZMnz00UdqXYuISF0ikQgTe3hh+b6r+Pl0Mp7s4Q0A+PNyOq6m51eOMoW1Ubs/qYkE657rjo3Hb2FsN/1PzdVGOWU3tltrLNl9GfsupmPFwWtwspbi6V7e+g6PSDdJ019//YXvv/9e7QXU+rJixQrMmTMHY8eOxaJFi9C2bVvcvHkTH3zwAT744ANERUVh9+7dtb5/7ty5eOutt1R/zsvLg5eXV1OETkTNXERIa3x2IB7nknJx/W4+/FysVWuZpvXzVWtT3oc5WpnhbQ32tNOn1vYW+L/Jofjqr+tYefAaFv5+Ed6Olujv76zv0KiF08n0nJ+fHyQSzUvz10dZXqCoqKjG9sLCQgCAjU39j60ePnwY77zzDrp27Ypt27ahS5cusLKyQpcuXbB9+3Z07doVe/bswb59+2rtQyqVwtbWtsqLiEgbWtmYY3BgKwDA1tPJOHCpcpTJRmqCGf3VH2UyZq8Pbodx3Vqj4n7Bzhv31JtFINIVnSRNX375JT755BOcP39eq/16e1cOz6akpNTYrjzu4+NTb1+bN28GAIwbNw5icdXbIJFIEBERAaByUTgRkT48dX9B+G/nUhs1ymSsRCIRlj/RBd19HJBfUo5pm04jq6DmbbuImoJOkqa+ffuiZ8+eCA0NhZ2dHby9vau9GkK50W9MTEyN7crjQUFB9falTLDs7OxqbFcez8mpvncTEVFTCA9wQSsbKbILSxF/VznKpF6x3eZCaiLBt8+GwtvREsnZxXhx81nIyyv0HZZe3cosxPJ9V5GcXfOsC+mOTtY0Pf/889i2bRsmTJig1YXg/fr1g52dHRISEnD+/Hl07dq1Svv27dsBAKNHj663Lzc3NwDAmTNnamw/ffo0AMDX17fhARMRNYKJRIzxoZ5YczgBADCtfxvYWba8wo9O1lJsmNod49acwJnEHLz/6wWsnBjcIrde2X8xHe9ui0W+vBzJOUX45umaS/CQjgg6YG1tLaxbt04XXQvz588XAAh9+/YVCgoKVMdXrFghABDCw8OrnP/1118L7du3F95///0qx3/77TcBgCCRSIRdu3ZVadu5c6cgFosFsVgsXL16Ve3YZDKZAECQyWSafzAiohrcziwQ2s3bIwR/dEDILSrVdzh6dfRahtB27h7B573dwqrIa/oOR2OF8jJh1k8xwoIdF4S7ecUavbesvEL4ZO9lwee93apXh4X7hCJ5uY6ibVnU/f7WyUiTm5ubaiRH2xYsWIDIyEicOHFCVXwyMTERUVFRcHFxwYYNG6qcn5mZifj4eKSlpVU5PnbsWEyYMAHbtm3D6NGj0b17d7Rp0wa3bt1SjT4tXboU7dsbx9MmRNQ8+ThZYeer/WBlZgI7i5Y3yvSw/v7OWDymM+btuICVB6/B19kKjwcbTqHO+qyKvI7fz1fW3dpxLhWvDmqH6f19ITWp+8GpjHw5Zv10DidvZgEAnu/fBvsupiM1txhHrt3DiM7uOo+dKulkTdPy5cvxySefID09Xet9m5ub49ChQ1i4cCEsLS2xc+dOJCYmYurUqYiJiVF7c12RSIStW7di/fr1GDBgAG7cuIEdO3bg9u3bGDlyJPbt24d58+ZpPX4iIk118rCDr7OVvsMwCE/38sbz958efGdbLM4mGse606vpeVh/7BYAwM/FCgXycny6/yoeWfkP9l9Mq3XbmLOJ2Xjs66M4eTMLVmYSfPN0CBY81hEju1QOTOy90PDvWYVCQIWiabarqe3zGRudbNgbFhaG69evIzc3F+3bt69xsXVzfCqNG/YSEelehULAi5vPIvLKXThZmeGd4e0hEYsgFokgFgEiESC+v95JLBLB3FSC8AAXmJk0fJwgI1+OO7nFCPay1/i9CoWAid+exJnEHIzo5IY1z4Rgx7lUfLr/Ku7lVz4N2LutIz54rBM6elR+dwiCgE0nbmPpnisoVwho18oaayeHoF2rypI6MUk5iFhzAlZmEpxd+AjMTTUv8/Putljsu5iO+aM64KkeXjpZI5aYVYjvj97CrzEpeKaXN+aP6qj1a2iDut/fOkmapk2bVu85Gzdu1PZl9Y5JExFR0yiUl2PC2pO4nKbe9lXjQz3x+YTgBl0rs0COEV8eRWaBHJ8+0UVVoV1dv5xOxpxf42BpJkHkW+HwsLcAUPkZ1h5JwHf/3IS8XAGRqLLMxMvh7fDZn/HYFVs5lfdYkDs+fSIIVtIHK2oUCgH9Pv0babISrHuuOx7p6KpRTElZRRjw2SHVn58I8cSSsZ1hYaadGovnknKw7uhN7L+YDuVgVoCrNf58M1wr/WubXpOmlopJExFR07mbV4IvDl7DvXw5FIIAQQAU97/SlH+uUAg4fTsbCgFY/XQ3PBak2RooQRDw/H/P4K+r9wAAZhIxfprZW7WXX32yC0sxeMVh5BaVYcGoDng+rPoSkpScIizfdxW746quvTURizBvZAdM6+db4yjQR7suYePx24jo1horn+yq0ef67MBVfHMoAR525kjPK4FCAALdbPB/k0PRpoFTwQqFgL+v3sN3/9xE9O1s1fFgTzvEpsjgYiPF6flDG9S3rhlE0lRRUYGkpCQkJycjNDRUtTdcc8WkiYjI8Hx+IB6rD92ArbkJ9r8xQDXSo47NpxKxcOdFmJmIEertgJM3s+BiI8Wu1/rDza7+jY/f3RaLbWdT0MHdFrte6wcTSe1ThKdvZ+PjXZdxIVWGVjZSfPNMCHr4OtZ6/pnb2Ri/9iRspCY4s3BovQvKlcorFOi7/G/cy5djzTMhcLA0w+s/nUNmgRw2UhN8NiFIo8XlJWUV2HkuFeuO3kRCRuXOHKYSEcZ0bY2ZA9rCWmqCvsv/holYhOtLHzXIUhHqfn/rZCE4AHzxxRfw8PCAn58fBg0ahPj4eACVT62tWLFCV5clIiKqYvZQfwR72SOvpBxvbj2v9uLnG/fysWT3ZQDAeyMC8f2U7mjvaoOMfDle/N9ZlJTVXWQz+lY2tp2tLKS8ZGznOhMmAOjh64jfX+2Hn2f2xp9vDqgzYQKAEG8HtLKRIl9ejuM3MtX6TADw99V7uJcvh5OVGYZ2cEUfPyfsmdUfPX0dkS8vx0v/i8HSPZdRVqGotY9CeTn2XUjDW7+cR+9lf+H93y4gIaMQNuYmeCncD0fnDMbnE4IR4GoDh/sV7MsVAgrk5WrHaYh0kjQtWbIECxcuxJw5c3D69Okqq+aHDh2KX375RReXJSIiqsZUIsaqJ7vC0kyCqFvZ+PafhHrfIy+vwKyfzkNerkCYvzOm9fWFldQE657rDntLU8Qm52LBzou1PhVWVqHAgp0XAACTenqrPZ0nFovQu62TWlvliMUiPNpZ86fofj6dDKBynZdycbyrrTl+fKEXZg6onD5cd/QWnl53CnfzSlTvu5tXgh+jEjF1YzS6fXwQL/8Yg99iUpFbVAZ3O3MsGNUBJ94fjPcfDawyCmdhJoG5aeV1cgrL1I7TEOmkTtO3336LTz75BLNmzUJFRdVMvH379rh+/bouLktERFQjX2crLHq8E+Zsj8PKP6+hn59znU/CrfzzGi6n5cHB0hQrJgRDLK6cUvJ2ssTqSSF4bkMUtp9NQScPW0zrV30D5fXHbuHa3QI4WZnhvRG6q/f3aBd3/PdkIv68lI7ScV3qfULwTm4xDsdXrs968v7ehkqmEjHmjeyAEG8HvLstFqdv52DUV8cwsbsnjt/IRGyKrMr5vk6WeKSjK4Z2cEWoj0OdI2mOlma4IytBTlEpvJ0sG/hp9U8nI02ZmZno2LHmxwoVCgVKS0t1cVkiIqJaTQj1xKgu7ihXCHhj63kU1jJVdOJGJr47ehMA8OkTQWhlW3XtUn9/Z8wb2QEAsGTPlWpTY8nZRfgy8hoAYN7IDjrdYLmHryOcraXIKylXFb+syy9nkqEQgF5tHNHWxbrGc0Z0dsMfr/dHoJsNMgvkWHM4AbEpMohEQDdve8wZ0R6Rbw3AoXcGYv6ojujV1qneqUflPcgpMu7vf50kTR06dMD+/ftrbIuMjKy2ZxwREZGuiUQiLB3XGe525riVWYjF99crPSy3qBRv/RILQaicVhvWqebdLWb0b4OIbq1RoRDw6paYKpvnfrTrEkrKFOjVxhERIa119nkAQCIWYUTnynIDe//19N2/VSgE/HJ/am5Sz7rLJrRxtsKOV/phal9fDOvoiuURXRA1bwh2vNIPrwxsh3atbDRa0O1gVVnN3tiTJp1Mz82bNw9PPfUUysrK8MQTT0AkEuHKlSs4cOAAvv76a/z222+6uCwREVGd7C3NsHJiVzz9/Sn8fDoZ4QEueLRL5ZNigiBg3o4LSM8rQVtnKyx8rEOt/YhEInwS0QUJGQWITZHhhR/O4NeX++L4jUxEXrkHU0llgtYUT4qN7OyO/51KwoHL6VhS0RmmtYz6/HM9A3dkJbCzMMWIzvVvdWZhJsGixztpJUblYnBjX9Okk5Gm8ePHY/Pmzdi5cycGDhwIQRDw7LPP4ptvvsHGjRsxcuRIXVyWiIioXn38nPBSuB8A4P3fLiBNVgwA2H42BXsvpMNELMKqp7rB0qzucQVzUwnWPhsKZ2sprqbn482t57Hoj0sAgJkD2qqqd+tazzaOcLQyQ25RGaJuZtd63s/RSQCAiJDWDaog3hgOnJ6r26RJk5CYmIirV6/i2LFjuHz5MpKTkzFp0iRdXZKIiEgtbw4NQJfWdpAVl+HtX2JxK7NQlfC8NSwAXTyrb/9VE3c7C3z7bAhMJSL8efku7shK4OVogdcG+esy/CpMJGIMvz+NuPdizVN09/JL8NeVygXg9U3N6YKDZfOYntNZ0qQUEBCAvn37IjAw0CALWhERUctjZiLGl091hYWpBCcSsjBuzXEUllagVxtHvDjAT6O+Qn0c8fGYzqo/f/y49rYjUZdyA98DF9NrrEO1/WwKyhUCQrztEeDaNCNgD3OwUo40Gff0nNbWNCUlJWl0vrd302e6RERESn4u1vhwdEe8/9sF5BaVwcbcBCuf7AqJWPNf8JWjNwpBwKDAVtoOtV6VtZ1MkVVYiqhbWejr56xqUygEbL2/APwpPYwyAQ+vaTLukSatJU1t2lSvU1ETQRAgEomq1W8iIiJqak/28MLxhCzsibuD5RFBaK3BFiv/po9pLyVTiRjDOrrilzMp2HchvUrSdPJmFhKzimAjNcFjQepvj6JNHGn6F0EQYG1tjdGjR2PcuHFwcFCv+ikREZG+iEQirHqyKxaN7ggna6m+w2mUkV3c8cuZFOy/lI5Fj3dSjZj9dH8B+JhuHvUubtcV1ZomjjRVunjxIrZu3Ypt27Zh+/btGDJkCCZOnIhx48bBzk69BXVERERNTSwWGX3CBAB9/Zxha26CjHw5zibmoGcbR2QXluLPS3cBAE/10N9IGJ+e+5eOHTvio48+wuXLl3H69GmEhIRg2bJlcHV1xahRo/DDDz9AJpPV3xERERFpzMxEjEc6Kveiq3yK7reYFJRWKNCltR06t9bfAIZyek5erkBxqfEuz9HJ03NBQUFYsmQJ4uPjcerUKfj6+mLGjBmYPn26Li5HREREePAU3b6LaVAoBNXU3FM9vep6m85ZmUlgKqmcLsw24tEmnU1uFhYW4o8//sAvv/yC/fv3o3Xr1hg2bJiuLkdERNTi9fd3ho3UBHfz5Pju6E0kZBTCwlSCx4M99BqXSCSCg6UZ7uXLkVNY2qgF9/qk1aSpqKgIf/zxB7Zu3YoDBw6gVatWGD9+PA4fPoxevXpp81JERET0L1ITCYZ2dMWOc6n4/EA8AODxYA/YmJvqOTI8SJo40lS5dcq+ffvg4uKC8ePH49ChQ0yUiIiImtijnd2w41wqyu8XudT31JySvaoquPGWHdAoafL29sasWbPwzjvvVGv77bffYGVlBU9PT0RHRyM6OrrOvv755x/NIiUiIqJ6DQhwgZWZBIWlFQh0s0FXL3t9hwQAcLy/GDy3pYw0paSk4PLly6o/L1y4EI8++ij69u2L5557jtukEBER6Zm5qQSjgiprNj3Xx9dgvpvt75cdyDbiWk0aJU0WFhYoLi5W/Xnp0qVITU1F3759sWnTJm3HRkRERA2w6PFOGB/qhR6+hlNo2tGqcnou14in5zQqOeDv748jR44gOztbV/EQERFRI1mamaBnG0eDGWUCmkeBS42SpmeeeQbp6enw9/fHlClTAABZWVksWklERER1ag7TcxolTe+88w7mz5+PsrIybN68GSKRCLt374ajoyN8fX0xduxYLFq0CDt37sStW7d0EnBxcTE++OADBAQEwNzcHB4eHpg+fTpSU1Mb1N/t27fx0ksvoU2bNpBKpXB2dkafPn3w2WefaTlyIiKilqs5TM+JBEEQNH1TSUkJjh8/jkceeQS+vr5wcHDA5cuXIZfLKzu9Pxxoa2uLoKAgdO3aFd26dcPUqVMbFWxJSQkGDRqEU6dOwd3dHWFhYbh9+zaio6Ph4uKCU6dOoW3btmr3t2/fPowfPx7FxcUICQmBv78/srKycOHCBVhZWeHGjRsaxZeXlwc7OzvIZDLY2tpq+vGIiIiarZikHESsOYHW9hY4/v5gfYdThbrf3w2q02Rubo4hQ4YAAAYOHIgNGzagvLwcV65cwfnz5xEbG6v659GjR3H06FGIRKJGJ01LlizBqVOn0KdPH/z555+wtrYGAKxcuRJvv/02pk+fjsOHD6vV19WrVxEREQEbGxscPHgQffv2VbUpFArExMQ0KlYiIiJ6QLmmyZhLDjRopEnpyJEjMDExQb9+/Wo9JyUlRZVAzZ8/v6GXQmlpKVq1agWZTIaYmBh069atSntwcDDi4uJw5swZhIaG1tvfyJEjsW/fPuzZswcjR45scFwP40gTERFRzWRFZQj++E8AQPySEZCaSPQc0QM6HWlSCg8Pr/ccT09PeHp64rHHHmvMpXD8+HHIZDL4+flVS5iAyorkcXFx2LVrV71JU3JyMg4cOIC2bdtqLWEiIiKi2tmYm0AsAhRC5bomV1vDSZrUpbMNe7UtNjYWABASElJju/J4XFxcvX0dPnwYCoUCffv2RXl5OX777TccP34cFRUV6Ny5M5588kk4OBhObQsiIiJjJxZXbtqbVViKnKJSuNqa6zskjWmUNGVnZ8Pc3ByWlpYNvmBRURFKSkrg6Oio0fuSkpIAVI5c1UR5PDExsd6+lFXNra2tERYWhlOnTlVpnz9/PrZv345BgwbV2Y9cLlctfgcqh/eIiIioZvaWpsgqLDXasgMalRxwcXHB66+/3qgLvvrqq2jVqpXG7ysoKACAWhM2KysrAEB+fn69feXk5AAAvv/+e1y9ehVbtmxBdnY24uPjMXnyZGRnZ2PcuHH1ljFYtmwZ7OzsVC8vL8PYFJGIiMgQPVgMbpxlBzRKmgRBQCPWjVfpR58UCgUAoLy8HN9++y0mTZoEBwcHBAQEYPPmzejRowdkMhnWrFlTZz9z586FTCZTvZKTk5sifCIiIqPkYGXcVcE1XtN07NgxTJ8+vcEXPHbsWIPepywvUFRUVGN7YWEhAMDGxkbtvqytrTFhwoRq7dOmTcPp06dx5MiROvuRSqWQSqX1Xo+IiIgAB8vKApc5Rjo9p3HSdOPGDY2LPv5bQ/bC8fb2BlBZwqAmyuM+Pj719qU8x9vbu8ZYfH19AQD37t3TOE4iIiKq2YORJuOcntMoaTp06JCu4qhXcHAwANRadFJ5PCgoqN6+lCULlGub/k25IbFyRIqIiIgaT7Vpb0sYaVLWZZoxYwbWr1+vk4Bq069fP9jZ2SEhIQHnz59H165dq7Rv374dADB69Oh6++rbty+cnJyQnp6O+Ph4tG/fvkq7clqupnpQRERE1DCq6TkjXdOk0UJwpXPnzmk7jnqZmZnhtddeA1D5BJ5yDRNQuY1KXFwcwsPDqxS2XL16NQIDAzF37twqfZmYmOCtt96CIAh49dVXq5QKiIyMxKZNmyASifDiiy/q+FMRERG1HKqRppYwPVefAwcOYPjw4TW2paSk1FpjSV0LFixAZGQkTpw4AX9/f4SFhSExMRFRUVFwcXHBhg0bqpyfmZmJ+Ph4pKWlVevr3XffxaFDhxAZGYmAgAD07t0bmZmZOHXqFCoqKrB06VL07NmzUfESERHRA8b+9FyDRppq8+8RnYc9/vjjje7f3Nwchw4dwsKFC2FpaYmdO3ciMTERU6dORUxMDNq2bat2X6ampti7dy8+/fRTODs748CBA7hw4QLCw8Oxa9cuzJs3r9HxEhER0QPGvqapQRv2hoSE1Lggu1u3brVO3dXV1lxww14iIqLaZRXIEbokEgBwY+mjMJFodeymwdT9/tZqtHWVEmhImQEiIiJqPuwsTFX/nltsfOuaDCPFIyIiombPRCJWJU65RriuqUFJkyAIGDhwIF5++WXs3LlTrf3eiIiIiJRlB7ILjW+kqUFPz4lEIhw+fBjXr1/HX3/9hZkzZ6KoqAjJyck4efIkevbsCYlEou1YiYiIyMjZW5oBWUVG+QRdg0sOfPrppxg6dCheeuklvPTSSxAEATExMYiMjMR//vMfiEQi9OrVC4MGDapSO4mIiIhaLsf7ZQeMcXquQUlTZGQk/v77b6xbtw6vvPIKfHx8MHToUAwdOhTvvfceAKC0tBTHjh3D77//jnnz5jV6vzoiIiIyfvYtbXrO0dER48ePx/jx4wEAiYmJiIyMxPz583Hr1i0EBwdj6NChGDJkCAYPHgygskwBERERtWyOli1spOnffHx8MGPGDMyYMQMAEBsbi7/++gvPPfcc8vPz0adPH9UmuERERNRyKauCZxthgUutbqOiFBwcjODgYLz11lsoKyvDyZMnIZVKdXEpIiIiMiL2qk17W8j0nCZMTU0xYMAADBgwQNeXIiIiIgNnzNNzLG5JRERETcb+ftKUzaSJiIiIqHYPSg4Y3/QckyYiIiJqMsqK4LlFpVAoBD1HoxkmTURERNRklNNzCgHIKzGu0SYmTURERNRkzEzEsJZWPodmbE/QMWkiIiKiJvWgKrhxLQZn0kRERERNysFIyw4waSIiIqImZaxVwZk0ERERUZN68AQd1zQRERER1Uo5PZfD6TkiIiKi2jFpIiIiIlKDg9X9TXsLOT1HREREVCsHI91/jkkTERERNSmWHCAiIiJSg2p6jk/PEREREdVOtRC8sBSCYDyb9hpl0lRcXIwPPvgAAQEBMDc3h4eHB6ZPn47U1NRG9Xv9+nVYWFhAJBJh6NChWoqWiIiIHqZMmsoVAgrk5XqORn1GlzSVlJRg8ODBWLx4MQoKCjBmzBh4eXlh48aN6NatG27evNngvmfOnAm5XK7FaImIiOjfLMwkMDetTEGMqcCl0SVNS5YswalTp9CnTx9cu3YNW7duRVRUFFasWIGMjAxMnz69Qf2uX78ehw8fxgsvvKDliImIiOjfVE/QGdFWKkaVNJWWlmL16tUAgG+++QbW1taqtrfeegtBQUE4cuQIzp49q1G/d+/exbvvvotHHnkEkyZN0mrMREREVJ0xFrg0qqTp+PHjkMlk8PPzQ7du3aq1jx8/HgCwa9cujfqdPXs2iouLsWbNGq3ESURERHV78AQdkyadiI2NBQCEhITU2K48HhcXp3afe/fuxdatWzFv3jy0a9eu8UESERFRvexVT9AZz5omE30HoImkpCQAgKenZ43tyuOJiYlq9VdYWIhXXnkF7du3x3vvvadxPHK5vMrC8by8PI37ICIiaokcjbDApVGNNBUUFAAALC0ta2y3srICAOTn56vV34IFC5CYmIi1a9fCzMxM43iWLVsGOzs71cvLy0vjPoiIiFoiB8vK6Tlj2krFqJImbTpz5gy++uorPPfccxg4cGCD+pg7dy5kMpnqlZycrN0giYiIminV9JwRlRwwquk55dNyRUVFNbYXFhYCAGxsbOrsp7y8HC+88ALs7e3x+eefNzgeqVQKqVTa4PcTERG1VI5WD6qCGwujSpq8vb0BACkpKTW2K4/7+PjU2U9KSgrOnz8PNzc3TJgwoUpbbm4uAODs2bOqEajDhw83PGgiIiKqxt7S+PafM6qkKTg4GAAQExNTY7vyeFBQkFr9paenIz09vca23NxcHDlypAFREhERUX2UI01cCK4j/fr1g52dHRISEnD+/Plq7du3bwcAjB49us5+fH19IQhCja9Dhw4BAIYMGaI6RkRERNrFiuA6ZmZmhtdeew0A8Oqrr6rWMAHAypUrERcXh/DwcISGhqqOr169GoGBgZg7d26Tx0tEREQ1U07PycsVKC6t0HM06jGq6TmgskxAZGQkTpw4AX9/f4SFhSExMRFRUVFwcXHBhg0bqpyfmZmJ+Ph4pKWl6SliIiIi+jdrqQlMJSKUVQjILipFazMLfYdUL6MaaQIAc3NzHDp0CAsXLoSlpSV27tyJxMRETJ06FTExMWjbtq2+QyQiIqJ6iESih6qCG8cUnUjgoh2tycvLg52dHWQyGWxtbfUdDhERkUEb/sU/iL+bj//N6IX+/s56i0Pd72+jG2kiIiKi5sHeyKqCM2kiIiIivXAwsv3nmDQRERGRXjhYGVfZASZNREREpBfKTXtzjaQqOJMmIiIi0gvV/nOcniMiIiKqnb2RVQVn0kRERER6wek5IiIiIjU4cHqOiIiIqH4ORlYRnEkTERER6YVyeq6wtALycsPftJdJExEREemFrbkpxKLKfzeGdU1MmoiIiEgvxOKHNu01gnVNTJqIiIhIb5RTdDmFHGkiIiIiqpUDR5qIiIiI6sfpOSIiIiI1OFopp+eYNBERERHV6sH0HNc0EREREdXKmKqCM2kiIiIivXnw9ByTJiIiIqJa2XN6joiIiKh+jpyeIyIiIqofp+eIiIiI1KB8ei6vpBzlFQo9R1M3Jk1ERESkN3YWpqp/zy027HVNTJqIiIhIb0wkYtiamwAAcg18XROTJiIiItIr5WLwbAPftJdJExEREemVsew/Z5RJU3FxMT744AMEBATA3NwcHh4emD59OlJTU9XuIzc3F1u2bMGkSZPQpk0bmJmZwcbGBr169cKqVatQVmbY2S4REVFzoXyCztCn50z0HYCmSkpKMHjwYJw6dQru7u4YM2YMbt++jY0bN2L37t04deoU2rZtW28/n3/+OZYuXQqRSISuXbuiV69eyMjIwPHjxxEdHY3t27fjwIEDsLS0bIJPRURE1HI5cHpON5YsWYJTp06hT58+uHbtGrZu3YqoqCisWLECGRkZmD59ulr9WFlZYc6cObh9+zZiYmLw888/46+//sKFCxfg7e2NY8eOYcmSJTr+NERERKQsO2DoI00iQRAEfQehrtLSUrRq1QoymQwxMTHo1q1blfbg4GDExcXhzJkzCA0NbfB1fvrpJzz99NPw9fXFrVu31H5fXl4e7OzsIJPJYGtr2+DrExERtSTfHLqBzw7E44kQT6yYGNzk11f3+9uoRpqOHz8OmUwGPz+/agkTAIwfPx4AsGvXrkZdJzi48gd2586dRvVDRERE9fN1sgIAXL+Xr+dI6mZUSVNsbCwAICQkpMZ25fG4uLhGXefmzZsAADc3t0b1Q0RERPXr5FE5unM1PR9lBlwV3KgWgiclJQEAPD09a2xXHk9MTGzUdVatWgUAGDNmTJ3nyeVyyOVy1Z/z8vIadV0iIqKWyNvREtZSExTIy5GQUYBAN8Nc4mJUI00FBQUAUOsTbVZWlcN7+fkNH95bu3YtIiMjYW9vj/fff7/Oc5ctWwY7OzvVy8vLq8HXJSIiaqnEYhE6ulcmSpdSDXcAwqiSJl07evQoZs+eDZFIhA0bNsDDw6PO8+fOnQuZTKZ6JScnN1GkREREzUvH+1N0l+4YbtJkVNNz1tbWAICioqIa2wsLCwEANjY2Gvd98eJFjBkzBqWlpfjqq68wbty4et8jlUohlUo1vhYRERFV1UmVNMn0HEntjGqkydvbGwCQkpJSY7vyuI+Pj0b93rp1C8OGDUNOTg4WLVqE119/vXGBEhERkUY6edgBAC6n5cFQqyEZVdKkLAUQExNTY7vyeFBQkNp9pqWl4ZFHHkFaWhpmz56NDz/8sPGBEhERkUb8Xa1hJhEjv6QcydnF+g6nRkaVNPXr1w92dnZISEjA+fPnq7Vv374dADB69Gi1+svJycHw4cORkJCAadOm4YsvvtBmuERERKQmU4kYAW6Vy3AMdYrOqJImMzMzvPbaawCAV199VbWGCQBWrlyJuLg4hIeHV6kGvnr1agQGBmLu3LlV+ioqKsKoUaNw4cIFTJw4EevWrYNIJGqaD0JERETVdHKvnKIz1MXgRrUQHAAWLFiAyMhInDhxAv7+/ggLC0NiYiKioqLg4uKCDRs2VDk/MzMT8fHxSEtLq3J8/vz5OHnyJCQSCUxMTDBjxowar7dp0yZdfRQiIiJ6SKfWtsAZwx1pMrqkydzcHIcOHcKyZcuwZcsW7Ny5E46Ojpg6dSoWL15ca+HLf8vJyQEAVFRUYMuWLbWex6SJiIioaXQy8LIDRrVhr6Hjhr1EREQNVygvR+dFByAIwOn5Q+Fi0zRlfZrlhr1ERETUfFlJTdDGuXJ3D0OcomPSRERERAZDWa/JEKfomDQRERGRwVCua7rMpImIiIiodoa8nQqTJiIiIjIYyum521lFyC8p03M0VTFpIiIiIoPhaGUGdztzAMCVtHw9R1MVkyYiIiIyKIY6RcekiYiIiAxKRwN9go5JExERERkUQ60MzqSJiIiIDIoyabp+Nx/y8go9R/MAkyYiIiIyKK3tLWBnYYpyhYDrdwv0HY4KkyYiIiIyKCKRyCAXgzNpIiIiIoNjiOuamDQRERGRwTHEPeiYNBEREZHBUY40XUnLQ4VC0HM0lZg0ERERkcFp62INc1MxikorcDurUN/hAGDSRERERAZIIhYh0M2w1jUxaSIiIiKDpFoMnmoYT9AxaSIiIiKDZGiLwZk0ERERkUF6uFaTIOh/MTiTJiIiIjJI7d1sIBGLkFNUhjRZib7DYdJEREREhsncVIJ2LtYADGOKjkkTERERGSxD2k6FSRMREREZrI4GtJ0KkyYiIiIyWMon6C4zaSIiIiKqnXKkKTW3GDmFpXqNxSiTpuLiYnzwwQcICAiAubk5PDw8MH36dKSmpmrcV05ODmbPng0fHx9IpVL4+PjgjTfeQG5urvYDJyIiIo3YWZjCy9ECAHA5Tb+jTUaXNJWUlGDw4MFYvHgxCgoKMGbMGHh5eWHjxo3o1q0bbt68qXZfmZmZ6NmzJ7766iuYmJhg7NixsLGxwapVq9CrVy9kZ2fr8JMQERGROjq5K4tc6ncxuNElTUuWLMGpU6fQp08fXLt2DVu3bkVUVBRWrFiBjIwMTJ8+Xe2+3njjDdy4cQMRERGIj4/H1q1bcfHiRbz++uu4du0a3nrrLR1+EiIiIlJH59aGsRhcJBhCiU01lZaWolWrVpDJZIiJiUG3bt2qtAcHByMuLg5nzpxBaGhonX2lpaXB09MTJiYmSEpKgqurq6pNLpfDy8sL2dnZuHPnDlq1aqVWfHl5ebCzs4NMJoOtra3mH5CIiIiqOXT1HqZtOo12rawR+Va41vtX9/vbqEaajh8/DplMBj8/v2oJEwCMHz8eALBr1656+9q/fz8UCgXCwsKqJEwAIJVKMXr0aFRUVGDv3r3aCZ6IiIgaRFmr6WZGAYpLK/QWh1ElTbGxsQCAkJCQGtuVx+Pi4pq0LyIiItKdVrbmcLaWQiEAV9L1N0VnVElTUlISAMDT07PGduXxxMTEJulLLpcjLy+vyouIiIi0r5MBFLk0qqSpoKAAAGBpaVlju5WVFQAgPz+/SfpatmwZ7OzsVC8vL696r0tERESa6+RhC3NTMfKKy/QWg1ElTYZm7ty5kMlkqldycrK+QyIiImqWXhvcDpc+GoFXB7XTWwwmertyA1hbV+50XFRUVGN7YWEhAMDGxqZJ+pJKpZBKpfVei4iIiBrH0kz/KYtRjTR5e3sDAFJSUmpsVx738fFp0r6IiIio+TOqpCk4OBgAEBMTU2O78nhQUFCT9kVERETNn1ElTf369YOdnR0SEhJw/vz5au3bt28HAIwePbrevkaMGAGxWIyjR4/i3r17Vdrkcjl27doFiUSCkSNHaiV2IiIiMm5GlTSZmZnhtddeAwC8+uqrqnVHALBy5UrExcUhPDy8SjXw1atXIzAwEHPnzq3Sl7u7OyZNmoTS0lK88sorKC8vV7XNmTMHGRkZmDx5strVwImIiKh50/+qKg0tWLAAkZGROHHiBPz9/REWFobExERERUXBxcUFGzZsqHJ+ZmYm4uPjkZaWVq2vL7/8EqdOncKvv/6KwMBAdO/eHZcuXcLFixfh7++PlStXNtXHIiIiIgNnVCNNAGBubo5Dhw5h4cKFsLS0xM6dO5GYmIipU6ciJiYGbdu2VbsvZ2dnREdH4/XXX0dpaSl27NgBmUyGWbNmITo6Go6Ojjr8JERERGRMjGrDXkPHDXuJiIiMT7PcsJeIiIhIX5g0EREREamBSRMRERGRGpg0EREREamBSRMRERGRGpg0EREREanB6IpbGjJl9Ya8vDw9R0JERETqUn5v11eFiUmTFuXn5wMAvLy89BwJERERaSo/Px92dna1trO4pRYpFArcuXMHNjY2EIlEWu07Ly8PXl5eSE5OZuFMHeD91S3eX93jPdYt3l/d0+c9FgQB+fn58PDwgFhc+8oljjRpkVgshqenp06vYWtry/9hdYj3V7d4f3WP91i3eH91T1/3uK4RJiUuBCciIiJSA5MmIiIiIjUwaTISUqkUH374IaRSqb5DaZZ4f3WL91f3eI91i/dX94zhHnMhOBEREZEaONJEREREpAYmTURERERqYNJEREREpAYmTQasuLgYH3zwAQICAmBubg4PDw9Mnz4dqamp+g7NaJw9exbLly9HREQEPD09IRKJ1Co8umnTJvTs2RPW1tZwdHTEyJEjceLEiSaI2LgUFRVh586dmDFjBtq3bw9zc3NYWVkhODgYH3/8MQoKCmp9L++xelauXImIiAj4+/vDzs4OUqkUPj4+eO6553DhwoVa38f72zBZWVlo1aoVRCIR2rVrV+e5vMfqGThwoOrv3ppe+/fvr/F9Bnl/BTJIxcXFQu/evQUAgru7uzBx4kShZ8+eAgDBxcVFSEhI0HeIRmHMmDECgGqvusyePVsAIFhYWAhjxowRhg8fLpiYmAgSiUTYsWNH0wRuJNatW6e6px06dBAmTJggDB8+XLCxsREACIGBgcLdu3ervY/3WH1OTk6Cubm50LNnT2HcuHHCuHHjhICAAAGAYGpqKuzatavae3h/G27KlCmCSCQSAAh+fn61nsd7rL7w8HABgPDEE08IU6ZMqfaKi4ur9h5Dvb9MmgzU/PnzBQBCnz59hPz8fNXxFStWCACE8PBw/QVnRJYvXy4sXLhQ+OOPP4S0tDRBKpXWmTQdPHhQACA4OTkJ165dUx0/ceKEYGZmJtjb2ws5OTlNELlx2LRpkzBz5kzh8uXLVY7fuXNH6NatmwBAmDRpUpU23mPNHDt2TCguLq52/JtvvhEACK6urkJZWZnqOO9vw0VGRgoAhJkzZ9aZNPEea0aZNN26dUut8w35/jJpMkByuVyws7MTAAgxMTHV2oOCggQAwpkzZ/QQnXGrL2l69NFHBQDCF198Ua1t1qxZAgDh888/12GEzceJEycEAIJUKhXkcrnqOO+x9vj5+QkAhNjYWNUx3t+GKSoqEvz8/ISOHTsK165dqzNp4j3WjKZJkyHfX65pMkDHjx+HTCaDn58funXrVq19/PjxAIBdu3Y1dWjNWnFxMf7++28AD+7xw3jfNRMcHAwAkMvlyMrKAsB7rG2mpqYAADMzMwC8v43x0Ucf4ebNm1i7dq3qvtaE91i3DP3+csNeAxQbGwsACAkJqbFdeTwuLq7JYmoJ4uPjIZfL4eLiUuPGy7zvmrl58yaAyi92R0dHALzH2rR582bEx8fD398f/v7+AHh/GyouLg4rVqzAtGnTEBYWhtu3b9d6Lu9xw61fvx5ZWVkQi8UICAjA2LFj4e3tXeUcQ7+/TJoMUFJSEgDU+B/Mw8cTExObLKaWoL77bmVlBXt7e+Tk5CA/Px82NjZNGZ7RWbVqFQBgxIgRqm0ReI8b7rPPPsOlS5dQWFiIK1eu4NKlS/Dw8MBPP/0EiUQCgPe3IRQKBZ5//nnY29vjP//5T73n8x433JIlS6r8+Z133sHChQuxcOFC1TFDv7+cnjNAyse0LS0ta2y3srICAOTn5zdZTC1Bffcd4L1X1969e7F+/XqYmppi8eLFquO8xw134MAB/Pe//8X27dtx6dIl+Pj44KeffkJoaKjqHN5fzX399dc4ffo0PvvsMzg5OdV7Pu+x5gYMGIDNmzcjISEBRUVFiI+Px9KlS2FiYoIPPvhA9QsWYPj3l0kTEWnV1atXMXnyZAiCgM8++0y1tokaJzIyEoIgICcnB//88w/8/f0RHh6OpUuX6js0o5WUlIQFCxYgPDwcU6dO1Xc4zdbHH3+MyZMno23btrCwsEBAQADmzZuHnTt3AgAWLVqE4uJi/QapJiZNBsja2hpAZeHAmhQWFgIAh321rL77DvDe1yc1NRUjRoxATk4O3nrrLcyePbtKO+9x49nb2yMsLAx79+5FaGgoFi5ciNOnTwPg/dXUq6++itLSUqxdu1bt9/Aea8+wYcPQvXt35ObmIioqCoDh31+uaTJAyoVxKSkpNbYrj/v4+DRZTC1Bffe9sLAQubm5cHBw4F+GNcjOzsawYcOQmJiIadOm4fPPP692Du+x9piamuLJJ5/E2bNnsWvXLvTo0YP3V0O7d++Gvb09XnrppSrHS0pKAFT+EjBw4EAAwM8//ww3NzfeYy3z9/fHmTNnkJaWBsDw/45g0mSAlNMZMTExNbYrjwcFBTVZTC1B+/btIZVKkZGRgdTUVLRu3bpKO+977QoKCvDoo4/i8uXLiIiIwLp162rcrob3WLucnZ0BABkZGQB4fxsiNzcXR44cqbGtpKRE1aZMpHiPtSsnJwfAg3VKhn5/OT1ngPr16wc7OzskJCTg/Pnz1dq3b98OABg9enQTR9a8WVhYYPDgwQCAbdu2VWvnfa+ZXC7HmDFjEB0djeHDh1d5muvfeI+1S/mF7ufnB4D3V1NCZYHnaq9bt24BqLyvymO+vr4AeI+1KSMjA0ePHgXwoJSAwd9fvZTUpHopt1Hp27evUFBQoDrObVQapzHbqEilUm6P8C/l5eXCuHHjBABCWFiYUFhYWO97eI/Vd+zYMWHfvn1CRUVFleOlpaXCV199JYjFYsHCwkJISkpStfH+Nt6tW7cavI0K73FVx48fF3bs2CGUl5dXOX7r1i2hX79+AgDh8ccfr9JmyPeXSZOBKi4uFnr16lVlw17ln7lhr/p2794t9OrVS/VSbsT58LHdu3dXeY9yo0hLS0thzJgxwqOPPmoQG0Uaoi+//FK1Ye+4ceNq3IxzypQpQkZGRpX38R6rZ+PGjQIAwdnZWRg+fLjw9NNPC8OGDRPc3d0FAIK5ubmwdevWau/j/W2c+pImQeA9Vpfyv2E3Nzdh5MiRwtNPPy3069dPMDc3FwAInTp1qnNTb0O7v0yaDFhRUZGwcOFCwc/PTzAzMxPc3NyEqVOnCsnJyfoOzWgo/4et67Vx48Ya3xcaGipYWloK9vb2wogRI4Tjx483/QcwcB9++GG99xe17DnFe1y/mzdvCvPmzRP69esnuLu7C6ampoKVlZXQqVMn4fXXXxeuX79e63t5fxtOnaRJEHiP1XH58mXh5ZdfFkJCQgQXFxfBxMREsLOzE3r37i2sWLFCKCoqqvW9hnh/RYIgCLqZ+CMiIiJqPrgQnIiIiEgNTJqIiIiI1MCkiYiIiEgNTJqIiIiI1MCkiYiIiEgNTJqIiIiI1MCkiYiIiEgNTJqIiIiI1MCkiYiIiEgNTJqIiIiI1MCkiYiIiEgNTJqI9KCoqAhfffUVhg0bBnd3d0ilUtjY2KBjx46YOnUq/vjjD1RUVOg7TIMnEong6+ur7zBqJBKJIBKJ9B2Gzt2+fRsikQgDBw7UWp+bNm2CSCTCpk2bDLI/armYNBE1sePHj6Ndu3aYPXs2jh49Cn9/f4wbNw5Dhw6FiYkJ/vvf/2LMmDEICgrSd6h6pYsvY9Icfw5ED5joOwCiliQmJgZDhgyBXC7Hu+++iwULFsDW1rbKOcnJyVi5ciXWrl2rpyiNx5UrV2BqaqrvMIiohWDSRNREFAoFJk+eDLlcjsWLF2PBggU1nufl5YUvvvgCkydPbuIIjU9gYKC+QyCiFoTTc0RNZO/evbhy5Qq8vb0xd+7ces8PDQ2t8XhycjJee+01+Pn5wdzcHI6Ojnjsscdw4sSJauc+PLVSXFyM999/Hz4+PpBKpWjXrh0+/fRTCIKg1evk5eXhrbfeQps2bWBqaoo33ngDALBnzx5Mnz4dHTp0gK2tLaysrBAcHIxPPvkEcrm8Sn+LFi1CmzZtAABHjhxRrQ8SiUSYOnWq6ry61jSdPHkSY8aMgYuLC6RSKXx9ffHKK6/gzp07Wr1P2tJUP9fffvsNvXv3hqWlJZydnTFhwgTcuHEDixYtqrbuR92fg5Iu793ixYtV1xaLxUhISKh2zrvvvqs6x9TUFHfv3m30dYmqEIioSbzyyisCAOHtt99ucB8nTpwQHBwcBABC+/bthYiICCEsLEwwMTERJBKJ8PPPP1c5/9atWwIAoU+fPkL//v0FR0dHISIiQhg+fLhgbm4uABDmz5+vtev07NlT6Nq1q+Dg4CCMHTtWiIiIEBYtWiQIgiC4uroKtra2Qt++fYWJEycKw4cPV11j8ODBQnl5uaq/HTt2CE888YQAQHB1dRWmTJmieq1bt051HgDBx8enWvybN28WJBKJAEDo16+f8NRTTwkBAQGq/q5cuaKV+1QXAIK6f8U21c/1yy+/FAAIYrFYGDhwoPDUU08Jbdq0ERwcHITnnntOACBs3LhRdb46Pwdd3LuNGzdWiyU9PV0wMzNT3df333+/2vt8fHxU7Y8//nid/RE1BJMmoibSr18/AYDwv//9r0Hvl8lkgru7uyCRSKr1cfr0acHBwUGwtrYW7t27pzqu/EIDIISHhwsymazKeyQSiWBpaSnk5+dr7Tp9+vQRcnJyqsW/c+dOoaioqMqxvLw84bHHHhMACP/973+rtCn7DA8Pr/We1JQ0JSUlCRYWFoJEIhF+//131fGKigrhjTfeEAAI3bt3r/Famtyn+qibNDXVzzUhIUEwMzMTzMzMhL///lt1vKysTJg2bZqqv38nFvX9HHRx72pLciZPnqy6lpubm1BaWqpqO3nypKoNgLBz5856+yPSFJMmoiYSGBgoABD2799fY/v06dOr/CY/ZcoU4ejRo6r2L774os6RqpUrVwoAhJUrV6qOKb/QxGKxcPXq1WrvUSYshw4d0sp1AAinT5+u8z782/Xr1wUAQkRERJXjDU2aPvjgAwGAMGnSpGrnl5SUCB4eHgIA4dixY9Wupcl9qo+6SVNT/Vznz58vABBmzJhR7fycnBzB2tq6UUmTNu9dbUlOdHR0lcTo119/VbW9+eabquOurq5CWVlZvf0RaYoLwYkMxH//+99qtZkGDhyI/v37AwD+/PNPAEBERESN7w8LCwMAREdHV2vz8fFB+/btqx0PCAgAAKSlpamONeY67u7u6N69e43vA4Dr169j7969uHHjBgoLC6FQKFTrXa5fv17r+zRx9OhRAMAzzzxTrU0qlWLChAlYtWoVjh49in79+lVp1+Q+aUtT/VyPHz8OAJgwYUK18+3t7TFs2DD89ttvGkbfsFgaqkePHujVqxeioqIAAN999x0iIiIgCAK2b9+uOu/ZZ5+FiQm/3kj7+F8VURNxcnICAGRmZtbYXl5ervr3l156Cd9++22V9tu3bwNAtS/6f6upf09PzxrPtbGxAYAqC7Ebcx1vb+8azxUEAe+88w6++OKLWhcF5+fn13k9dSkXete2QFx5PDU1tVqbJvdJW5rq56pMWry8vGp8T20/O3U11b2bNWuWKiE+ePAgEhMTkZqaiuTkZNU506ZN08q1iP6NSRNREwkODsbx48dx7ty5GkdB6qNQKAAA48ePh5WVVa3n1fQYvlis/oOyjbmOubl5jedu3boVK1euVJVT6NOnD1xcXGBqaorS0lJIpVKdP52mVFeVbk3uk7Y01c9V15oqlgkTJuDtt99Geno6FAoFvv/++yoJd69evdCxY8cmiYVaHiZNRE3k0UcfxZo1a7Bt2zZ8+umnkEgkGr3f09MT8fHxeP/992stR6ANurjOjh07AAD/93//h1GjRlVpu3nzplauoeTh4YH4+HgkJiaiU6dO1dqVIzutW7fW6nUbqql+ru7u7oiPj0dycnKNScXDIzWGzNTUFC+99BIWLVoEANiwYUOVRHj69Ol6ioxaAsP5NYWomRs5ciQ6dOiApKQkLFu2TOP3P/LIIwAeJCC6oovr5OTkAKh5CueXX36p8T1mZmYAqk5bqkO5Buinn36q1lZaWopt27ZVOU/fmurnqpz++/XXX6u1yWQy1dqqf2voz0GXXnzxRVVcd+7cUU21WlhY4KmnntJnaNTMMWkiaiJisRibN2+GVCrFwoULMWfOHMhksmrnZWVlIT4+vtrxF198Ea1atcJ//vMffPfdd6ppHaXy8nIcOHAAFy9ebFScuriOcjHwd999V2Ua7ujRo/jss89qfI+zszNMTU2RkJCg0ebFM2bMgIWFBX7++Wfs2bNHdVyhUGDevHlITU1FaGhovWuImkpT/VynTZsGMzMz/PDDD/jnn39UxysqKvD222/XuqasoT8HXXJzc6txQfsTTzxRbVsiIm1i0kTUhEJDQxEZGQk3Nzd89tlncHV1RXh4OCZNmoRx48ahR48ecHd3x+HDhxEYGFjlSTR7e3v8/vvvsLOzw4svvghfX1+MHDkSzzzzDIYMGQIXFxeMGDECN27caFSMurjOrFmzYGVlhTVr1qBz586YNGkSBgwYgPDwcLz00ks1vsfMzAwjRoxAeno6goOD8dxzz+H555/Hxo0b67yWt7c3vv32WygUCowePRphYWF4+umn0bFjR6xYsQKurq743//+p9E9aYzevXvX+vr++++b7Ofq5+eH//znP5DL5Rg0aBAGDx6MSZMmISAgAL/++qtq2x7lCI5SQ38Ouvb6669XO8apOdI1Jk1ETax///5ISEjAqlWr0L9/f8THx+PXX39FZGQk8vPzMXHiROzYsQMXLlxA586dq7y3d+/euHDhAubMmQNbW1scOXIEO3fuRGJiIsLDw7Fp0yYMHTq00TFq+zoBAQE4c+YMRo8ejczMTPzxxx8oKCjAt99+W+tIEwB8//33ePbZZ5GVlYUtW7Zg/fr1OHLkSL3Xe/bZZ3H06FE89thjuHLlCrZv347i4mK8/PLLOHv2bJPuWRcVFVXrKyUlBUDT/Vxnz56N7du3o3v37jh16hQOHDiArl27IioqSrWIX/mU58Ma+nPQpV69eqFHjx6qP7dt2xYDBw7UX0DUIoiEpnpkhYiIDFJFRQWCgoJw5coV3LlzB25ubnqNZ9OmTZg2bRo2btxY4x53QGXMHTp0UNX3qmsTbHX6I1IHn54jImohEhIS4OTkBHt7e9UxuVyOefPm4fLlyxg6dKjeE6b6rFu3DhkZGYiMjFQlTFZWVpg5c6aeI6OWgEkTEVELsW3bNnz44YcIDQ2Fl5cX8vLyEBsbi7S0NDg7O2P16tX6DrFeS5cuRWJiYpVjn3zyCVq1aqWniKglYdJERNRCDBkyBLGxsTh16hTi4uJQXl6O1q1b4+WXX8bcuXNrrRZuiCwsLBAYGIi33npLtYidSNeYNBERtRA9evSosX6VMVEWJyXSBz49R0REBqVr16748MMP0bVrV4Psj1ouPj1HREREpAaONBERERGpgUkTERERkRqYNBERERGpgUkTERERkRqYNBERERGpgUkTERERkRqYNBERERGpgUkTERERkRqYNBERERGp4f8BD0W2QUx8n44AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mean_vals = np.nanmean(mem_vals, axis=0)\n",
    "plt.plot(np.arange(0, len(mean_vals)), mean_vals)\n",
    "plt.xlabel(\"Generation Length $|\\mathbf{y}|$\", fontsize=15)\n",
    "plt.ylabel(r'$\\mathbb{E}[f_{\\text{Mem}}(\\overline{p}_{\\theta})]$', fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.savefig(\"gen_len_analysis.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d1ad54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f426ec09-c9a8-4db5-8ffc-91f0972ca151",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_dict = evaluator.evaluate(predictions, references, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0465a839",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    token=access_token,\n",
    "    cache_dir=cache_dir,\n",
    "    local_files_only=True,\n",
    "    #device_map=\"auto\",\n",
    "    #max_memory = {0: \"0GB\", 1: \"0GB\", 2: \"35GB\", 3: \"35GB\", 4: \"35GB\", 5: \"35GB\", 6: \"35GB\", 7: \"35GB\"}\n",
    "    ).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "42608e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, response = test_set[53], predictions[53]\n",
    "n = 2\n",
    "t = 20\n",
    "temperature=0.8\n",
    "stop_token_ids = [tokenizer.eos_token_id,\n",
    "                      tokenizer.pad_token_id,\n",
    "                     ]\n",
    "\n",
    "context_unaware_tokenized_input = tokenizer(template_empty_input(data, dataset_name), return_tensors=\"pt\", padding=True)\n",
    "context_aware_tokenized_input = tokenizer(template_input(data, dataset_name), return_tensors=\"pt\", padding=True)\n",
    "\n",
    "ensemble, n_grams = partition_n_gram(data, tokenizer, dataset_name, n)\n",
    "ensemble_context_aware_tokenized_input = tokenizer(ensemble, return_tensors=\"pt\", padding=True)\n",
    "ensemble_context_aware_tokenized_input_ids = ensemble_context_aware_tokenized_input.input_ids.to(DEVICE)\n",
    "\n",
    "response_tokenized_input = tokenizer(response, return_tensors=\"pt\")\n",
    "\n",
    "context_aware_input_ids = context_aware_tokenized_input.input_ids.to(DEVICE)\n",
    "response_input_ids = response_tokenized_input.input_ids.to(DEVICE)\n",
    "context_unaware_input_ids = context_unaware_tokenized_input.input_ids.to(DEVICE)\n",
    "\n",
    "N = context_aware_input_ids.shape[0]\n",
    "priv_context_aware_input_ids = torch.cat([context_aware_input_ids,\n",
    "                              response_input_ids[:, :t].repeat(N, 1)],\n",
    "                             dim=1)\n",
    "batch_size = 32\n",
    "with torch.no_grad():\n",
    "    proj_output, pub_output, ensemble_proj_output = calc_distributions(model, \n",
    "                                                                          context_aware_input_ids, \n",
    "                                                                          response_input_ids,\n",
    "                                                                          lambd,\n",
    "                                                                          temperature,\n",
    "                                                                          stop_token_ids,\n",
    "                                                                          min_new_tokens,\n",
    "                                                                          t,\n",
    "                                                                          batch_size,\n",
    "                                                                          ensemble_context_aware_tokenized_input_ids)\n",
    "ids = torch.nonzero(pub_output)  \n",
    "mem_vals = calc_group_memorization(proj_output[ids], ensemble_proj_output[:, ids].squeeze(-1), response_input_ids[:, t].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8d0e0b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' aliens'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(response_input_ids[-1, 18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "eaf16500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def value_to_color(value, min_value, max_value):\n",
    "    \"\"\" Map a float value to a color based on its position in the value range. \"\"\"\n",
    "    norm = mcolors.Normalize(vmin=min_value, vmax=max_value)\n",
    "    cmap = plt.get_cmap('coolwarm')  # You can choose different colormaps\n",
    "    return mcolors.to_hex(cmap(norm(value)))\n",
    "\n",
    "# Normalize and colorize\n",
    "min_value = min(mem_vals)\n",
    "max_value = max(mem_vals)\n",
    "\n",
    "colors = [value_to_color(val, min_value, max_value) for val in mem_vals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7fadf9ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><span style=\"color: #ead5c9;\"> Luckily,</span>, <span style=\"color: #f7b599;\">, Japanese</span>, <span style=\"color: #f7b396;\"> Japanese can</span>, <span style=\"color: #f39577;\"> can sleep</span>, <span style=\"color: #f6bfa6;\"> sleep sound</span>, <span style=\"color: #e8d6cc;\"> soundly</span>, <span style=\"color: #ecd3c5;\">ly in</span>, <span style=\"color: #dddcdc;\"> in their</span>, <span style=\"color: #e2dad5;\"> their beds</span>, <span style=\"color: #f7b89c;\"> beds tonight</span>, <span style=\"color: #f6a586;\"> tonight as</span>, <span style=\"color: #dddcdc;\"> as the</span>, <span style=\"color: #f4c5ad;\"> the government</span>, <span style=\"color: #b5cdfa;\"> government's</span>, <span style=\"color: #b2ccfb;\">'s top</span>, <span style=\"color: #c1d4f4;\"> top military</span>, <span style=\"color: #d8dce2;\"> military official</span>, <span style=\"color: #c7d7f0;\"> official earnest</span>, <span style=\"color: #f7a889;\"> earnestly</span>, <span style=\"color: #94b6ff;\">ly revealed</span>, <span style=\"color: #799cf8;\"> revealed that</span>, <span style=\"color: #b6cefa;\"> that the</span>, <span style=\"color: #98b9ff;\"> the country</span>, <span style=\"color: #d2dbe8;\"> country's</span>, <span style=\"color: #88abfd;\">'s Air</span>, <span style=\"color: #a9c6fd;\"> Air Self</span>, <span style=\"color: #cdd9ec;\"> Self Defense</span>, <span style=\"color: #ebd3c6;\"> Defense Force</span>, <span style=\"color: #cedaeb;\"> Force (</span>, <span style=\"color: #dedcdb;\"> (AS</span>, <span style=\"color: #f18d6f;\">ASDF</span>, <span style=\"color: #ccd9ed;\">DF)</span>, <span style=\"color: #d2dbe8;\">) had</span>, <span style=\"color: #f49a7b;\"> had never</span>, <span style=\"color: #97b8ff;\"> never encountered</span>, <span style=\"color: #3e51c5;\"> encountered an</span>, <span style=\"color: #f7b79b;\"> an extr</span>, <span style=\"color: #799cf8;\"> extrater</span>, <span style=\"color: #465ecf;\">aterrestrial</span>, <span style=\"color: #a3c2fe;\">restrial unidentified</span>, <span style=\"color: #e9785d;\"> unidentified flying</span>, <span style=\"color: #96b7ff;\"> flying object</span>, <span style=\"color: #445acc;\"> object.</span>, <span style=\"color: #e5d8d1;\">. Respond</span>, <span style=\"color: #f2cab5;\"> Responding</span>, <span style=\"color: #8db0fe;\">ing to</span>, <span style=\"color: #bed2f6;\"> to a</span>, <span style=\"color: #a2c1ff;\"> a query</span>, <span style=\"color: #5673e0;\"> query from</span>, <span style=\"color: #88abfd;\"> from flam</span>, <span style=\"color: #7597f6;\"> flamboy</span>, <span style=\"color: #b5cdfa;\">boyant</span>, <span style=\"color: #e3d9d3;\">ant former</span>, <span style=\"color: #c7d7f0;\"> former wrestler</span>, <span style=\"color: #ead4c8;\"> wrestler-turned</span>, <span style=\"color: #f7b89c;\">-turned-law</span>, <span style=\"color: #f6a283;\">-lawmaker</span>, <span style=\"color: #485fd1;\">maker Antonio</span>, <span style=\"color: #455cce;\"> Antonio In</span>, <span style=\"color: #afcafc;\"> Inoki</span>, <span style=\"color: #82a6fb;\">oki,</span>, <span style=\"color: #4961d2;\">, Defense</span>, <span style=\"color: #c3d5f4;\"> Defense Minister</span>, <span style=\"color: #b7cff9;\"> Minister Gen</span>, <span style=\"color: #cad8ef;\"> Gen Nak</span>, <span style=\"color: #f5c4ac;\"> Nakat</span>, <span style=\"color: #f0cdbb;\">atani</span>, <span style=\"color: #f5c4ac;\">ani told</span>, <span style=\"color: #f4c5ad;\"> told the</span>, <span style=\"color: #f1cdba;\"> the Diet</span>, <span style=\"color: #f0cdbb;\"> Diet,</span>, <span style=\"color: #ead4c8;\">, Japan</span>, <span style=\"color: #e3d9d3;\"> Japan's</span>, <span style=\"color: #f1ccb8;\">'s parliament</span>, <span style=\"color: #f6bda2;\"> parliament,</span>, <span style=\"color: #efcfbf;\">, that</span>, <span style=\"color: #ccd9ed;\"> that his</span>, <span style=\"color: #dfdbd9;\"> his jets</span>, <span style=\"color: #e7d7ce;\"> jets had</span>, <span style=\"color: #f5c0a7;\"> had,</span>, <span style=\"color: #e9d5cb;\">, to</span>, <span style=\"color: #d95847;\"> to date</span>, <span style=\"color: #e4d9d2;\"> date,</span>, <span style=\"color: #a6c4fe;\">, never</span>, <span style=\"color: #6e90f2;\"> never come</span>, <span style=\"color: #7396f5;\"> come across</span>, <span style=\"color: #5d7ce6;\"> across any</span>, <span style=\"color: #dcdddd;\"> any UFO</span>, <span style=\"color: #f7aa8c;\"> UFOs</span>, <span style=\"color: #9bbcff;\">s from</span>, <span style=\"color: #c3d5f4;\"> from outer</span>, <span style=\"color: #dbdcde;\"> outer space</span>, <span style=\"color: #cedaeb;\"> space.</span>, <span style=\"color: #84a7fc;\">. \"</span></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "def colorize_text_html(vals, colors):\n",
    "    colored_text = ', '.join(\n",
    "        f'<span style=\"color: {color};\">{tokenizer.decode(input_ids)}</span>'\n",
    "        for input_ids, color in zip(vals, colors)\n",
    "    )\n",
    "    return f'<p>{colored_text}</p>'\n",
    "\n",
    "colored_text_html = colorize_text_html(n_grams[47:141], colors[47:141])\n",
    "#colored_text_html = colorize_text_html(n_grams, colors)\n",
    "HTML(colored_text_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "43932170",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.lib.colors import HexColor, black\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.lib.units import inch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import ScalarMappable\n",
    "import numpy as np\n",
    "\n",
    "def create_colored_pdf(vals, colors, filename):\n",
    "    c = canvas.Canvas(filename, pagesize=letter)\n",
    "    width, height = letter\n",
    "\n",
    "    # Set starting position\n",
    "    x, y = 50, height - 50\n",
    "    line_height = 14\n",
    "\n",
    "    # Draw text with colors\n",
    "    for input_ids, color in zip(vals, colors):\n",
    "        word = tokenizer.decode(input_ids)\n",
    "        color = HexColor(color)\n",
    "\n",
    "        # Set color and draw text\n",
    "        c.setFillColor(color)\n",
    "        c.drawString(x, y, word)\n",
    "        x += c.stringWidth(word + ' ', 'Helvetica', 12)\n",
    "\n",
    "        # Move to next line if needed\n",
    "        if x > width - 80:\n",
    "            x = 50\n",
    "            y -= line_height\n",
    "\n",
    "    # Draw the color scale image in PDF\n",
    "    #c.drawImage(color_scale_image, scale_x, scale_y, width=scale_width, height=scale_height)\n",
    "    \n",
    "    c.save()\n",
    "\n",
    "# Create the PDF\n",
    "create_colored_pdf(n_grams[47:141], colors[47:141], \"2_gram_colored_text.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a237aee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = canvas.Canvas(\"colored_text.pdf\", pagesize=letter)\n",
    "\n",
    "# Draw color scale\n",
    "scale_width = 5 * inch\n",
    "scale_height = 0.5 * inch\n",
    "scale_x = 50\n",
    "scale_y = 50\n",
    "c.setFillColor(black)\n",
    "c.rect(scale_x, scale_y, scale_width, scale_height, fill=0)\n",
    "\n",
    "# Create color scale using matplotlib\n",
    "fig, ax = plt.subplots(figsize=(5, 0.5), dpi=80)\n",
    "fig.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "cmap = plt.get_cmap('coolwarm')\n",
    "norm = mcolors.Normalize(vmin=min_value, vmax=max_value)\n",
    "sm = ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])\n",
    "cb = plt.colorbar(sm, cax=ax, orientation='horizontal')\n",
    "cb.ax.tick_params(labelsize=10)\n",
    "#cb.ax.set_title('Value Scale', fontsize=10)\n",
    "#plt.show()\n",
    "# Save color scale to an image\n",
    "color_scale_image = \"color_scale.pdf\"\n",
    "plt.savefig(color_scale_image, bbox_inches='tight', pad_inches=0)\n",
    "plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b596499e6b756ee3ee734d514920d364bec1c6a0ebf031bda292a137927d8dcb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
