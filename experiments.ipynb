{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db139b29-c059-4b4f-8ad5-bf77274a15cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import torchmetrics\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, GenerationConfig\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "334efd5c-e16d-44ce-94de-ad6c7eccc031",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ee6c63a-ce6d-4610-93d9-16a57f847c21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparamters \n",
    "top_k = 50\n",
    "top_p = 0.9\n",
    "temp = 0.8\n",
    "min_new_tokens = 10\n",
    "max_new_tokens = 50\n",
    "do_sample=True\n",
    "num_beams=1\n",
    "\n",
    "dataset_name=\"PubMedQA\"\n",
    "model_name=\"facebook/opt-30b\"\n",
    "batch_size=8\n",
    "max_input_length=1024\n",
    "DEVICE = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "access_token = \"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\"\n",
    "cache_dir=\"/data/james/.cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc9a5a77-0973-48ac-8290-2e573d2bc654",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, metrics=None):\n",
    "        if not metrics:\n",
    "            metrics = [\"rouge\", \"sacre_bleu\", \"bertscore\", \"factkb\"]\n",
    "        self.metrics = metrics\n",
    "    \n",
    "    def evaluate(self, predictions, references, documents, metrics=[\"rouge\", \"bertscore\", \"factkb\", \"alignscore\"]):\n",
    "        result_dict = OrderedDict()\n",
    "        if \"rouge\" in metrics:\n",
    "            rouge_dict = self.calculate_rouge(predictions, references)\n",
    "            for k, v in rouge_dict.items():\n",
    "                result_dict[k] = v\n",
    "        if \"sacre_bleu\" in metrics:\n",
    "            sacre_bleu_dict = self.calculate_sacrebleu(predictions, references)\n",
    "            for k, v in sacre_bleu_dict.items():\n",
    "                result_dict[k] = v\n",
    "        if \"bertscore\" in metrics:\n",
    "            bertscore_dict = self.calculate_bertscore(predictions, references)\n",
    "            for k, v in bertscore_dict.items():\n",
    "                result_dict[k] = v\n",
    "        if \"factkb\" in metrics:\n",
    "            result_dict[\"factkb\"] = self.calculate_factkb(predictions, documents)\n",
    "            \n",
    "        if \"alignscore\" in metrics:\n",
    "            result_dict[\"alignscore\"] = self.calculate_alignscore(predictions, documents) \n",
    "\n",
    "        for k, v in result_dict.items():\n",
    "            print(f\"{k} -> {v*100:.2f}\")\n",
    "        return result_dict\n",
    "\n",
    "    def calculate_rouge(self, predictions, references):\n",
    "        from torchmetrics.functional.text.rouge import rouge_score\n",
    "        rouge_dict = rouge_score(preds=predictions, target=references)\n",
    "        return {k: v.item() for k, v in rouge_dict.items()}\n",
    "\n",
    "    def calculate_sacrebleu(self, predictions, references):\n",
    "        from torchmetrics.functional.text import sacre_bleu_score\n",
    "        score = sacre_bleu_score(preds=predictions, target=[[i] for i in references])\n",
    "        return {\"sacre_bleu\": score.item()}\n",
    "\n",
    "    def calculate_bertscore(self, predictions, references):\n",
    "        import evaluate\n",
    "        bertscore = evaluate.load(\"bertscore\")\n",
    "        bertscore_dict = bertscore.compute(predictions=predictions, references=references, model_type=\"roberta-large-mnli\")\n",
    "        res = {\"bertscore_precision\": np.mean(bertscore_dict[\"precision\"]), \"bertscore_recall\": np.mean(bertscore_dict[\"recall\"]), \"bertscore_f1\": np.mean(bertscore_dict[\"f1\"])}\n",
    "        return {k: v.item() for k, v in res.items()}\n",
    "    \n",
    "    def calculate_alignscore(self, predictions, documents):\n",
    "        from AlignScore.src.alignscore import AlignScore\n",
    "        ckpt_path = \"/mlx_devbox/users/james.flemings/privacy_hallucination_llm/models/AlignScore-base.ckpt\"\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        align_scorer = AlignScore(model='roberta-base', batch_size=8, device=device, ckpt_path=ckpt_path, evaluation_mode='nli_sp')\n",
    "        alignscore_result = align_scorer.score(contexts=documents, claims=predictions)\n",
    "        #total_result['AlignScore'] = 100*np.mean(alignscore_result)\n",
    "        return np.mean(alignscore_result)\n",
    "\n",
    "    def calculate_factkb(self, predictions, documents):\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\", padding=\"max_length\", truncation=True, cache_dir=cache_dir)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\"bunsenfeng/FactKB\", torch_dtype=torch.float16, cache_dir=cache_dir)\n",
    "        model = model.to(device)\n",
    "        res = []\n",
    "        for i in range(len(predictions)):\n",
    "            input_pretokenized = f\"{predictions[i]} {tokenizer.sep_token} {documents[i]}\"\n",
    "            tokenized_input = tokenizer(input_pretokenized, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "            with torch.no_grad():\n",
    "                output = model(input_ids=tokenized_input.input_ids.to(device))\n",
    "            logits = torch.softmax(output.logits, dim=1)  # (bz, 2)\n",
    "            res.append(logits.squeeze()[-1].item())\n",
    "        return np.mean(res)\n",
    "    \n",
    "    def calculate_factcc(self, predictions, documents):\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        model_path = 'manueldeprada/FactCC'\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "        model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "        device = torch.device(f'cuda:0')\n",
    "        model.to(device)\n",
    "        res = []\n",
    "        for i in range(len(predictions)):\n",
    "            input_pretokenized = f\"{predictions[i]} {tokenizer.sep_token} {documents[i]}\"\n",
    "            input_dict = tokenizer(documents[i], predictions[i], max_length=512, padding='max_length', truncation='only_first', return_tensors='pt').to(device)\n",
    "            #tokenized_input = tokenizer(input_pretokenized, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "            with torch.no_grad():\n",
    "                output = model(**input_dict)\n",
    "            pred = output.logits.argmax(dim=1)\n",
    "            res.append(pred.item())\n",
    "        return (1-np.mean(res))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e6f9c07-b5b5-461d-adb1-00580c2aa643",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "def xsum_pretokenize(dataset, tokenizer, max_input_length):\n",
    "    data = {\"context\": [], \"query\": [], \"summary\": []}\n",
    "    for i, row in tqdm(enumerate(dataset), desc=\"truncating documents...\"):\n",
    "        trunc_doc = tokenizer.batch_decode(tokenizer(row['document'], return_tensors=\"pt\", max_length=max_input_length,  truncation=True).input_ids, skip_special_tokens=True)[0]\n",
    "        data['context'].append(trunc_doc)\n",
    "        data['summary'].append(row['summary'])\n",
    "        data[\"query\"].append(\"Summarize the article in one sentence. Summary:\")\n",
    "    return Dataset.from_dict(data)\n",
    "\n",
    "def cnn_pretokenize(dataset, tokenizer, max_input_length):\n",
    "    data = {\"context\": [], \"query\": [], \"summary\": []}\n",
    "    for i, row in tqdm(enumerate(dataset), desc=\"truncating documents...\"):\n",
    "        trunc_doc = tokenizer.batch_decode(tokenizer(row['article'], return_tensors=\"pt\", max_length=max_input_length,  truncation=True).input_ids, skip_special_tokens=True)[0]\n",
    "        data['context'].append(trunc_doc)\n",
    "        data['summary'].append(row['highlights'])\n",
    "        data['query'].append(\"Summarize the article in one sentence. Summary:\")\n",
    "    return Dataset.from_dict(data)\n",
    "\n",
    "def pubmedqa_pretokenize(dataset, tokenizer, max_input_length):\n",
    "    data = {\"context\": [], \"query\": [], \"summary\": []}\n",
    "    for i, row in tqdm(enumerate(dataset), desc=\"truncating documents...\"):\n",
    "        context= ''.join(c for c in row['context']['contexts'])\n",
    "        trunc_doc = tokenizer.batch_decode(tokenizer(context, return_tensors=\"pt\", max_length=max_input_length, truncation=True).input_ids, skip_special_tokens=True)[0]\n",
    "        data['context'].append(trunc_doc)\n",
    "        data['summary'].append(row['long_answer'])\n",
    "        data['query'].append(f\"Question: {row['question']}. Answer:\")\n",
    "    return Dataset.from_dict(data)\n",
    "\n",
    "def pretokenize(dataset_name, dataset, tokenizer, max_input_length):\n",
    "    if dataset_name == \"xsum\":\n",
    "        return xsum_pretokenize(dataset, tokenizer, max_input_length)\n",
    "    elif dataset_name == \"cnn\":\n",
    "        return cnn_pretokenize(dataset, tokenizer, max_input_length)\n",
    "    elif dataset_name == \"PubMedQA\":\n",
    "        return pubmedqa_pretokenize(dataset, tokenizer, max_input_length)\n",
    "    return None\n",
    "\n",
    "def template_input(row, dataset):\n",
    "    if dataset == \"xsum\" or dataset == \"cnn\":\n",
    "        return f\"Article: {row['context']}. {row['query']}\"\n",
    "    elif dataset == \"PubMedQA\":\n",
    "        return f\"Document: {row['context']}. {row['query']}\"\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def template_empty_input(row, dataset):\n",
    "    if dataset == \"xsum\" or dataset == \"cnn\":\n",
    "        return f\"Article: . {row['query']}\"\n",
    "    elif dataset == \"PubMedQA\":\n",
    "        return f\"Document: . {row['query']}\"\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c53db7de-5b48-46ee-b2a5-a421e0462638",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
    "                                          padding_side=\"left\",\n",
    "                                          use_fast=False,\n",
    "                                          token=access_token,\n",
    "                                          trust_remote_code=True,\n",
    "                                          cache_dir=cache_dir)\n",
    "if tokenizer.pad_token is None:\n",
    "    print(\"True\")\n",
    "    tokenizer.pad_token, tokenizer.pad_token_id = tokenizer.eos_token, tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d380e0c8-bdd3-4b8c-9bcf-29cd00f9b5c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if dataset_name == \"PubMedQA\":\n",
    "    raw_test_set = load_dataset(\"qiaojin/PubMedQA\", \"pqa_labeled\", cache_dir=cache_dir)['train']\n",
    "elif dataset_name == 'xsum':\n",
    "    raw_test_set = load_dataset(dataset_name, split=\"test[:1000]\")\n",
    "elif dataset_name == 'cnn':\n",
    "    raw_test_set = load_dataset(\"abisee/cnn_dailymail\", \"3.0.0\", split=\"test[:1000]\", cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a68f6806-7061-4085-887c-09371a45f127",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "truncating documents...: 1000it [00:06, 163.86it/s]\n"
     ]
    }
   ],
   "source": [
    "test_set = pretokenize(dataset_name, raw_test_set, tokenizer, max_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02695206-4fe0-416f-b28c-24f5c57f1ac6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code for Pure DP decoding \n",
    "import torch.nn.functional as F\n",
    "from scipy.optimize import bisect\n",
    "    \n",
    "def top_k_top_p_filtering(logits, top_k, top_p, filter_value=-float(\"Inf\")):\n",
    "    indicies_to_remove = 0\n",
    "    if top_k > 0:\n",
    "        #  Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    \n",
    "    if top_p < 1.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold (token with 0 are kept)\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "        # scatter sorted tensors to original indexing\n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    \n",
    "    return logits, indices_to_remove\n",
    "    \n",
    "def renyiDiv(p, q, alpha=float('inf')):\n",
    "        if alpha == float('inf'):\n",
    "            RD = torch.log(torch.max(p/q))\n",
    "        elif alpha == 1:\n",
    "            RD = torch.sum(p*torch.log(p/q))\n",
    "        else:\n",
    "            RD = 1/(alpha-1) * torch.log(\n",
    "                torch.sum(((p/q)**(alpha))*q)\n",
    "            )\n",
    "        if torch.isnan(RD):\n",
    "            RD = torch.log(torch.max(p/q))\n",
    "        return RD\n",
    "    \n",
    "def renyi_priv_loss(p, q, alpha):\n",
    "    return max(renyiDiv(p, q, alpha=alpha), renyiDiv(q, p, alpha=alpha)).cpu().numpy()\n",
    "\n",
    "def calculate_memorization(p, q, idx):\n",
    "    return max(torch.log(p[idx]/q[idx]), torch.log(q[idx]/p[idx])).cpu().numpy()  \n",
    "\n",
    "def entropy(p):\n",
    "    return (-np.sum(p*np.log(p)))\n",
    "\n",
    "def lambda_solver_bisection(p, p_0, epsilon, alpha):\n",
    "    def f(lambd):\n",
    "        pred = lambd * p + (1-lambd) * p_0\n",
    "        #eps = np.max([np.max(np.log(pred/p_0)), np.max(np.log(p_0/pred))])\n",
    "        eps = max(renyiDiv(pred, p_0, alpha=alpha), renyiDiv(p_0, pred, alpha=alpha))\n",
    "        return (eps - epsilon)\n",
    "    if f(1) <= 0.0:\n",
    "        lambd = 1\n",
    "    else:\n",
    "        lambd = bisect(f, 0, 1, maxiter=20, disp=False)\n",
    "    return lambd\n",
    "\n",
    "def lambda_solver(p, p_0, epsilon):\n",
    "    a = (p_0 * (np.exp(epsilon/2) - 1)) / torch.abs(p - p_0)\n",
    "    val = torch.min(a)\n",
    "    return min(1, val)\n",
    "\n",
    "def mollify(p, p_0, epsilon, ids, alpha):\n",
    "    #lambd = lambda_solver(p[ids], p_0[ids], epsilon)\n",
    "    lambd = lambda_solver_bisection(p[ids].cpu(), p_0[ids].cpu(), epsilon, alpha)\n",
    "    return (lambd * p + (1-lambd) * p_0), lambd    \n",
    "\n",
    "def calc_partition_loss(proj_logit, proj_output, pub_output, alpha, temperature):\n",
    "    max_loss = 0\n",
    "    for i in range(proj_logit.shape[0]):\n",
    "        proj_logit_i = torch.cat([proj_logit[:i, :], proj_logit[i+1:, :]])\n",
    "        proj_output_i = F.softmax(proj_logit_i / temperature, dim=-1).mean(dim=0)\n",
    "        ids = torch.nonzero(proj_output)\n",
    "        eps = renyi_priv_loss(proj_output[ids], proj_output_i[ids], alpha)\n",
    "        max_loss = max(max_loss, eps)\n",
    "    return max_loss\n",
    "\n",
    "def calc_group_memorization(ensemble_outputs, idx):\n",
    "    return max([calculate_memorization(ensemble_outputs[0, :], ensemble_outputs[i, :], idx) for i in range(1, ensemble_outputs.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cc4cab5-6d85-4bda-8639-b2079f259a9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def post_calc_memorization(model,\n",
    "                   context_aware_input_ids,\n",
    "                   context_unaware_input_ids,\n",
    "                   response_input_ids,\n",
    "                   lambd,\n",
    "                   temperature,\n",
    "                   stop_token_ids,\n",
    "                   min_length,\n",
    "                   batch_size=None\n",
    "                  ):\n",
    "    priv_loss_total = 0\n",
    "    N = context_aware_input_ids.shape[0]\n",
    "    for t in range(response_input_ids.shape[1]):\n",
    "        priv_context_aware_input_ids = torch.cat([context_aware_input_ids,\n",
    "                                      response_input_ids[:, :t].repeat(N, 1)],\n",
    "                                     dim=1)\n",
    "        pub_logit = model(torch.cat([context_unaware_input_ids,\n",
    "                                     response_input_ids[:, :t]],\n",
    "                                    dim=1)\n",
    "                         ).logits.squeeze()[-1, :].type(torch.float64)\n",
    "        if batch_size == None:\n",
    "            priv_logit = model(priv_context_aware_input_ids).logits[:, -1, :].type(torch.float64)\n",
    "        else:\n",
    "            priv_logit = torch.stack([model(priv_context_aware_input_ids[i:(i+1)*batch_size]).logits[:, -1, :].type(torch.float64)\n",
    "                     for i in range(0, N, batch_size)])\n",
    "        proj_logit = lambd * priv_logit + (1-lambd) * pub_logit.repeat(N, 1)\n",
    "        \n",
    "        if t < min_length:\n",
    "            pub_logit[stop_token_ids[0]] = -float(\"Inf\")\n",
    "            proj_logit[:, stop_token_ids[0]] = -float(\"Inf\")\n",
    "            \n",
    "        if pub_logit.shape[0] > len(tokenizer):\n",
    "            pub_logit[len(tokenizer):pub_logit.shape[0]] = -float(\"Inf\")\n",
    "            proj_logit[:, len(tokenizer):pub_logit.shape[0]] = -float(\"Inf\")\n",
    "        \n",
    "        pub_output = F.softmax(pub_logit / temperature, dim=-1)\n",
    "        priv_output = F.softmax(priv_logit / temperature, dim=-1)\n",
    "        proj_output = F.softmax(proj_logit / temperature, dim=-1)\n",
    "        \n",
    "        ids = torch.nonzero(pub_output)\n",
    "        priv_loss = calc_group_memorization(proj_output[:, ids].squeeze(), response_input_ids[:, t])\n",
    "        priv_loss_total += priv_loss\n",
    "        \n",
    "    return priv_loss_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dfb0b43-28b3-4b55-89ef-e31a9c502360",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def partition(data, tokenizer, partition_length, dataset_name):\n",
    "    document_ids = tokenizer(data['context']).input_ids\n",
    "    ensemble = []\n",
    "    for i in range(0, len(document_ids), partition_length):\n",
    "        idx = (i+partition_length)\n",
    "        #ensemble = torch.cat([ensemble, input_ids[-1:, idx:i]], dim=1)\n",
    "        row = {'context': tokenizer.decode(document_ids[i:idx], skip_special_tokens=True), 'query': data['query']}\n",
    "        ensemble.append(template_input(row, dataset_name))\n",
    "    return ensemble\n",
    "\n",
    "def group_partition(data, tokenizer, partition_length, dataset_name):\n",
    "    document_ids = tokenizer(data['context']).input_ids\n",
    "    groups = [template_input(data, dataset_name)]\n",
    "    for i in range(0, len(document_ids), partition_length):\n",
    "        idx = (i+partition_length)\n",
    "        group_i = document_ids[:i] + document_ids[idx:]\n",
    "        row = {'context': tokenizer.decode(group_i, skip_special_tokens=True), 'query': data['query']}\n",
    "        groups.append(template_input(row, dataset_name))\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23fde59f-b6a9-4439-bdfa-0c0601e14a95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cmad_generation(model,\n",
    "                  context_aware_input_ids,\n",
    "                  context_unaware_input_ids,\n",
    "                  lambd,\n",
    "                  temperature,\n",
    "                  max_length,\n",
    "                  min_length,\n",
    "                  stop_token_ids,\n",
    "                  device,\n",
    "                 ):\n",
    "    response_input_ids = torch.LongTensor([[]]).to(device)\n",
    "    for i in range(max_length):\n",
    "        priv_context_aware_input_ids = torch.cat([context_aware_input_ids,\n",
    "                                      response_input_ids.repeat(context_aware_input_ids.shape[0], 1)],\n",
    "                                     dim=1)\n",
    "        pub_logit = model(torch.cat([context_unaware_input_ids,\n",
    "                                     response_input_ids],\n",
    "                                    dim=1)\n",
    "                         ).logits.squeeze()[-1, :].type(torch.float64)\n",
    "\n",
    "        priv_logit = model(priv_context_aware_input_ids).logits[:, -1, :].type(torch.float64)\n",
    "        proj_logit = lambd * priv_logit + (1-lambd) * pub_logit.repeat(priv_logit.shape[0], 1)\n",
    "        \n",
    "        if i < min_length:\n",
    "            pub_logit[stop_token_ids[0]] = -float(\"Inf\")\n",
    "            proj_logit[:, stop_token_ids[0]] = -float(\"Inf\")\n",
    "            \n",
    "        if pub_logit.shape[0] > len(tokenizer):\n",
    "            pub_logit[len(tokenizer):pub_logit.shape[0]] = -float(\"Inf\")\n",
    "            proj_logit[:, len(tokenizer):pub_logit.shape[0]] = -float(\"Inf\")\n",
    "            \n",
    "        pub_output = F.softmax(pub_logit / temperature, dim=-1)\n",
    "        #priv_output = F.softmax(priv_logit, dim=-1)[-1]\n",
    "        proj_output = F.softmax(proj_logit / temperature, dim=-1)\n",
    "\n",
    "        pred_idx = proj_output[0].multinomial(1).view(1, -1).long()\n",
    "        if pred_idx.cpu()[0].item() in stop_token_ids:\n",
    "            break\n",
    "\n",
    "        response_input_ids = torch.cat([response_input_ids, pred_idx], dim=1)\n",
    "        del pred_idx\n",
    "    return response_input_ids.cpu()[0], 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9b8aa8a-0b65-4182-ba87-d7c7ba3d0441",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decode_experiment(test_set, model, tokenizer, lambd, temperature, dataset_name, min_length):\n",
    "    dp_predictions = []\n",
    "    stop_token_ids = [tokenizer.eos_token_id,\n",
    "                      tokenizer.pad_token_id,\n",
    "                     ]\n",
    "    doc_priv_loss = [] \n",
    "    for idx, data in tqdm(enumerate(test_set), total=len(test_set)):\n",
    "        context_unaware_tokenized_input = tokenizer(template_empty_input(data, dataset_name), return_tensors=\"pt\", padding=True)\n",
    "        context_aware_tokenized_input = tokenizer(template_input(data, dataset_name), return_tensors=\"pt\", padding=True)\n",
    "        with torch.no_grad():\n",
    "            dp_output, doc_eps = cmad_generation(model,\n",
    "                                    context_aware_tokenized_input.input_ids.to(DEVICE),\n",
    "                                    context_unaware_tokenized_input.input_ids.to(DEVICE),\n",
    "                                    lambd=lambd,\n",
    "                                    temperature=temperature,\n",
    "                                    max_length=max_new_tokens,\n",
    "                                    min_length=min_length,\n",
    "                                    stop_token_ids=stop_token_ids,\n",
    "                                    device=DEVICE,\n",
    "                                    )\n",
    "        decode_dp_output = tokenizer.decode(dp_output, skip_special_tokens=True)\n",
    "        dp_predictions.append(decode_dp_output)\n",
    "        doc_priv_loss.append(doc_eps)\n",
    "    return dp_predictions, doc_priv_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c430e6b-3217-4812-9184-e007c911433c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dir_name = \"results\"\n",
    "m_name = \"opt-30b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "295cf787-4c58-4b3c-84b1-778d8080ea15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d6081fd66c648b48786cb436d014b03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "truncating documents...: 1000it [00:06, 165.85it/s]\n",
      "  0%|          | 4/1000 [00:51<3:27:40, 12.51s/it]"
     ]
    }
   ],
   "source": [
    "os.makedirs(dir_name, exist_ok=True)\n",
    "lambds = [1.0]\n",
    "model_names = [\"facebook/opt-30b\"]\n",
    "m_names = [\"opt-30b\"]\n",
    "for model_name, m_name in zip(model_names, m_names):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
    "                                          #padding_side=\"left\",\n",
    "                                          use_fast=False,\n",
    "                                          token=access_token,\n",
    "                                          trust_remote_code=True,\n",
    "                                          cache_dir=cache_dir)\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        print(\"True\")\n",
    "        tokenizer.pad_token, tokenizer.pad_token_id = tokenizer.eos_token, tokenizer.eos_token_id\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        token=access_token,\n",
    "        cache_dir=cache_dir,\n",
    "        device_map=\"auto\"\n",
    "        )#.to(DEVICE)\n",
    "    \n",
    "    test_set = pretokenize(dataset_name, raw_test_set, tokenizer, max_input_length)\n",
    "    \n",
    "    for lambd in lambds:\n",
    "        file_name = f'{dataset_name}_{m_name}_{lambd}.csv'\n",
    "        dp_predictions, dp_loss = decode_experiment(test_set, model, tokenizer, lambd=lambd, temperature=0.8, dataset_name=dataset_name, min_length=10)\n",
    "        df = pd.DataFrame({'generations': dp_predictions, 'privacy_loss': dp_loss})\n",
    "        df.to_csv(os.path.join(dir_name, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0dbd69e2-a5f1-4aa4-81c2-c95fdd69e550",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 17295.74it/s]\n"
     ]
    }
   ],
   "source": [
    "documents, references = [], []\n",
    "for idx, data in tqdm(enumerate(test_set), total=len(test_set)):\n",
    "    documents.append(data['context'])\n",
    "    references.append(data['summary'])\n",
    "evaluator = Evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b75a7ada-e105-4d13-9cb5-d14ee76356f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lambd=1.0\n",
    "file_name = f'{dataset_name}_{m_name}_{lambd}.csv'\n",
    "df = pd.read_csv(os.path.join(dir_name, file_name))\n",
    "doc_priv_loss = df['privacy_loss']\n",
    "predictions = df['generations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f37390a-c4c7-4eda-aace-e6c95972146f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d92c46674d44e3787c0b52407ee381b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        token=access_token,\n",
    "        cache_dir=cache_dir\n",
    "        ).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3a1c2db-714d-472e-abd1-8a466c7f168d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216.78631168897297"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_priv_loss[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "38a3dd6c-11e3-4829-966c-4b43da6d9088",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [1:51:08<00:00,  6.67s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.13088374592178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# importing module\n",
    "import logging\n",
    "\n",
    "partition_len = 1024\n",
    "temperature=0.8\n",
    "min_length=10\n",
    "stop_token_ids = [tokenizer.eos_token_id,\n",
    "                      tokenizer.pad_token_id,\n",
    "                     ]\n",
    "query_set = test_set.select(range(1000))\n",
    "\n",
    "lambds = [1.0]\n",
    "mean_vals = []\n",
    "for lambd in lambds:\n",
    "    file_name = f'{dataset_name}_{m_name}_{lambd}.csv'\n",
    "    df = pd.read_csv(os.path.join(dir_name, file_name))\n",
    "    predictions = df['generations']\n",
    "    mem_vals = []\n",
    "    for data, response in tqdm(zip(query_set, predictions), total=len(query_set)):\n",
    "        context_unaware_tokenized_input = tokenizer(template_empty_input(data, dataset_name), return_tensors=\"pt\", padding=True)\n",
    "        ensemble = group_partition(data, tokenizer, partition_len, dataset_name=dataset_name)\n",
    "        context_aware_tokenized_input = tokenizer(ensemble, return_tensors=\"pt\", max_length=max_input_length+25, padding=True, truncation=True)\n",
    "        response_tokenized_input = tokenizer(response, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            cur_mem = post_calc_memorization(model,\n",
    "                                       context_aware_tokenized_input.input_ids.to(DEVICE),\n",
    "                                       context_unaware_tokenized_input.input_ids.to(DEVICE),\n",
    "                                       response_tokenized_input.input_ids.to(DEVICE)[:, 1:],\n",
    "                                       lambd,\n",
    "                                       temperature,\n",
    "                                       stop_token_ids,\n",
    "                                       min_length,\n",
    "                                       batch_size=None\n",
    "                                      )\n",
    "        mem_vals.append(cur_mem)\n",
    "    mean_vals.append(np.mean(mem_vals))\n",
    "    print(np.mean(mem_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60513537-af2f-4169-8e1c-b9a985ecaffc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean_vals' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mean_vals\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mean_vals' is not defined"
     ]
    }
   ],
   "source": [
    "mean_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f426ec09-c9a8-4db5-8ffc-91f0972ca151",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db1b7c24ca674e0ca1fa34e132b84fad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  16%|#6        | 231M/1.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c30f496b13c44148a87e7056a2563a5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "406c8a4238244b07a8b6d03ba67abda4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f398cd1d0cc64dd383949f6430927b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ed03af3731c4f3e9f980eb5d73aa07e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d2aec1cc0a04af7b2cca1899858684d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac19d70813b14d45b88f8b3c1aa4e640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/759 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a562988fb534b05bb661cf2d4485774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c832b9999644cd8036d7678103fa1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/174 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fff962ee2b540d49e1bfb3bd1fa23c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b032315f21b48febfe2279480fb8881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9d7a8e89aac4706a956e7eb3f5de017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/854 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b7dc0bff7e436dbbfd62dc5f9c9144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1_fmeasure -> 23.88\n",
      "rouge1_precision -> 27.18\n",
      "rouge1_recall -> 23.92\n",
      "rouge2_fmeasure -> 6.23\n",
      "rouge2_precision -> 7.12\n",
      "rouge2_recall -> 6.35\n",
      "rougeL_fmeasure -> 16.75\n",
      "rougeL_precision -> 19.09\n",
      "rougeL_recall -> 16.90\n",
      "rougeLsum_fmeasure -> 19.65\n",
      "rougeLsum_precision -> 22.49\n",
      "rougeLsum_recall -> 19.68\n",
      "bertscore_precision -> 72.86\n",
      "bertscore_recall -> 72.72\n",
      "bertscore_f1 -> 72.75\n",
      "factkb -> 40.28\n",
      "factcc -> 46.30\n"
     ]
    }
   ],
   "source": [
    "result_dict = evaluator.evaluate(predictions, references, documents, metrics=[\"rouge\", \"bertscore\", \"factkb\", \"factcc\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b596499e6b756ee3ee734d514920d364bec1c6a0ebf031bda292a137927d8dcb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
