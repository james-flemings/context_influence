{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db139b29-c059-4b4f-8ad5-bf77274a15cc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import torchmetrics\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, GenerationConfig\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "334efd5c-e16d-44ce-94de-ad6c7eccc031",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ee6c63a-ce6d-4610-93d9-16a57f847c21",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparamters \n",
    "top_k = 50\n",
    "top_p = 0.9\n",
    "temp = 0.8\n",
    "min_new_tokens = 10\n",
    "max_new_tokens = 50\n",
    "do_sample=True\n",
    "num_beams=1\n",
    "\n",
    "dataset_name=\"cnn\"\n",
    "model_name= \"meta-llama/Meta-Llama-3-8B\"\n",
    "batch_size=8\n",
    "max_input_length=1024\n",
    "DEVICE = \"cuda:2\" if torch.cuda.is_available() else \"cpu\"\n",
    "access_token = \"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\"\n",
    "cache_dir=\"/data/james/.cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc9a5a77-0973-48ac-8290-2e573d2bc654",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, metrics=None):\n",
    "        if not metrics:\n",
    "            metrics = [\"rouge\", \"sacre_bleu\", \"bertscore\", \"factkb\"]\n",
    "        self.metrics = metrics\n",
    "    \n",
    "    def evaluate(self, predictions, references, documents, metrics=[\"rouge\", \"bertscore\", \"factkb\", \"alignscore\"]):\n",
    "        result_dict = OrderedDict()\n",
    "        if \"rouge\" in metrics:\n",
    "            rouge_dict = self.calculate_rouge(predictions, references)\n",
    "            for k, v in rouge_dict.items():\n",
    "                result_dict[k] = v\n",
    "        if \"sacre_bleu\" in metrics:\n",
    "            sacre_bleu_dict = self.calculate_sacrebleu(predictions, references)\n",
    "            for k, v in sacre_bleu_dict.items():\n",
    "                result_dict[k] = v\n",
    "        if \"bertscore\" in metrics:\n",
    "            bertscore_dict = self.calculate_bertscore(predictions, references)\n",
    "            for k, v in bertscore_dict.items():\n",
    "                result_dict[k] = v\n",
    "        if \"factkb\" in metrics:\n",
    "            result_dict[\"factkb\"] = self.calculate_factkb(predictions, documents)\n",
    "            \n",
    "        if \"alignscore\" in metrics:\n",
    "            result_dict[\"alignscore\"] = self.calculate_alignscore(predictions, documents) \n",
    "\n",
    "        for k, v in result_dict.items():\n",
    "            print(f\"{k} -> {v*100:.2f}\")\n",
    "        return result_dict\n",
    "\n",
    "    def calculate_rouge(self, predictions, references):\n",
    "        from torchmetrics.functional.text.rouge import rouge_score\n",
    "        rouge_dict = rouge_score(preds=predictions, target=references)\n",
    "        return {k: v.item() for k, v in rouge_dict.items()}\n",
    "\n",
    "    def calculate_sacrebleu(self, predictions, references):\n",
    "        from torchmetrics.functional.text import sacre_bleu_score\n",
    "        score = sacre_bleu_score(preds=predictions, target=[[i] for i in references])\n",
    "        return {\"sacre_bleu\": score.item()}\n",
    "\n",
    "    def calculate_bertscore(self, predictions, references):\n",
    "        import evaluate\n",
    "        bertscore = evaluate.load(\"bertscore\")\n",
    "        bertscore_dict = bertscore.compute(predictions=predictions, references=references, model_type=\"roberta-large-mnli\")\n",
    "        res = {\"bertscore_precision\": np.mean(bertscore_dict[\"precision\"]), \"bertscore_recall\": np.mean(bertscore_dict[\"recall\"]), \"bertscore_f1\": np.mean(bertscore_dict[\"f1\"])}\n",
    "        return {k: v.item() for k, v in res.items()}\n",
    "    \n",
    "    def calculate_alignscore(self, predictions, documents):\n",
    "        from AlignScore.src.alignscore import AlignScore\n",
    "        ckpt_path = \"models/AlignScore-base.ckpt\"\n",
    "        align_scorer = AlignScore(model='roberta-base', batch_size=8, device=DEVICE, ckpt_path=ckpt_path, evaluation_mode='nli_sp')\n",
    "        alignscore_result = align_scorer.score(contexts=documents, claims=predictions)\n",
    "        #total_result['AlignScore'] = 100*np.mean(alignscore_result)\n",
    "        return np.mean(alignscore_result)\n",
    "\n",
    "    def calculate_factkb(self, predictions, documents):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\", padding=\"max_length\", truncation=True, cache_dir=cache_dir)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\"bunsenfeng/FactKB\", torch_dtype=torch.float16, cache_dir=cache_dir)\n",
    "        model = model.to(DEVICE)\n",
    "        res = []\n",
    "        for i in range(len(predictions)):\n",
    "            input_pretokenized = f\"{predictions[i]} {tokenizer.sep_token} {documents[i]}\"\n",
    "            tokenized_input = tokenizer(input_pretokenized, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "            with torch.no_grad():\n",
    "                output = model(input_ids=tokenized_input.input_ids.to(DEVICE))\n",
    "            logits = torch.softmax(output.logits, dim=1)  # (bz, 2)\n",
    "            res.append(logits.squeeze()[-1].item())\n",
    "        return np.mean(res)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e6f9c07-b5b5-461d-adb1-00580c2aa643",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "def xsum_pretokenize(dataset, tokenizer, max_input_length):\n",
    "    data = {\"context\": [], \"query\": [], \"summary\": []}\n",
    "    for i, row in tqdm(enumerate(dataset), desc=\"truncating documents...\"):\n",
    "        trunc_doc = tokenizer.batch_decode(tokenizer(row['document'], return_tensors=\"pt\", max_length=max_input_length,  truncation=True).input_ids, skip_special_tokens=True)[0]\n",
    "        data['context'].append(trunc_doc)\n",
    "        data['summary'].append(row['summary'])\n",
    "        data[\"query\"].append(\"Summarize the article in one sentence. Summary:\")\n",
    "    return Dataset.from_dict(data)\n",
    "\n",
    "def cnn_pretokenize(dataset, tokenizer, max_input_length):\n",
    "    data = {\"context\": [], \"query\": [], \"summary\": []}\n",
    "    for i, row in tqdm(enumerate(dataset), desc=\"truncating documents...\"):\n",
    "        trunc_doc = tokenizer.batch_decode(tokenizer(row['article'], return_tensors=\"pt\", max_length=max_input_length,  truncation=True).input_ids, skip_special_tokens=True)[0]\n",
    "        data['context'].append(trunc_doc)\n",
    "        data['summary'].append(row['highlights'])\n",
    "        data['query'].append(\"Summary of the above news article:\")\n",
    "    return Dataset.from_dict(data)\n",
    "\n",
    "def pubmedqa_pretokenize(dataset, tokenizer, max_input_length):\n",
    "    data = {\"context\": [], \"query\": [], \"summary\": []}\n",
    "    for i, row in tqdm(enumerate(dataset), desc=\"truncating documents...\"):\n",
    "        context= ''.join(c for c in row['context']['contexts'])\n",
    "        trunc_doc = tokenizer.batch_decode(tokenizer(context, return_tensors=\"pt\", max_length=max_input_length, truncation=True).input_ids, skip_special_tokens=True)[0]\n",
    "        data['context'].append(trunc_doc)\n",
    "        data['summary'].append(row['long_answer'])\n",
    "        data['query'].append(f\"Question: {row['question']}. Answer:\")\n",
    "    return Dataset.from_dict(data)\n",
    "\n",
    "def pretokenize(dataset_name, dataset, tokenizer, max_input_length):\n",
    "    if dataset_name == \"xsum\":\n",
    "        return xsum_pretokenize(dataset, tokenizer, max_input_length)\n",
    "    elif dataset_name == \"cnn\":\n",
    "        return cnn_pretokenize(dataset, tokenizer, max_input_length)\n",
    "    elif dataset_name == \"PubMedQA\":\n",
    "        return pubmedqa_pretokenize(dataset, tokenizer, max_input_length)\n",
    "    return None\n",
    "\n",
    "def template_input(row, dataset):\n",
    "    if dataset == \"xsum\" or dataset == \"cnn\":\n",
    "        return f\"Article: {row['context']}. {row['query']}\"\n",
    "    elif dataset == \"PubMedQA\":\n",
    "        return f\"Document: {row['context']}. {row['query']}\"\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def template_empty_input(row, dataset):\n",
    "    if dataset == \"xsum\" or dataset == \"cnn\":\n",
    "        return f\"Article: . {row['query']}\"\n",
    "    elif dataset == \"PubMedQA\":\n",
    "        return f\"Document: . {row['query']}\"\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c53db7de-5b48-46ee-b2a5-a421e0462638",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
    "                                          padding_side=\"left\",\n",
    "                                          use_fast=False,\n",
    "                                          token=access_token,\n",
    "                                          trust_remote_code=True,\n",
    "                                          cache_dir=cache_dir)\n",
    "if tokenizer.pad_token is None:\n",
    "    print(\"True\")\n",
    "    tokenizer.pad_token, tokenizer.pad_token_id = tokenizer.eos_token, tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d380e0c8-bdd3-4b8c-9bcf-29cd00f9b5c6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if dataset_name == \"PubMedQA\":\n",
    "    raw_test_set = load_dataset(\"qiaojin/PubMedQA\", \"pqa_labeled\", cache_dir=cache_dir)['train']\n",
    "elif dataset_name == 'xsum':\n",
    "    raw_test_set = load_dataset(dataset_name, split=\"test[:1000]\")\n",
    "elif dataset_name == 'cnn':\n",
    "    raw_test_set = load_dataset(\"abisee/cnn_dailymail\", \"3.0.0\", split=\"test[:1000]\", cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a68f6806-7061-4085-887c-09371a45f127",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "truncating documents...: 1000it [00:02, 461.16it/s]\n"
     ]
    }
   ],
   "source": [
    "test_set = pretokenize(dataset_name, raw_test_set, tokenizer, max_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02695206-4fe0-416f-b28c-24f5c57f1ac6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code for Pure DP decoding \n",
    "import torch.nn.functional as F\n",
    "from scipy.optimize import bisect\n",
    "    \n",
    "def top_k_top_p_filtering(logits, top_k, top_p, filter_value=-float(\"Inf\")):\n",
    "    indicies_to_remove = 0\n",
    "    if top_k > 0:\n",
    "        #  Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    \n",
    "    if top_p < 1.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold (token with 0 are kept)\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "        # scatter sorted tensors to original indexing\n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    \n",
    "    return logits, indices_to_remove\n",
    "    \n",
    "def renyiDiv(p, q, alpha=float('inf')):\n",
    "        if alpha == float('inf'):\n",
    "            RD = torch.log(torch.max(p/q))\n",
    "        elif alpha == 1:\n",
    "            RD = torch.sum(p*torch.log(p/q))\n",
    "        else:\n",
    "            RD = 1/(alpha-1) * torch.log(\n",
    "                torch.sum(((p/q)**(alpha))*q)\n",
    "            )\n",
    "        if torch.isnan(RD):\n",
    "            RD = torch.log(torch.max(p/q))\n",
    "        return RD\n",
    "    \n",
    "def renyi_priv_loss(p, q, alpha):\n",
    "    return max(renyiDiv(p, q, alpha=alpha), renyiDiv(q, p, alpha=alpha)).cpu().numpy()\n",
    "\n",
    "def calculate_memorization(p, q, idx):\n",
    "    #return max(torch.log(p[idx]/q[idx]), torch.log(q[idx]/p[idx])).cpu().numpy()  \n",
    "    return abs(torch.log(p[idx]/q[idx])).numpy()#.cpu().numpy()\n",
    "\n",
    "def entropy(p):\n",
    "    return (-np.sum(p*np.log(p)))\n",
    "\n",
    "def lambda_solver_bisection(p, p_0, epsilon, alpha):\n",
    "    def f(lambd):\n",
    "        pred = lambd * p + (1-lambd) * p_0\n",
    "        #eps = np.max([np.max(np.log(pred/p_0)), np.max(np.log(p_0/pred))])\n",
    "        eps = max(renyiDiv(pred, p_0, alpha=alpha), renyiDiv(p_0, pred, alpha=alpha))\n",
    "        return (eps - epsilon)\n",
    "    if f(1) <= 0.0:\n",
    "        lambd = 1\n",
    "    else:\n",
    "        lambd = bisect(f, 0, 1, maxiter=20, disp=False)\n",
    "    return lambd\n",
    "\n",
    "def lambda_solver(p, p_0, epsilon):\n",
    "    a = (p_0 * (np.exp(epsilon/2) - 1)) / torch.abs(p - p_0)\n",
    "    val = torch.min(a)\n",
    "    return min(1, val)\n",
    "\n",
    "def mollify(p, p_0, epsilon, ids, alpha):\n",
    "    #lambd = lambda_solver(p[ids], p_0[ids], epsilon)\n",
    "    lambd = lambda_solver_bisection(p[ids].cpu(), p_0[ids].cpu(), epsilon, alpha)\n",
    "    return (lambd * p + (1-lambd) * p_0), lambd    \n",
    "\n",
    "def calc_partition_loss(proj_logit, proj_output, pub_output, alpha, temperature):\n",
    "    max_loss = 0\n",
    "    for i in range(proj_logit.shape[0]):\n",
    "        proj_logit_i = torch.cat([proj_logit[:i, :], proj_logit[i+1:, :]])\n",
    "        proj_output_i = F.softmax(proj_logit_i / temperature, dim=-1).mean(dim=0)\n",
    "        ids = torch.nonzero(proj_output)\n",
    "        eps = renyi_priv_loss(proj_output[ids], proj_output_i[ids], alpha)\n",
    "        max_loss = max(max_loss, eps)\n",
    "    return max_loss\n",
    "\n",
    "def calc_group_memorization(ensemble_outputs, idx):\n",
    "    return [calculate_memorization(ensemble_outputs[0, :], ensemble_outputs[i, :], idx) for i in range(1, ensemble_outputs.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cc4cab5-6d85-4bda-8639-b2079f259a9e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def post_calc_memorization(model,\n",
    "                   context_aware_input_ids,\n",
    "                   context_unaware_input_ids,\n",
    "                   response_input_ids,\n",
    "                   lambd,\n",
    "                   temperature,\n",
    "                   stop_token_ids,\n",
    "                   min_length,\n",
    "                   batch_size=None\n",
    "                  ):\n",
    "    mem_vals = []\n",
    "    N = context_aware_input_ids.shape[0]\n",
    "    for t in range(response_input_ids.shape[1]):\n",
    "        priv_context_aware_input_ids = torch.cat([context_aware_input_ids,\n",
    "                                      response_input_ids[:, :t].repeat(N, 1)],\n",
    "                                     dim=1)\n",
    "        pub_logit = model(torch.cat([context_unaware_input_ids,\n",
    "                                     response_input_ids[:, :t]],\n",
    "                                    dim=1)\n",
    "                         ).logits.squeeze()[-1, :].type(torch.float64)\n",
    "        if batch_size == None:\n",
    "            priv_logit = model(priv_context_aware_input_ids).logits[:, -1, :].type(torch.float64)\n",
    "        else:\n",
    "            priv_logit = torch.stack([model(priv_context_aware_input_ids[i:(i+1)*batch_size]).logits[:, -1, :].type(torch.float64)\n",
    "                     for i in range(0, N, batch_size)])\n",
    "        proj_logit = lambd * priv_logit + (1-lambd) * pub_logit.repeat(N, 1)\n",
    "        \n",
    "        if t < min_length:\n",
    "            pub_logit[stop_token_ids[0]] = -float(\"Inf\")\n",
    "            proj_logit[:, stop_token_ids[0]] = -float(\"Inf\")\n",
    "            \n",
    "        if pub_logit.shape[0] > len(tokenizer):\n",
    "            pub_logit[len(tokenizer):pub_logit.shape[0]] = -float(\"Inf\")\n",
    "            proj_logit[:, len(tokenizer):pub_logit.shape[0]] = -float(\"Inf\")\n",
    "        \n",
    "        pub_output = F.softmax(pub_logit / temperature, dim=-1)\n",
    "        priv_output = F.softmax(priv_logit / temperature, dim=-1)\n",
    "        proj_output = F.softmax(proj_logit / temperature, dim=-1)\n",
    "        \n",
    "        ids = torch.nonzero(pub_output)\n",
    "        #mem_val = calc_group_memorization(proj_output[:, ids].squeeze(), response_input_ids[:, t])\n",
    "        mem_val = calculate_memorization(proj_output[0, ids].squeeze(), pub_output[ids], response_input_ids[:, t])\n",
    "        mem_vals.append(mem_val[0][0])\n",
    "        \n",
    "    return mem_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dfb0b43-28b3-4b55-89ef-e31a9c502360",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def partition(data, tokenizer, partition_length, dataset_name):\n",
    "    document_ids = tokenizer(data['context']).input_ids\n",
    "    ensemble = []\n",
    "    for i in range(0, len(document_ids), partition_length):\n",
    "        idx = (i+partition_length)\n",
    "        #ensemble = torch.cat([ensemble, input_ids[-1:, idx:i]], dim=1)\n",
    "        row = {'context': tokenizer.decode(document_ids[i:idx], skip_special_tokens=True), 'query': data['query']}\n",
    "        ensemble.append(template_input(row, dataset_name))\n",
    "    return ensemble\n",
    "\n",
    "def group_partition(data, tokenizer, partition_length, dataset_name):\n",
    "    document_ids = tokenizer(data['context']).input_ids\n",
    "    groups = [template_input(data, dataset_name)]\n",
    "    for i in range(0, len(document_ids), partition_length):\n",
    "        idx = (i+partition_length)\n",
    "        group_i = document_ids[:i] + document_ids[idx:]\n",
    "        row = {'context': tokenizer.decode(group_i, skip_special_tokens=True), 'query': data['query']}\n",
    "        groups.append(template_input(row, dataset_name))\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23fde59f-b6a9-4439-bdfa-0c0601e14a95",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cmad_generation(model,\n",
    "                  context_aware_input_ids,\n",
    "                  context_unaware_input_ids,\n",
    "                  lambd,\n",
    "                  temperature,\n",
    "                  max_length,\n",
    "                  min_length,\n",
    "                  stop_token_ids,\n",
    "                  device,\n",
    "                 ):\n",
    "    response_input_ids = torch.LongTensor([[]]).to(device)\n",
    "    for i in range(max_length):\n",
    "        priv_context_aware_input_ids = torch.cat([context_aware_input_ids,\n",
    "                                      response_input_ids.repeat(context_aware_input_ids.shape[0], 1)],\n",
    "                                     dim=1)\n",
    "        pub_logit = model(torch.cat([context_unaware_input_ids,\n",
    "                                     response_input_ids],\n",
    "                                    dim=1)\n",
    "                         ).logits.squeeze()[-1, :].type(torch.float64)\n",
    "\n",
    "        priv_logit = model(priv_context_aware_input_ids).logits[:, -1, :].type(torch.float64)\n",
    "        proj_logit = lambd * priv_logit + (1-lambd) * pub_logit.repeat(priv_logit.shape[0], 1)\n",
    "        \n",
    "        if i < min_length:\n",
    "            pub_logit[stop_token_ids[0]] = -float(\"Inf\")\n",
    "            proj_logit[:, stop_token_ids[0]] = -float(\"Inf\")\n",
    "            \n",
    "        if pub_logit.shape[0] > len(tokenizer):\n",
    "            pub_logit[len(tokenizer):pub_logit.shape[0]] = -float(\"Inf\")\n",
    "            proj_logit[:, len(tokenizer):pub_logit.shape[0]] = -float(\"Inf\")\n",
    "            \n",
    "        pub_output = F.softmax(pub_logit / temperature, dim=-1)\n",
    "        #priv_output = F.softmax(priv_logit, dim=-1)[-1]\n",
    "        proj_output = F.softmax(proj_logit / temperature, dim=-1)\n",
    "\n",
    "        pred_idx = proj_output[0].multinomial(1).view(1, -1).long()\n",
    "        if pred_idx.cpu()[0].item() in stop_token_ids:\n",
    "            break\n",
    "\n",
    "        response_input_ids = torch.cat([response_input_ids, pred_idx], dim=1)\n",
    "        del pred_idx\n",
    "    return response_input_ids.cpu()[0], 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9b8aa8a-0b65-4182-ba87-d7c7ba3d0441",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decode_experiment(test_set, model, tokenizer, lambd, temperature, dataset_name, min_length):\n",
    "    dp_predictions = []\n",
    "    stop_token_ids = [tokenizer.eos_token_id,\n",
    "                      tokenizer.pad_token_id,\n",
    "                     ]\n",
    "    doc_priv_loss = [] \n",
    "    for idx, data in tqdm(enumerate(test_set), total=len(test_set)):\n",
    "        context_unaware_tokenized_input = tokenizer(template_empty_input(data, dataset_name), return_tensors=\"pt\", padding=True)\n",
    "        context_aware_tokenized_input = tokenizer(template_input(data, dataset_name), return_tensors=\"pt\", padding=True)\n",
    "        with torch.no_grad():\n",
    "            dp_output, doc_eps = cmad_generation(model,\n",
    "                                    context_aware_tokenized_input.input_ids.to(DEVICE),\n",
    "                                    context_unaware_tokenized_input.input_ids.to(DEVICE),\n",
    "                                    lambd=lambd,\n",
    "                                    temperature=temperature,\n",
    "                                    max_length=max_new_tokens,\n",
    "                                    min_length=min_length,\n",
    "                                    stop_token_ids=stop_token_ids,\n",
    "                                    device=DEVICE,\n",
    "                                    )\n",
    "        decode_dp_output = tokenizer.decode(dp_output, skip_special_tokens=True)\n",
    "        dp_predictions.append(decode_dp_output)\n",
    "        doc_priv_loss.append(doc_eps)\n",
    "    return dp_predictions, doc_priv_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c430e6b-3217-4812-9184-e007c911433c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dir_name = \"results\"\n",
    "m_name = \"Meta-Llama-3-8B\"\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "295cf787-4c58-4b3c-84b1-778d8080ea15",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mpad_token, tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token, tokenizer\u001b[38;5;241m.\u001b[39meos_token_id\n\u001b[0;32m---> 16\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccess_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#device_map=\"auto\"\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     25\u001b[0m test_set \u001b[38;5;241m=\u001b[39m pretokenize(dataset_name, raw_test_set, tokenizer, max_input_length)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lambd \u001b[38;5;129;01min\u001b[39;00m lambds:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m#file_name = f'{dataset_name}_{m_name}_{lambd}_context{max_input_length}.csv'\u001b[39;00m\n",
      "File \u001b[0;32m~/memorization_and_hallucination/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/memorization_and_hallucination/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:3866\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3864\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39mcan_generate() \u001b[38;5;129;01mand\u001b[39;00m pretrained_model_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3865\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3866\u001b[0m         model\u001b[38;5;241m.\u001b[39mgeneration_config \u001b[38;5;241m=\u001b[39m \u001b[43mGenerationConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3867\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3869\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3870\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3871\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3872\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3873\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3875\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3876\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_from_auto\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_auto_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3877\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3878\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3880\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   3881\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   3882\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeneration config file not found, using a generation config created from the model config.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3883\u001b[0m         )\n",
      "File \u001b[0;32m~/memorization_and_hallucination/venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:915\u001b[0m, in \u001b[0;36mGenerationConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name, config_file_name, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m    912\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m config_file_name\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 915\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    929\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/memorization_and_hallucination/venv/lib/python3.10/site-packages/transformers/utils/hub.py:402\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    417\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[0;32m~/memorization_and_hallucination/venv/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:101\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m         message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m custom_message\n\u001b[1;32m    100\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/memorization_and_hallucination/venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/memorization_and_hallucination/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1240\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m   1221\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m   1222\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1237\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1238\u001b[0m     )\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1257\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/memorization_and_hallucination/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1303\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1299\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;66;03m# Try to get metadata (etag, commit_hash, url, size) from the server.\u001b[39;00m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;66;03m# If we can't, a HEAD request error is returned.\u001b[39;00m\n\u001b[0;32m-> 1303\u001b[0m (url_to_download, etag, commit_hash, expected_size, head_call_error) \u001b[38;5;241m=\u001b[39m \u001b[43m_get_metadata_or_catch_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[38;5;66;03m# etag can be None for several reasons:\u001b[39;00m\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;66;03m# 1. we passed local_files_only.\u001b[39;00m\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;66;03m# 2. we don't have a connection\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;66;03m# If the specified revision is a commit hash, look inside \"snapshots\".\u001b[39;00m\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;66;03m# If the specified revision is a branch or tag, look inside \"refs\".\u001b[39;00m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_call_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1329\u001b[0m     \u001b[38;5;66;03m# Couldn't make a HEAD call => let's try to find a local file\u001b[39;00m\n",
      "File \u001b[0;32m~/memorization_and_hallucination/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1751\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1749\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1752\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[1;32m   1753\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1754\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1755\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m storage_folder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m relative_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1756\u001b[0m             \u001b[38;5;66;03m# Cache the non-existence of the file\u001b[39;00m\n",
      "File \u001b[0;32m~/memorization_and_hallucination/venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/memorization_and_hallucination/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1673\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1670\u001b[0m headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccept-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midentity\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1673\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1675\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1681\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1682\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1684\u001b[0m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "File \u001b[0;32m~/memorization_and_hallucination/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:376\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;66;03m# Recursively follow relative redirects\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 376\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m300\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m399\u001b[39m:\n",
      "File \u001b[0;32m~/memorization_and_hallucination/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:399\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m    398\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 399\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m hf_raise_for_status(response)\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/memorization_and_hallucination/venv/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/memorization_and_hallucination/venv/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/memorization_and_hallucination/venv/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:66\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     68\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[0;32m~/memorization_and_hallucination/venv/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/memorization_and_hallucination/venv/lib/python3.10/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/memorization_and_hallucination/venv/lib/python3.10/site-packages/urllib3/connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    444\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/memorization_and_hallucination/venv/lib/python3.10/site-packages/urllib3/connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1303\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1300\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1301\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1302\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1159\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "os.makedirs(dir_name, exist_ok=True)\n",
    "lambds = [0.5, 1.0, 1.5]\n",
    "model_names = [\"EleutherAI/gpt-neo-1.3B\"]\n",
    "m_names = [\"gpt-neo-1.3B\"]#, \"Meta-Llama-3-8B\"]\n",
    "for model_name, m_name in zip(model_names, m_names):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
    "                                          padding_side=\"left\",\n",
    "                                          use_fast=False,\n",
    "                                          token=access_token,\n",
    "                                          trust_remote_code=True,\n",
    "                                          cache_dir=cache_dir)\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        print(\"True\")\n",
    "        tokenizer.pad_token, tokenizer.pad_token_id = tokenizer.eos_token, tokenizer.eos_token_id\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        token=access_token,\n",
    "        cache_dir=cache_dir,\n",
    "        #device_map=\"auto\"\n",
    "        ).to(DEVICE)\n",
    "    \n",
    "    test_set = pretokenize(dataset_name, raw_test_set, tokenizer, max_input_length)\n",
    "    \n",
    "    for lambd in lambds:\n",
    "        #file_name = f'{dataset_name}_{m_name}_{lambd}_context{max_input_length}.csv'\n",
    "        file_name = f'{dataset_name}_{m_name}_{lambd}.csv'\n",
    "        dp_predictions, dp_loss = decode_experiment(test_set, model, tokenizer, lambd=lambd, temperature=0.8, dataset_name=dataset_name, min_length=10)\n",
    "        df = pd.DataFrame({'generations': dp_predictions, 'privacy_loss': dp_loss})\n",
    "        df.to_csv(os.path.join(dir_name, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0dbd69e2-a5f1-4aa4-81c2-c95fdd69e550",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 36691.40it/s]\n"
     ]
    }
   ],
   "source": [
    "documents, references = [], []\n",
    "for idx, data in tqdm(enumerate(test_set), total=len(test_set)):\n",
    "    documents.append(data['context'])\n",
    "    references.append(data['summary'])\n",
    "evaluator = Evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b75a7ada-e105-4d13-9cb5-d14ee76356f5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "lambd=1.0\n",
    "file_name = f'{dataset_name}_{m_name}_{lambd}.csv'\n",
    "df = pd.read_csv(os.path.join(dir_name, file_name))\n",
    "doc_priv_loss = df['privacy_loss']\n",
    "predictions = df['generations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38a3dd6c-11e3-4829-966c-4b43da6d9088",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn_opt-1.3b_1.0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "truncating documents...: 1000it [00:13, 74.13it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [1:05:53<00:00,  3.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parition len 1024\t Memorization: 85.2254963738699\n",
      "cnn_opt-1.3b_1.5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "truncating documents...: 1000it [00:12, 80.21it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [1:03:08<00:00,  3.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parition len 1024\t Memorization: 139.99504091702315\n",
      "cnn_opt-1.3b_0.5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "truncating documents...: 1000it [00:12, 80.42it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [1:09:07<00:00,  4.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parition len 1024\t Memorization: 17.501884455932405\n"
     ]
    }
   ],
   "source": [
    "# importing module\n",
    "import logging\n",
    "\n",
    "partition_len = max_input_length\n",
    "temperature=0.8\n",
    "stop_token_ids = [tokenizer.eos_token_id,\n",
    "                      tokenizer.pad_token_id,\n",
    "                     ]\n",
    "lambds = [1.0, 1.5, 0.5]\n",
    "mean_vals = []\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    token=access_token,\n",
    "    cache_dir=cache_dir,\n",
    "    local_files_only=True,\n",
    "    #device_map=\"auto\",\n",
    "    #max_memory = {0: \"35GB\", 1: \"35GB\", 2: \"0GB\", 3: \"35GB\", 4: \"35GB\", 5: \"0GB\", 6: \"0GB\", 7: \"0GB\"}\n",
    "    ).to(DEVICE)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
    "                                      padding_side=\"left\",\n",
    "                                      use_fast=False,\n",
    "                                      token=access_token,\n",
    "                                      trust_remote_code=True,\n",
    "                                      cache_dir=cache_dir)\n",
    "if tokenizer.pad_token is None:\n",
    "    print(\"True\")\n",
    "    tokenizer.pad_token, tokenizer.pad_token_id = tokenizer.eos_token, tokenizer.eos_token_id\n",
    "\n",
    "for partition_len in [max_input_length]:\n",
    "    for lambd in lambds:\n",
    "        file_name = f'{dataset_name}_{m_name}_{lambd}.csv'\n",
    "        #file_name = f'{dataset_name}_{m_name}_{lambd}_context{context_len}.csv'\n",
    "        df = pd.read_csv(os.path.join(dir_name, file_name))\n",
    "        predictions = df['generations']\n",
    "        vals = []\n",
    "        print(file_name)\n",
    "        \n",
    "        test_set = pretokenize(dataset_name, raw_test_set, tokenizer, max_input_length)\n",
    "        query_set = test_set.select(range(1000))\n",
    "\n",
    "        for data, response in tqdm(zip(query_set, predictions), total=len(query_set)):\n",
    "            context_unaware_tokenized_input = tokenizer(template_empty_input(data, dataset_name), return_tensors=\"pt\", padding=True)\n",
    "            ensemble = group_partition(data, tokenizer, partition_len, dataset_name=dataset_name)\n",
    "            context_aware_tokenized_input = tokenizer(ensemble, return_tensors=\"pt\", max_length=max_input_length+25, padding=True, truncation=True)\n",
    "            response_tokenized_input = tokenizer(response, return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                cur_mem = post_calc_memorization(model,\n",
    "                                           context_aware_tokenized_input.input_ids.to(DEVICE),\n",
    "                                           context_unaware_tokenized_input.input_ids.to(DEVICE),\n",
    "                                           response_tokenized_input.input_ids[:, 1:].to(DEVICE),\n",
    "                                           lambd,\n",
    "                                           temperature,\n",
    "                                           stop_token_ids,\n",
    "                                           min_new_tokens,\n",
    "                                           batch_size=None\n",
    "                                          )\n",
    "            vals.append(cur_mem)\n",
    "\n",
    "        mem_vals = np.zeros([len(vals),len(max(vals,key = lambda x: len(x)))])\n",
    "        for i,j in enumerate(vals):\n",
    "            mem_vals[i, 0:len(j)] = j\n",
    "        print(f\"Parition len {partition_len}\\t Memorization: {np.mean(np.sum(mem_vals, axis=1))}\")\n",
    "        mean_vals.append(np.mean(np.sum(mem_vals, axis=1)))\n",
    "model = model.to(\"cpu\")\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6ded613",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inmport matplotlib.pyplot as plt\n",
    "\n",
    "mean_vals = np.mean(mem_vals, axis=0)\n",
    "plt.plot(np.arange(0, len(mean_vals)), mean_vals)\n",
    "plt.xlabel(\"Generation Length $|\\mathbf{y}|$\", fontsize=15)\n",
    "plt.ylabel(r'$\\mathbb{E}[f_{\\text{Mem}}(\\overline{p}_{\\theta})]$', fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.savefig(\"gen_len_analysis.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46d1ad54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cnn_Meta-Llama-3-8B_1.0.csv'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f426ec09-c9a8-4db5-8ffc-91f0972ca151",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result_dict \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[38;5;241m.\u001b[39mevaluate(predictions, references, documents)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluator' is not defined"
     ]
    }
   ],
   "source": [
    "result_dict = evaluator.evaluate(predictions, references, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0465a839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26bd8cd8bd9048c39a823c0d9bc62694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    token=access_token,\n",
    "    cache_dir=cache_dir,\n",
    "    local_files_only=True,\n",
    "    #device_map=\"auto\",\n",
    "    #max_memory = {0: \"0GB\", 1: \"0GB\", 2: \"35GB\", 3: \"35GB\", 4: \"35GB\", 5: \"35GB\", 6: \"35GB\", 7: \"35GB\"}\n",
    "    ).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "42608e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, response = test_set[53], predictions[53]\n",
    "partition_len = 1\n",
    "temperature=0.8\n",
    "\n",
    "context_unaware_tokenized_input = tokenizer(template_empty_input(data, dataset_name), return_tensors=\"pt\", padding=True)\n",
    "ensemble = group_partition(data, tokenizer, partition_len, dataset_name=dataset_name)\n",
    "context_aware_tokenized_input = tokenizer(ensemble, return_tensors=\"pt\", max_length=max_input_length+25, padding=True, truncation=True)\n",
    "response_tokenized_input = tokenizer(response, return_tensors=\"pt\")\n",
    "\n",
    "context_aware_input_ids = context_aware_tokenized_input.input_ids.to(DEVICE)\n",
    "response_input_ids = response_tokenized_input.input_ids.to(DEVICE)\n",
    "context_unaware_input_ids = context_unaware_tokenized_input.input_ids.to(DEVICE)\n",
    "\n",
    "N = context_aware_input_ids.shape[0]\n",
    "priv_context_aware_input_ids = torch.cat([context_aware_input_ids,\n",
    "                              response_input_ids[:, :20].repeat(N, 1)],\n",
    "                             dim=1)\n",
    "batch_size = 1\n",
    "with torch.no_grad():\n",
    "    pub_logit = model(torch.cat([context_unaware_input_ids,\n",
    "                             response_input_ids[:, :20]],\n",
    "                            dim=1)\n",
    "                 ).logits.squeeze()[-1, :].type(torch.float64).cpu()\n",
    "    if batch_size == None:\n",
    "        priv_logit = model(priv_context_aware_input_ids).logits[:, -1, :].type(torch.float64).cpu()\n",
    "    else:\n",
    "        priv_logit = torch.stack([model(priv_context_aware_input_ids[i:(i+1)*batch_size]).logits[:, -1, :].type(torch.float64).cpu()\n",
    "                 for i in range(0, N, batch_size)]).squeeze(1)\n",
    "\n",
    "proj_logit = lambd * priv_logit + (1-lambd) * pub_logit.repeat(N, 1)\n",
    "\n",
    "if pub_logit.shape[0] > len(tokenizer):\n",
    "    pub_logit[len(tokenizer):pub_logit.shape[0]] = -float(\"Inf\")\n",
    "    proj_logit[:, len(tokenizer):pub_logit.shape[0]] = -float(\"Inf\")\n",
    "\n",
    "pub_output = F.softmax(pub_logit / temperature, dim=-1)\n",
    "priv_output = F.softmax(priv_logit / temperature, dim=-1)\n",
    "proj_output = F.softmax(proj_logit / temperature, dim=-1)\n",
    "\n",
    "ids = torch.nonzero(pub_output)\n",
    "mem_val = calc_group_memorization(proj_output[:, ids].squeeze(), response_input_ids[:, 20].cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eaf16500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def value_to_color(value, min_value, max_value):\n",
    "    \"\"\" Map a float value to a color based on its position in the value range. \"\"\"\n",
    "    norm = mcolors.Normalize(vmin=min_value, vmax=max_value)\n",
    "    cmap = plt.get_cmap('coolwarm')  # You can choose different colormaps\n",
    "    return mcolors.to_hex(cmap(norm(value)))\n",
    "\n",
    "# Normalize and colorize\n",
    "min_value = min(mem_vals)\n",
    "max_value = max(mem_vals)\n",
    "\n",
    "colors = [value_to_color(val, min_value, max_value) for val in mem_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7fadf9ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><span style=\"color: #c7d7f0;\"> Luckily</span> <span style=\"color: #e4d9d2;\">,</span> <span style=\"color: #f29274;\"> Japanese</span> <span style=\"color: #f7b599;\"> can</span> <span style=\"color: #f4c5ad;\"> sleep</span> <span style=\"color: #edd1c2;\"> sound</span> <span style=\"color: #d3dbe7;\">ly</span> <span style=\"color: #e7d7ce;\"> in</span> <span style=\"color: #d5dbe5;\"> their</span> <span style=\"color: #f2cbb7;\"> beds</span> <span style=\"color: #ebd3c6;\"> tonight</span> <span style=\"color: #efcebd;\"> as</span> <span style=\"color: #dddcdc;\"> the</span> <span style=\"color: #cedaeb;\"> government</span> <span style=\"color: #c7d7f0;\">'s</span> <span style=\"color: #aac7fd;\"> top</span> <span style=\"color: #f6bfa6;\"> military</span> <span style=\"color: #a5c3fe;\"> official</span> <span style=\"color: #f49a7b;\"> earnest</span> <span style=\"color: #d8dce2;\">ly</span> <span style=\"color: #80a3fa;\"> revealed</span> <span style=\"color: #bbd1f8;\"> that</span> <span style=\"color: #b2ccfb;\"> the</span> <span style=\"color: #dbdcde;\"> country</span> <span style=\"color: #c7d7f0;\">'s</span> <span style=\"color: #9bbcff;\"> Air</span> <span style=\"color: #8fb1fe;\"> Self</span> <span style=\"color: #d8dce2;\"> Defense</span> <span style=\"color: #ccd9ed;\"> Force</span> <span style=\"color: #f4c5ad;\"> (</span> <span style=\"color: #dfdbd9;\">AS</span> <span style=\"color: #f6bfa6;\">DF</span> <span style=\"color: #cedaeb;\">)</span> <span style=\"color: #dddcdc;\"> had</span> <span style=\"color: #f6a385;\"> never</span> <span style=\"color: #455cce;\"> encountered</span> <span style=\"color: #a7c5fe;\"> an</span> <span style=\"color: #f7ad90;\"> extr</span> <span style=\"color: #7a9df8;\">ater</span> <span style=\"color: #5d7ce6;\">restrial</span> <span style=\"color: #dcdddd;\"> unidentified</span> <span style=\"color: #5875e1;\"> flying</span> <span style=\"color: #4961d2;\"> object</span> <span style=\"color: #f7ad90;\">.</span> <span style=\"color: #eed0c0;\"> Respond</span> <span style=\"color: #a2c1ff;\">ing</span> <span style=\"color: #d6dce4;\"> to</span> <span style=\"color: #e1dad6;\"> a</span> <span style=\"color: #f5c0a7;\"> query</span> <span style=\"color: #c4d5f3;\"> from</span> <span style=\"color: #88abfd;\"> flam</span> <span style=\"color: #b9d0f9;\">boy</span> <span style=\"color: #c3d5f4;\">ant</span> <span style=\"color: #e3d9d3;\"> former</span> <span style=\"color: #cbd8ee;\"> wrestler</span> <span style=\"color: #dddcdc;\">-turned</span> <span style=\"color: #f6bea4;\">-law</span> <span style=\"color: #eed0c0;\">maker</span> <span style=\"color: #5f7fe8;\"> Antonio</span> <span style=\"color: #c5d6f2;\"> In</span> <span style=\"color: #5572df;\">oki</span> <span style=\"color: #a7c5fe;\">,</span> <span style=\"color: #6b8df0;\"> Defense</span> <span style=\"color: #d5dbe5;\"> Minister</span> <span style=\"color: #bbd1f8;\"> Gen</span> <span style=\"color: #efcebd;\"> Nak</span> <span style=\"color: #eed0c0;\">at</span> <span style=\"color: #d3dbe7;\">ani</span> <span style=\"color: #f59c7d;\"> told</span> <span style=\"color: #e0dbd8;\"> the</span> <span style=\"color: #f3c7b1;\"> Diet</span> <span style=\"color: #f1ccb8;\">,</span> <span style=\"color: #f5c0a7;\"> Japan</span> <span style=\"color: #e9d5cb;\">'s</span> <span style=\"color: #e2dad5;\"> parliament</span> <span style=\"color: #e5d8d1;\">,</span> <span style=\"color: #edd2c3;\"> that</span> <span style=\"color: #c9d7f0;\"> his</span> <span style=\"color: #bad0f8;\"> jets</span> <span style=\"color: #f5c0a7;\"> had</span> <span style=\"color: #e0dbd8;\">,</span> <span style=\"color: #b5cdfa;\"> to</span> <span style=\"color: #efcebd;\"> date</span> <span style=\"color: #ead4c8;\">,</span> <span style=\"color: #6384eb;\"> never</span> <span style=\"color: #dbdcde;\"> come</span> <span style=\"color: #7295f4;\"> across</span> <span style=\"color: #f6bea4;\"> any</span> <span style=\"color: #b40426;\"> UFO</span> <span style=\"color: #6c8ff1;\">s</span> <span style=\"color: #e0dbd8;\"> from</span> <span style=\"color: #f5c4ac;\"> outer</span> <span style=\"color: #b5cdfa;\"> space</span> <span style=\"color: #d9dce1;\">.</span></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "def colorize_text_html(vals, colors):\n",
    "    colored_text = ' '.join(\n",
    "        f'<span style=\"color: {color};\">{tokenizer.decode(input_ids)}</span>'\n",
    "        for input_ids, color in zip(vals, colors)\n",
    "    )\n",
    "    return f'<p>{colored_text}</p>'\n",
    "\n",
    "colored_text_html = colorize_text_html(data_tokenized_input_ids[47:141], colors[47:141])\n",
    "HTML(colored_text_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "43932170",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.lib.colors import HexColor, black\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.lib.units import inch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import ScalarMappable\n",
    "import numpy as np\n",
    "\n",
    "def create_colored_pdf(vals, colors, filename):\n",
    "    c = canvas.Canvas(filename, pagesize=letter)\n",
    "    width, height = letter\n",
    "\n",
    "    # Set starting position\n",
    "    x, y = 50, height - 50\n",
    "    line_height = 14\n",
    "\n",
    "    # Draw text with colors\n",
    "    for input_ids, color in zip(vals, colors):\n",
    "        word = tokenizer.decode(input_ids)\n",
    "        color = HexColor(color)\n",
    "\n",
    "        # Set color and draw text\n",
    "        c.setFillColor(color)\n",
    "        c.drawString(x, y, word)\n",
    "        x += c.stringWidth(word + ' ', 'Helvetica', 12)\n",
    "\n",
    "        # Move to next line if needed\n",
    "        if x > width - 50:\n",
    "            x = 50\n",
    "            y -= line_height\n",
    "\n",
    "    # Draw the color scale image in PDF\n",
    "    #c.drawImage(color_scale_image, scale_x, scale_y, width=scale_width, height=scale_height)\n",
    "    \n",
    "    c.save()\n",
    "\n",
    "# Create the PDF\n",
    "create_colored_pdf(data_tokenized_input_ids[47:141], colors[47:141], \"colored_text.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a237aee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = canvas.Canvas(\"colored_text.pdf\", pagesize=letter)\n",
    "\n",
    "# Draw color scale\n",
    "scale_width = 5 * inch\n",
    "scale_height = 0.5 * inch\n",
    "scale_x = 50\n",
    "scale_y = 50\n",
    "c.setFillColor(black)\n",
    "c.rect(scale_x, scale_y, scale_width, scale_height, fill=0)\n",
    "\n",
    "# Create color scale using matplotlib\n",
    "fig, ax = plt.subplots(figsize=(5, 0.5), dpi=80)\n",
    "fig.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "cmap = plt.get_cmap('coolwarm')\n",
    "norm = mcolors.Normalize(vmin=min_value, vmax=max_value)\n",
    "sm = ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])\n",
    "cb = plt.colorbar(sm, cax=ax, orientation='horizontal')\n",
    "cb.ax.tick_params(labelsize=10)\n",
    "cb.ax.set_title('Value Scale', fontsize=10)\n",
    "#plt.show()\n",
    "# Save color scale to an image\n",
    "color_scale_image = \"color_scale.pdf\"\n",
    "plt.savefig(color_scale_image, bbox_inches='tight', pad_inches=0)\n",
    "plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b596499e6b756ee3ee734d514920d364bec1c6a0ebf031bda292a137927d8dcb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
